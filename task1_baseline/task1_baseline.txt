# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/bin_kfold_CSV.py
# ==============================================================================
# CORRECTED K-FOLD SPLITTING SCRIPT FOR SURVIVAL ANALYSIS
# ==============================================================================
# This script creates balanced 5-fold cross-validation splits.
#
# STRATEGY:
# It uses a simplified and targeted stratification approach to solve the
# issue of covariate shift that previously caused poor performance on fold 3.
#
# 1. It stratifies on the binary survival outcome (event/censored).
# 2. It ALSO stratifies on binned pre-operative PSA, the key feature that
#    was previously imbalanced.
#
# This ensures that each fold's training and test split has a similar
# distribution of both event rates AND key prognostic features.
# ==============================================================================

import os
import json
import pandas as pd
import glob
import argparse
from sklearn.model_selection import StratifiedKFold

def load_clinical_data(clin_dat_path: str) -> pd.DataFrame:
    """
    Load clinical data from JSON files, including key prognostic variables.
    """
    clinical_data = []
    json_files = glob.glob(os.path.join(clin_dat_path, "*.json"))
    print(f"Found {len(json_files)} JSON files in {clin_dat_path}")

    for json_file in json_files:
        patient_id = os.path.basename(json_file).replace('.json', '')
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)

            bcr_months = data.get("time_to_follow-up/BCR")
            bcr_status = data.get("BCR")

            if bcr_status == "1.0":
                censorship = 0  # Event (BCR) occurred
            elif bcr_status == "0.0":
                censorship = 1  # Censored (no BCR)
            else:
                continue

            if bcr_months is not None and bcr_months > 0:
                clinical_record = {
                    'patient_id': patient_id,
                    'bcr_months': bcr_months,
                    'censorship': censorship,
                    'psa': data.get("pre_operative_PSA"),
                    'isup_grade': data.get("ISUP")
                }
                clinical_data.append(clinical_record)
        except Exception as e:
            print(f"Error processing {json_file}: {e}")

    return pd.DataFrame(clinical_data)

def find_feature_files(feat_path: str, patient_ids: list) -> pd.DataFrame:
    """
    Find feature files for each patient.
    """
    feature_data = []
    for patient_id in patient_ids:
        pattern = os.path.join(feat_path, f"{patient_id}_*.pt")
        pt_files = glob.glob(pattern)
        if pt_files:
            feature_data.append({
                'patient_id': patient_id,
                'slide_id': os.path.basename(pt_files[0]).replace('.pt', '')
            })
    return pd.DataFrame(feature_data)

def create_merged_dataset(clinical_df: pd.DataFrame, feature_df: pd.DataFrame) -> pd.DataFrame:
    """
    Merge clinical and feature data, creating one row per patient.
    """
    merged_df = feature_df.merge(clinical_df, on='patient_id', how='inner')
    merged_df = merged_df.rename(columns={
        'bcr_months': 'bcr_survival_months',
        'censorship': 'bcr_censorship',
        'patient_id': 'case_id'
    })
    return merged_df

def create_stratification_variable(df: pd.DataFrame) -> pd.Series:
    """
    Create a SIMPLIFIED composite stratification variable for balanced folds.
    This version stratifies ONLY on binned PSA and BCR status.
    """
    print("\nCreating simplified stratification variable...")
    df_copy = df.copy()
    df_copy['psa'] = pd.to_numeric(df_copy['psa'], errors='coerce')

    if df_copy['psa'].isna().any():
        psa_median = df_copy['psa'].median()
        print(f"Warning: Missing PSA values found. Filling with median ({psa_median:.2f}).")
        df_copy['psa'].fillna(psa_median, inplace=True)

    # Bin PSA into 4 quantile-based groups (quartiles)
    try:
        # `duplicates='drop'` handles cases where bin edges are not unique
        psa_binned = pd.qcut(df_copy['psa'], q=4, labels=[f'PSA_Q{i+1}' for i in range(4)], duplicates='drop')
    except ValueError:
        print("Warning: Could not create 4 PSA bins. Falling back to 3 bins.")
        psa_binned = pd.qcut(df_copy['psa'], q=3, labels=[f'PSA_Q{i+1}' for i in range(3)], duplicates='drop')

    # Get BCR status as a string
    bcr_str = df_copy['bcr_censorship'].apply(lambda x: 'event' if x == 0 else 'censored')

    # Combine into a simple, robust stratum
    strata = psa_binned.astype(str) + '_' + bcr_str
    print("Final stratum distribution:")
    print(strata.value_counts())
    return strata

def perform_cross_validation(merged_df: pd.DataFrame, output_dir: str, n_splits: int = 5) -> None:
    """
    Perform stratified k-fold cross-validation and save the splits.
    Includes validation statistics to confirm balance.
    """
    stratification_var = create_stratification_variable(merged_df)
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    os.makedirs(output_dir, exist_ok=True)

    fold_stats = []
    print("\n--- Performing Cross-Validation ---")
    for fold, (train_idx, test_idx) in enumerate(skf.split(merged_df, stratification_var)):
        train_df = merged_df.iloc[train_idx]
        test_df = merged_df.iloc[test_idx]

        fold_dir = os.path.join(output_dir, f'fold_{fold}')
        os.makedirs(fold_dir, exist_ok=True)

        columns_to_save = ['slide_id', 'bcr_survival_months', 'bcr_censorship', 'case_id']
        train_df[columns_to_save].to_csv(os.path.join(fold_dir, 'train.csv'), index=False)
        test_df[columns_to_save].to_csv(os.path.join(fold_dir, 'test.csv'), index=False)

        # Collect statistics for validation
        stats = {
            'Fold': fold,
            'Train Size': len(train_df),
            'Test Size': len(test_df),
            'Test Event Rate': (test_df['bcr_censorship'] == 0).mean(),
            'Test Median PSA': test_df['psa'].median(),
            'Test Median ISUP': test_df['isup_grade'].median()
        }
        fold_stats.append(stats)
        print(f"Fold {fold}: Train={len(train_df)}, Test={len(test_df)}")

    # --- Print Validation Summary Table ---
    stats_df = pd.DataFrame(fold_stats)
    print("\n--- FOLD BALANCE VALIDATION SUMMARY ---")
    print(stats_df.to_string(index=False))

    # Check the variance of key stats across test folds (lower is better)
    event_rate_var = stats_df['Test Event Rate'].var()
    psa_var = stats_df['Test Median PSA'].var()
    print("\nVariance across test folds (lower is better):")
    print(f"  - Event Rate Variance: {event_rate_var:.6f}")
    print(f"  - Median PSA Variance: {psa_var:.4f}")
    print("\nâœ… Cross-validation splits created successfully.")
    print(f"Review the table above. The 'Test Median PSA' values should be very similar across all folds.")


def main():
    parser = argparse.ArgumentParser(description='Create CORRECTED k-fold splits for survival analysis.')
    parser.add_argument('--clin_dat_path', type=str, required=True, help='Path to directory with JSON clinical files')
    parser.add_argument('--feat_path', type=str, required=True, help='Path to directory with feature (.pt) files')
    parser.add_argument('--output_dir', type=str, required=True, help='Output directory for fold splits')
    parser.add_argument('--n_splits', type=int, default=5, help='Number of folds')
    args = parser.parse_args()

    print("Loading clinical data...")
    clinical_df = load_clinical_data(args.clin_dat_path)
    if clinical_df.empty:
        print("Error: No valid clinical data found.")
        return

    print("Finding feature files...")
    feature_df = find_feature_files(args.feat_path, clinical_df['patient_id'].tolist())
    if feature_df.empty:
        print("Error: No feature files found for the given patients.")
        return

    print("Creating merged dataset...")
    merged_df = create_merged_dataset(clinical_df, feature_df)
    if merged_df.empty:
        print("Error: Merged dataset is empty.")
        return

    perform_cross_validation(merged_df, args.output_dir, args.n_splits)

if __name__ == "__main__":
    main()

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/CHANGES.md
# CHIMERA Task1_baseline2 Changes

This document details all changes made in `/Users/robertspaans/Documents/git_repos/CHIMERA/task1_baseline2` compared to the original code in `/Users/robertspaans/Documents/git_repos/CHIMERA/common`.

## Overview

The modifications enable robust biochemical recurrence (BCR) survival analysis with Cox loss, support for variable bag sizes, clean output control, and comprehensive result logging.

---

## File Changes

### 1. `Aggregators/wsi_datasets/wsi_survival.py`

#### Support for "features" Directory
**Location:** Line 38  
**Change:** Added support for "features" as a valid data source directory name.

```python
# BEFORE:
assert os.path.basename(src) in ['feats_h5', 'feats_pt']

# AFTER:
assert os.path.basename(src) in ['feats_h5', 'feats_pt', 'features']
```

**Purpose:** Enables the dataset to work with feature directories named "features" in addition to the existing naming conventions.

---

### 2. `Aggregators/training/main_survival.py`

#### BCR-Specific Argument Defaults

**Input Dimension Default**  
**Location:** ~Line 170
```python
# BEFORE:
parser.add_argument('--in_dim', default=768, type=int, help='dim of input features')

# AFTER:
parser.add_argument('--in_dim', default=1024, type=int, help='dim of input features')
```

**Loss Function Default**  
**Location:** ~Line 175
```python
# BEFORE:
parser.add_argument('--loss_fn', type=str, default='nll', choices=['nll', 'cox', 'sumo', 'ipcwls', 'rank'], help='which loss function to use')

# AFTER:
parser.add_argument('--loss_fn', type=str, default='cox', choices=['nll', 'cox', 'sumo', 'ipcwls', 'rank'], help='which loss function to use')
```

**Task Default**  
**Location:** ~Line 183
```python
# BEFORE:
parser.add_argument('--task', type=str, default='unspecified_survival_task')

# AFTER:
parser.add_argument('--task', type=str, default='bcr_survival_task')
```

**Target Column Default**  
**Location:** ~Line 184
```python
# BEFORE:
parser.add_argument('--target_col', type=str, default='os_survival_days')

# AFTER:
parser.add_argument('--target_col', type=str, default='bcr_survival_months')
```

#### Robust Patching Info Extraction

**Location:** ~Line 250  
**Change:** Added error handling and default values for patching information extraction.

```python
# BEFORE:
mag, patch_size = extract_patching_info(os.path.dirname(src))
if (mag < 0 or patch_size < 0):
    raise ValueError(f"invalid patching info parsed for {src}")

# AFTER:
try:
    patching_info = extract_patching_info(os.path.dirname(src))
    if patching_info is None or len(patching_info) != 2:
        raise ValueError("Invalid patching info")
    mag, patch_size = patching_info
    if (mag < 0 or patch_size < 0):
        raise ValueError("Negative patching values")
except:
    # Set default values if parsing fails
    print(f"Warning: Could not parse patching info from {src}, using defaults")
    mag, patch_size = 20, 256  # Default magnification and patch size
```

**Purpose:** Prevents crashes when patching info cannot be parsed, using sensible defaults (20x magnification, 256 patch size).

#### CSV Export Functionality

**Location:** End of main() function (~Line 110-140)  
**Change:** Added complete CSV export functionality for easy result viewing.

```python
# ADDED ENTIRE BLOCK:
# Also save a CSV version for easy viewing
try:
    csv_dump_data = []
    for split, dumps in fold_dumps.items():
        for key, values in dumps.items():
            if isinstance(values, (list, tuple)):
                for i, val in enumerate(values):
                    csv_dump_data.append({
                        'split': split,
                        'metric': key,
                        'index': i,
                        'value': val
                    })
            else:
                csv_dump_data.append({
                    'split': split,
                    'metric': key,
                    'index': 0,
                    'value': values
                })
    
    if csv_dump_data:
        csv_dump_df = pd.DataFrame(csv_dump_data)
        csv_dump_df.to_csv(j_(args.results_dir, 'all_dumps.csv'), index=False)
        print(f"CSV dump saved to: {j_(args.results_dir, 'all_dumps.csv')}")
except Exception as e:
    print(f"Warning: Could not save CSV dump: {e}")
```

**Purpose:** Creates a human-readable CSV file alongside the HDF5 dumps for easier result inspection.

---

### 3. `Aggregators/training/trainer.py`

#### Cox Loss Accumulation Logic

**Location:** `train_loop_survival` function (lines ~270-410)  
**Change:** Complete rewrite to support Cox loss with variable bag sizes.

**Key additions:**
```python
# For Cox loss, we need to accumulate multiple samples before computing loss
is_cox_loss = isinstance(loss_fn, CoxLoss)

if is_cox_loss:
    # Accumulate samples for Cox loss computation
    accumulated_outputs = []
    accumulated_times = []
    accumulated_censorships = []
```

**Features:**
- Accumulates multiple samples for Cox loss computation
- Handles variable bag sizes with batch_size=1
- Special batch processing logic for Cox losses
- Modified loss computation and backpropagation

#### Enhanced Function Signatures

**validate_survival Function**  
**Location:** ~Line 409
```python
# BEFORE:
def validate_survival(model, loader,
                      loss_fn=None,
                      print_every=50,
                      dump_results=False,
                      recompute_loss_at_end=True,
                      verbose=1):

# AFTER:
def validate_survival(model, loader,
                      loss_fn=None,
                      print_every=50,
                      dump_results=False,
                      recompute_loss_at_end=True,
                      verbose=1,
                      split_name=None,
                      show_batch_progress=True):
```

**validate_classification Function**  
**Location:** ~Line 212
```python
# BEFORE:
def validate_classification(model, loader,
                            loss_fn=None,
                            print_every=50,
                            dump_results=False,
                            verbose=1):

# AFTER:
def validate_classification(model, loader,
                            loss_fn=None,
                            print_every=50,
                            dump_results=False,
                            verbose=1,
                            show_batch_progress=True):
```

#### Controlled Batch Progress Output

**validate_survival Per-batch Printing**  
**Location:** ~Line 440
```python
# BEFORE:
if verbose and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):

# AFTER:
# Only show per-batch progress if show_batch_progress is True (i.e., during training validation)
if verbose and show_batch_progress and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):
```

**validate_classification Per-batch Printing**  
**Location:** ~Line 248
```python
# BEFORE:
if verbose and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):

# AFTER:
# Only show per-batch progress if show_batch_progress is True (i.e., during training validation)
if verbose and show_batch_progress and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):
```

#### Updated Function Calls

**Training Validation Calls**  
**Location:** ~Line 107
```python
# ADDED explicit show_batch_progress=True during training validation
val_results, _ = validate_survival(model, datasets['val'], loss_fn,
                                 print_every=args.print_every, verbose=True, show_batch_progress=True)
```

**Final Evaluation Calls**  
**Location:** ~Line 144-148
```python
# ADDED explicit show_batch_progress=False during final evaluation
results[k], dumps[k] = validate_survival(model, loader, loss_fn, print_every=args.print_every,
                                        dump_results=True, verbose=1, show_batch_progress=False)
```

#### Train Results Logging

**Location:** ~Line 150  
**Change:** Ensure train results are properly logged during final evaluation.

```python
# Train results are now properly logged with verbose output enabled
log_dict_tensorboard(writer, results[k], f'final/{k}_', 0, verbose=True)
```

---

## Summary of Improvements

### 1. **BCR-Specific Configuration**
- Set appropriate defaults for BCR survival analysis
- Changed input dimension from 768 to 1024
- Changed default loss function from 'nll' to 'cox'
- Set default task and target column for BCR

### 2. **Robust Data Handling**
- Added support for "features" directory naming
- Implemented fallback defaults for patching info parsing
- Enhanced error handling throughout the pipeline

### 3. **Cox Loss Support**
- Complete rewrite of survival training loop
- Accumulation logic for variable bag sizes
- Proper handling of batch_size=1 scenarios
- Enhanced loss computation for Cox regression

### 4. **Output Control**
- Added `show_batch_progress` parameter for clean evaluation output
- Suppressed per-batch printing during final evaluation
- Maintained detailed progress during training
- Ensured comprehensive final summary metrics

### 5. **Result Management**
- Added CSV export functionality for easy result viewing
- Proper logging of train split results
- Enhanced result persistence and accessibility
- Comprehensive metric tracking across all splits

### 6. **Code Quality**
- Enhanced error handling and robustness
- Clear separation of training vs evaluation output
- Consistent parameter handling across functions
- Improved debugging and monitoring capabilities

---

## Usage Impact

These changes enable:
- **Robust BCR survival analysis** with appropriate defaults
- **Variable bag size handling** without crashes
- **Clean output** during evaluation phases
- **Comprehensive result tracking** across all data splits
- **Easy result inspection** through CSV exports
- **Proper Cox loss computation** for survival analysis

The modifications maintain backward compatibility while significantly enhancing the codebase's robustness and usability for biochemical recurrence prediction tasks.


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/kfold_CSV.py
import os
import json
import pandas as pd
import glob
import argparse
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import KBinsDiscretizer
from pathlib import Path

def load_clinical_data(clin_dat_path):
    """
    Load clinical data from JSON files for Task 1 (BCR prediction)
    Enhanced to include prognostic variables for stratified splitting
    """
    clinical_data = []
    json_files = glob.glob(os.path.join(clin_dat_path, "*.json"))
    
    print(f"Found {len(json_files)} JSON files in {clin_dat_path}")
    
    for json_file in json_files:
        patient_id = os.path.basename(json_file).replace('.json', '')
        
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # Extract survival information
            bcr_months = data.get("time_to_follow-up/BCR")
            bcr_status = data.get("BCR")
            
            # Convert BCR status to censorship (0 = event occurred, 1 = censored)
            if bcr_status == "1.0":
                censorship = 0  # Event occurred (BCR happened)
            elif bcr_status == "0.0":
                censorship = 1  # Censored (no BCR)
            else:
                print(f"Warning: Unexpected BCR value '{bcr_status}' for patient {patient_id}")
                continue
            
            # Extract key prognostic variables for stratification
            # These were identified as the main drivers of the fold 3 distribution issue
            psa = data.get("pre_operative_PSA")
            clinical_t = data.get("clinical_T_stage") 
            isup_grade = data.get("ISUP")
            primary_gleason = data.get("primary_gleason")
            secondary_gleason = data.get("secondary_gleason")
            age = data.get("age_at_prostatectomy")
            
            # Keep survival time in months
            if bcr_months is not None and bcr_months > 0:
                clinical_record = {
                    'patient_id': patient_id,
                    'bcr_months': bcr_months,
                    'censorship': censorship,
                    # Prognostic variables for stratification
                    'psa': psa,
                    'clinical_t': clinical_t,
                    'isup_grade': isup_grade,
                    'primary_gleason': primary_gleason,
                    'secondary_gleason': secondary_gleason,
                    'age': age
                }
                clinical_data.append(clinical_record)
            else:
                print(f"Warning: Invalid survival time for patient {patient_id}: {bcr_months} months")
                
        except Exception as e:
            print(f"Error processing {json_file}: {e}")
    
    return pd.DataFrame(clinical_data)

def find_feature_files(feat_path, patient_ids):
    """
    Find feature files for each patient
    Note: Multiple .pt files per patient will be automatically loaded as a bag during training
    """
    feature_data = []
    
    for patient_id in patient_ids:
        # Find all feature files for this patient
        pattern = os.path.join(feat_path, f"{patient_id}_*.pt")
        pt_files = glob.glob(pattern)
        
        if not pt_files:
            print(f"Warning: No feature files found for patient {patient_id}")
            continue
        
        # Use the first slide as representative - training will load all slides for this patient
        first_slide = pt_files[0]
        slide_id = os.path.basename(first_slide).replace('.pt', '')
        
        feature_data.append({
            'patient_id': patient_id,
            'slide_id': slide_id,
            'num_slides': len(pt_files)  # Track how many slides this patient has
        })
        
        if len(pt_files) > 1:
            print(f"Patient {patient_id} has {len(pt_files)} slides - will be loaded as bag during training")
    
    return pd.DataFrame(feature_data)

def create_merged_dataset(clinical_df, feature_df):
    """
    Merge clinical and feature data - create one row per patient
    """
    # Group feature files by patient (each patient may have multiple slides)
    patient_slide_counts = feature_df.groupby('patient_id').size()
    print(f"Slide counts per patient: min={patient_slide_counts.min()}, max={patient_slide_counts.max()}, mean={patient_slide_counts.mean():.1f}")
    
    # Create one row per patient with their first slide_id as representative
    # The actual training will load all slides for each patient automatically
    feature_df_unique = feature_df.groupby('patient_id').first().reset_index()
    
    # Merge clinical and feature data
    merged_df = feature_df_unique.merge(clinical_df, on='patient_id', how='inner')
    
    print(f"Total patients after merging: {len(merged_df)}")
    print(f"Unique patients: {merged_df['patient_id'].nunique()}")
    
    # Rename columns for consistency with survival training expectations
    merged_df = merged_df.rename(columns={
        'bcr_months': 'bcr_survival_months', 
        'censorship': 'bcr_censorship',
        'patient_id': 'case_id'  # Expected by WSI survival dataset
    })
    
    return merged_df

def perform_cross_validation_by_patient(merged_df: pd.DataFrame, output_dir: str, n_splits: int = 5) -> None:
    """
    Perform enhanced stratified k-fold cross-validation at patient level for survival data
    Uses composite stratification on PSA, ISUP grade, clinical T-stage, and BCR status
    to prevent the covariate shift issue observed in fold 3
    """
    # Since we now have one row per patient, we can work directly with merged_df
    patient_data = merged_df.copy()
    
    print(f"Total unique patients: {len(patient_data)}")
    print(f"Events: {(patient_data['bcr_censorship'] == 0).sum()}")
    print(f"Censored: {(patient_data['bcr_censorship'] == 1).sum()}")
    
    # Print clinical variable distributions
    print("\nClinical variable distributions:")
    for var in ['psa', 'clinical_t', 'isup_grade', 'age']:
        if var in patient_data.columns:
            values = pd.to_numeric(patient_data[var], errors='coerce').dropna()
            if len(values) > 0:
                print(f"{var}: median={values.median():.1f}, range=[{values.min():.1f}, {values.max():.1f}], missing={patient_data[var].isna().sum()}")
    
    # Create composite stratification variable
    print("\n" + "="*60)
    stratification_var = create_stratification_variable(patient_data)
    
    # Use the composite stratification variable for balanced splits
    print("\n" + "="*60)
    print("Performing stratified k-fold cross-validation...")
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    os.makedirs(output_dir, exist_ok=True)

    # Store fold statistics for validation
    fold_stats = []
    
    for fold, (train_idx, test_idx) in enumerate(skf.split(patient_data, stratification_var)):
        fold_dir = os.path.join(output_dir, f'fold_{fold}')
        os.makedirs(fold_dir, exist_ok=True)
        
        # Get train and test data
        train_df = patient_data.iloc[train_idx]
        test_df = patient_data.iloc[test_idx]
        
        # Prepare output columns - use case_id instead of patient_id for compatibility
        columns = ['slide_id', 'bcr_survival_months', 'bcr_censorship', 'case_id']
        
        # Save splits
        train_df[columns].to_csv(os.path.join(fold_dir, 'train.csv'), index=False)
        test_df[columns].to_csv(os.path.join(fold_dir, 'test.csv'), index=False)
        
        # Calculate detailed statistics for this fold
        train_events = (train_df['bcr_censorship'] == 0).sum()
        train_censored = (train_df['bcr_censorship'] == 1).sum()
        test_events = (test_df['bcr_censorship'] == 0).sum()
        test_censored = (test_df['bcr_censorship'] == 1).sum()
        
        # Calculate clinical statistics for validation
        fold_stat = {
            'fold': fold,
            'train_n': len(train_df),
            'test_n': len(test_df),
            'train_events': train_events,
            'train_censored': train_censored,
            'test_events': test_events,
            'test_censored': test_censored,
            'train_event_rate': train_events / len(train_df),
            'test_event_rate': test_events / len(test_df)
        }
        
        # Add clinical variable medians
        for var in ['psa', 'clinical_t', 'isup_grade', 'age']:
            if var in test_df.columns:
                train_values = pd.to_numeric(train_df[var], errors='coerce').dropna()
                test_values = pd.to_numeric(test_df[var], errors='coerce').dropna()
                
                if len(train_values) > 0:
                    fold_stat[f'train_{var}_median'] = train_values.median()
                if len(test_values) > 0:
                    fold_stat[f'test_{var}_median'] = test_values.median()
        
        fold_stats.append(fold_stat)
        
        print(f"Fold {fold}:")
        print(f"  Train: {len(train_df)} patients (events: {train_events}, censored: {train_censored}, event_rate: {train_events/len(train_df):.3f})")
        print(f"  Test:  {len(test_df)} patients (events: {test_events}, censored: {test_censored}, event_rate: {test_events/len(test_df):.3f})")
        
        # Print key clinical variables for this fold's test set
        for var in ['psa', 'clinical_t', 'isup_grade']:
            if var in test_df.columns:
                test_values = pd.to_numeric(test_df[var], errors='coerce').dropna()
                if len(test_values) > 0:
                    print(f"  Test {var} median: {test_values.median():.1f}")
        print()
    
    # Save fold statistics for analysis
    fold_stats_df = pd.DataFrame(fold_stats)
    fold_stats_df.to_csv(os.path.join(output_dir, 'fold_statistics.csv'), index=False)
    
    # Print summary comparison to validate balanced distributions
    print("="*60)
    print("FOLD BALANCE VALIDATION")
    print("="*60)
    
    print("Event rates across folds:")
    for _, row in fold_stats_df.iterrows():
        print(f"  Fold {row['fold']}: Train {row['train_event_rate']:.3f}, Test {row['test_event_rate']:.3f}")
    
    print(f"\nTest event rate variance: {fold_stats_df['test_event_rate'].var():.6f} (lower is better)")
    
    for var in ['psa', 'clinical_t', 'isup_grade']:
        test_col = f'test_{var}_median'
        if test_col in fold_stats_df.columns:
            values = fold_stats_df[test_col].dropna()
            if len(values) > 0:
                print(f"Test {var} median variance: {values.var():.3f} (lower is better)")
    
    print(f"\nFold statistics saved to: {os.path.join(output_dir, 'fold_statistics.csv')}")
    print("Review these statistics to ensure balanced distributions across folds.")

def create_stratification_variable(df):
    """
    Create a composite stratification variable to ensure balanced folds
    Based on the analysis, we need to stratify on PSA, clinical T-stage, and ISUP grade
    """
    print("Creating stratification variable for balanced fold distribution...")
    
    # Create a copy to work with
    df = df.copy()
    
    # Handle missing values and convert to numeric
    numeric_cols = ['psa', 'clinical_t', 'isup_grade', 'age']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Create risk stratification based on key prognostic factors
    strata = []
    
    for idx, row in df.iterrows():
        stratum_components = []
        
        # 1. PSA stratification (critical factor from analysis)
        psa = row.get('psa')
        if pd.notna(psa):
            if psa <= 6:
                stratum_components.append('PSA_low')
            elif psa <= 10:
                stratum_components.append('PSA_med')
            else:
                stratum_components.append('PSA_high')
        else:
            stratum_components.append('PSA_unk')
        
        # 2. ISUP grade stratification (critical factor from analysis)
        isup = row.get('isup_grade')
        if pd.notna(isup):
            if isup <= 2:
                stratum_components.append('ISUP_low')
            elif isup == 3:
                stratum_components.append('ISUP_med')
            else:
                stratum_components.append('ISUP_high')
        else:
            stratum_components.append('ISUP_unk')
        
        # 3. Clinical T-stage stratification (critical factor from analysis)
        clinical_t = row.get('clinical_t')
        if pd.notna(clinical_t):
            if clinical_t <= 2:
                stratum_components.append('T_early')
            else:
                stratum_components.append('T_advanced')
        else:
            stratum_components.append('T_unk')
        
        # 4. BCR status (for event balance)
        bcr_status = 'event' if row['bcr_censorship'] == 0 else 'censored'
        stratum_components.append(bcr_status)
        
        # Combine into composite stratum
        stratum = '_'.join(stratum_components)
        strata.append(stratum)
    
    df['stratum'] = strata
    
    # Check stratum distribution
    stratum_counts = df['stratum'].value_counts()
    print(f"Created {len(stratum_counts)} unique strata")
    print("Stratum distribution:")
    print(stratum_counts.head(10))
    
    # If we have too many small strata, simplify
    min_stratum_size = max(2, len(df) // 50)  # At least 2% of data per stratum
    small_strata = stratum_counts[stratum_counts < min_stratum_size].index
    
    if len(small_strata) > 0:
        print(f"Simplifying {len(small_strata)} small strata (< {min_stratum_size} patients)")
        
        # Simplify by removing less critical factors for small groups
        simplified_strata = []
        for idx, row in df.iterrows():
            if row['stratum'] in small_strata:
                # Use only the most critical factors for small groups
                components = []
                
                # Keep PSA and ISUP as most critical
                psa = row.get('psa')
                if pd.notna(psa):
                    components.append('PSA_high' if psa > 10 else 'PSA_low_med')
                else:
                    components.append('PSA_unk')
                
                isup = row.get('isup_grade')
                if pd.notna(isup):
                    components.append('ISUP_high' if isup >= 3 else 'ISUP_low_med')
                else:
                    components.append('ISUP_unk')
                
                # BCR status
                bcr_status = 'event' if row['bcr_censorship'] == 0 else 'censored'
                components.append(bcr_status)
                
                simplified_strata.append('_'.join(components))
            else:
                simplified_strata.append(row['stratum'])
        
        df['stratum'] = simplified_strata
        
        # Check final distribution
        final_counts = df['stratum'].value_counts()
        print(f"Final stratification: {len(final_counts)} strata")
        print("Final stratum distribution:")
        print(final_counts)
    
    return df['stratum']

def main():
    parser = argparse.ArgumentParser(description='Create k-fold splits for Task 1 with enhanced stratification')
    parser.add_argument('--clin_dat_path', type=str, required=True,
                       help='Path to directory containing JSON clinical data files')
    parser.add_argument('--feat_path', type=str, required=True,
                       help='Path to directory containing feature (.pt) files')
    parser.add_argument('--output_dir', type=str, 
                       default='/data/temporary/chimera/Baseline_models/Task1_ABMIL/splits_chimera2/',
                       help='Output directory for fold splits')
    parser.add_argument('--n_splits', type=int, default=5,
                       help='Number of folds (default: 5)')
    
    args = parser.parse_args()
    
    print("="*80)
    print("ENHANCED K-FOLD CROSS-VALIDATION FOR SURVIVAL ANALYSIS")
    print("="*80)
    print("This version addresses the covariate shift issue in fold 3 by:")
    print("â€¢ Stratifying on PSA levels (key prognostic factor)")
    print("â€¢ Stratifying on ISUP grade (tumor aggressiveness)")
    print("â€¢ Stratifying on clinical T-stage (disease extent)")
    print("â€¢ Maintaining balanced event rates across folds")
    print("â€¢ Validating distributions across all folds")
    print("="*80)
    
    # Validate paths
    if not os.path.exists(args.clin_dat_path):
        print(f"Error: Clinical data path does not exist: {args.clin_dat_path}")
        return
    
    if not os.path.exists(args.feat_path):
        print(f"Error: Feature path does not exist: {args.feat_path}")
        return
    
    print("Loading clinical data...")
    clinical_df = load_clinical_data(args.clin_dat_path)
    
    if clinical_df.empty:
        print("Error: No valid clinical data found")
        return
    
    print(f"Loaded clinical data for {len(clinical_df)} patients")
    
    print("Finding feature files...")
    feature_df = find_feature_files(args.feat_path, clinical_df['patient_id'].tolist())
    
    if feature_df.empty:
        print("Error: No feature files found")
        return
    
    print(f"Found feature files for {feature_df['patient_id'].nunique()} patients")
    
    print("Creating merged dataset...")
    merged_df = create_merged_dataset(clinical_df, feature_df)
    
    if merged_df.empty:
        print("Error: Merged dataset is empty")
        return
    
    print(f"Merged dataset columns: {list(merged_df.columns)}")
    print(f"Sample of merged data:")
    print(merged_df.head())
    
    print("Performing enhanced stratified cross-validation...")
    try:
        perform_cross_validation_by_patient(merged_df, args.output_dir, args.n_splits)
    except Exception as e:
        print(f"Error during cross-validation: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("="*80)
    print("âœ… ENHANCED CROSS-VALIDATION COMPLETE!")
    print("="*80)
    print("Next steps:")
    print("1. Review the fold_statistics.csv file to verify balanced distributions")
    print("2. Re-run your survival analysis with the new folds")
    print("3. Compare model performance across folds - they should be more consistent")
    print("4. The fold 3 covariate shift issue should be resolved!")
    print("="*80)

if __name__ == "__main__":
    main()


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/__init__.py


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/main_survival.py
"""
Main entry point for survival downstream tasks
"""

from __future__ import print_function

import argparse
import pdb
import os
from os.path import join as j_
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
# internal imports
from utils.file_utils import save_pkl
from utils.utils import (seed_torch, array2list, merge_dict, read_splits, 
                         parse_model_name, get_current_time, extract_patching_info)

from trainer import train
from wsi_datasets import WSISurvivalDataset
# pytorch imports
import torch
from torch.utils.data import DataLoader

import pandas as pd
import json


def build_datasets(csv_splits, model_type, batch_size=1, num_workers=2, train_kwargs={}, val_kwargs={}):
    """
    Construct dataloaders from the data splits
    """
    dataset_splits = {}
    label_bins = None
    
    # Initialize clinical data processor if path is provided
    clinical_processor = None
    if 'clinical_data_path' in train_kwargs and train_kwargs['clinical_data_path'] is not None:
        from wsi_datasets.clinical_processor import ClinicalDataProcessor
        print(f"Initializing clinical data processor with path: {train_kwargs['clinical_data_path']}")
        clinical_processor = ClinicalDataProcessor(clinical_data_path=train_kwargs['clinical_data_path'])
        
        # Fit the processor on the training set case IDs
        if 'train' in csv_splits:
            print("Fitting clinical data processor on training set...")
            train_case_ids = csv_splits['train']['case_id'].unique().tolist()
            clinical_processor.fit(train_case_ids)
            
            # Update clinical dimension in all dataset kwargs
            train_kwargs['clinical_processor'] = clinical_processor
            val_kwargs['clinical_processor'] = clinical_processor
    
    if clinical_processor is not None:
        # Add clinical processor to args for model initialization
        args.clinical_processor = clinical_processor
        
        # Set the clinical dimension in args based on processor output dimension
        args.clinical_dim = clinical_processor.output_dim
        print(f"Clinical processor initialized with output dimension: {args.clinical_dim}")
    
    for k in csv_splits.keys(): # ['train', 'val', 'test']
        df = csv_splits[k]
        dataset_kwargs = train_kwargs.copy() if (k == 'train') else val_kwargs.copy()
        dataset_kwargs['label_bins'] = label_bins
        dataset = WSISurvivalDataset(df, **dataset_kwargs)

        # If prototype methods, each WSI will have same feature bag dimension and is batchable
        # Otherwise, we need to use batch size of 1 to accommodate to different bag size for each WSI.
        # Alternatively, we can sample same number of patch features per WSI to have larger batch.
  
        batch_size = batch_size if dataset_kwargs.get('bag_size', -1) > 0 else 1

        dataloader = DataLoader(dataset, batch_size=batch_size,
                                shuffle=dataset_kwargs['shuffle'], num_workers=num_workers)
        dataset_splits[k] = dataloader
        print(f'split: {k}, n: {len(dataset)}')
        if (args.loss_fn == 'nll') and (k == 'train'):
            label_bins = dataset.get_label_bins()
    return dataset_splits


def main(args):
    if args.train_bag_size == -1:
        args.train_bag_size = args.bag_size
    if args.val_bag_size == -1:
        args.val_bag_size = args.bag_size
    if args.loss_fn != 'nll':
        args.n_label_bins = 0

    censorship_col = args.target_col.split('_')[0] + '_censorship'

    train_kwargs = dict(data_source=args.data_source,
                        survival_time_col=args.target_col,
                        censorship_col=censorship_col,
                        n_label_bins=args.n_label_bins,
                        label_bins=None,
                        bag_size=args.train_bag_size,
                        shuffle=True,
                        mri_feature_path=args.mri_feature_path,
                        clinical_data_path=args.clinical_data_path,
                        )

    # use the whole bag at test time
    val_kwargs = dict(data_source=args.data_source,
                      survival_time_col=args.target_col,
                      censorship_col=censorship_col,
                      n_label_bins=args.n_label_bins,
                      label_bins=None,
                      bag_size=args.val_bag_size,
                      shuffle=False,
                      mri_feature_path=args.mri_feature_path,
                      clinical_data_path=args.clinical_data_path,
                      )

    all_results, all_dumps = {}, {}

    # Cross-validation
    seed_torch(args.seed)
    csv_splits = read_splits(args)
    print('successfully read splits for: ', list(csv_splits.keys()))
    dataset_splits = build_datasets(csv_splits, 
                                    model_type=args.model_type,
                                    batch_size=args.batch_size,
                                    num_workers=args.num_workers,
                                    train_kwargs=train_kwargs,
                                    val_kwargs=val_kwargs)

    fold_results, fold_dumps = train(dataset_splits, args, mode='survival')

    for split, split_results in fold_results.items():
        all_results[split] = merge_dict({}, split_results) if (split not in all_results.keys()) else merge_dict(all_results[split], split_results)
        save_pkl(j_(args.results_dir, f'{split}_results.pkl'), fold_dumps[split]) # saves per-split, per-fold results to pkl
    
    final_dict = {}
    for split, split_results in all_results.items():
        final_dict.update({f'{metric}_{split}': array2list(val) for metric, val in split_results.items()})
    
    final_df = pd.DataFrame(final_dict)
    save_name = 'summary.csv'
    final_df.to_csv(j_(args.results_dir, save_name), index=False)
    with open(j_(args.results_dir, save_name + '.json'), 'w') as f:
        f.write(json.dumps(final_dict, sort_keys=True, indent=4))
    
    dump_path = j_(args.results_dir, 'all_dumps.h5')
    save_pkl(dump_path, fold_dumps)
    
    # Also save a CSV version for easy viewing
    try:
        csv_dump_data = []
        for split, dumps in fold_dumps.items():
            for key, values in dumps.items():
                if isinstance(values, (list, tuple)):
                    for i, val in enumerate(values):
                        csv_dump_data.append({
                            'split': split,
                            'metric': key,
                            'index': i,
                            'value': val
                        })
                else:
                    csv_dump_data.append({
                        'split': split,
                        'metric': key,
                        'index': 0,
                        'value': values
                    })
        
        if csv_dump_data:
            csv_dump_df = pd.DataFrame(csv_dump_data)
            csv_dump_df.to_csv(j_(args.results_dir, 'all_dumps.csv'), index=False)
            print(f"Saved detailed results to: {j_(args.results_dir, 'all_dumps.csv')}")
    except Exception as e:
        print(f"Warning: Could not save CSV dump: {e}")

    return final_dict

# Generic training settings
parser = argparse.ArgumentParser(description='Configurations for WSI Training')
### optimizer settings ###
parser.add_argument('--max_epochs', type=int, default=10,
                    help='maximum number of epochs to train (default: 20)')
parser.add_argument('--lr', type=float, default=1e-4,
                    help='learning rate')
parser.add_argument('--wd', type=float, default=1e-5,
                    help='weight decay')
parser.add_argument('--accum_steps', type=int, default=4,
                    help='grad accumulation steps')
parser.add_argument('--opt', type=str, default='adamW',
                    choices=['adamW', 'sgd', 'RAdam'])
parser.add_argument('--lr_scheduler', type=str,
                    choices=['cosine', 'linear', 'constant'], default='constant')
parser.add_argument('--warmup_steps', type=int,
                    default=-1, help='warmup iterations')
parser.add_argument('--warmup_epochs', type=int,
                    default=-1, help='warmup epochs')
parser.add_argument('--batch_size', type=int, default=1)

### misc ###
parser.add_argument('--print_every', default=1,
                    type=int, help='how often to print')
parser.add_argument('--seed', type=int, default=1,
                    help='random seed for reproducible experiment (default: 1)')
parser.add_argument('--num_workers', type=int, default=2)

### Earlystopper args ###
parser.add_argument('--early_stopping', type=int,
                    default=0, help='enable early stopping')
parser.add_argument('--es_min_epochs', type=int, default=3,
                    help='early stopping min epochs')
parser.add_argument('--es_patience', type=int, default=5,
                    help='early stopping min patience')
parser.add_argument('--es_metric', type=str, default='loss',
                    help='early stopping metric')

### model args ###
parser.add_argument('--model_type', type=str, choices=['ABMIL'], default='ABMIL',
                    help='type of model')
parser.add_argument('--emb_model_type', type=str, default='LinEmb_LR')
parser.add_argument('--ot_eps', default=0.1, type=float,
                    help='Strength for entropic constraint regularization for OT')
parser.add_argument('--model_config', type=str,
                    default='ABMIL_default', help="name of model config file")
parser.add_argument('--n_fc_layers', type=int)
parser.add_argument('--em_iter', type=int)
parser.add_argument('--tau', type=float)
parser.add_argument('--out_type', type=str, default='param_cat')


parser.add_argument('--in_dim', default=1024, type=int,
                    help='dim of input features')
parser.add_argument('--in_dropout', default=0.1, type=float,
                    help='Probability of dropping out input features.')
parser.add_argument('--mri_feature_path', type=str, default=None,
                    help='Path to directory containing pre-created MRI features (.pt files)')
parser.add_argument('--clinical_data_path', type=str, default=None,
                    help='Path to directory containing clinical data files (.json files)')
parser.add_argument('--bag_size', type=int, default=-1)
parser.add_argument('--train_bag_size', type=int, default=-1)
parser.add_argument('--val_bag_size', type=int, default=-1)
parser.add_argument('--loss_fn', type=str, default='cox', choices=['nll', 'cox', 'sumo', 'ipcwls', 'rank'],
                    help='which loss function to use')
parser.add_argument('--nll_alpha', type=float, default=0,
                    help='Balance between censored / uncensored loss')

# experiment task / label args ###
parser.add_argument('--exp_code', type=str, default=None,
                    help='experiment code for saving results')
parser.add_argument('--task', type=str, default='MM_bcr_survival_task2')
parser.add_argument('--target_col', type=str, default='bcr_survival_months')
parser.add_argument('--n_label_bins', type=int, default=4,
                    help='number of bins for event time discretization')

# dataset / split args ###
parser.add_argument('--data_source', type=str, default=None,
                    help='manually specify the data source')
parser.add_argument('--split_dir', type=str, default=None,
                    help='manually specify the set of splits to use')
parser.add_argument('--split_names', type=str, default='train,val,test',
                    help='delimited list for specifying names within each split')
parser.add_argument('--overwrite', action='store_true', default=False,
                    help='overwrite existing results')

# logging args ###
parser.add_argument('--results_dir', default='./results',
                    help='results directory (default: ./results)')
parser.add_argument('--tags', nargs='+', type=str, default=None,
                    help='tags for logging')
args = parser.parse_args()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if __name__ == "__main__":

    print('task: ', args.task)
    args.split_dir = j_('splits', args.split_dir)
    print('split_dir: ', args.split_dir)
    split_num = args.split_dir.split('/')[-1].split('_')
    args.split_name_clean = split_num[0]
    if len(split_num) > 1:
        args.split_k = int(split_num[1])
    else:
        args.split_k = 0


    ### Allows you to pass in multiple data sources (separated by comma). If single data source, no change.
    args.data_source = [src for src in args.data_source.split(',')]
    check_params_same = []
    for src in args.data_source: 
        ### assert data source exists + extract feature name ###
        print('data source: ', src)
        assert os.path.isdir(src), f"data source must be a directory: {src} invalid"

        ### parse patching info ###
        feat_name = os.path.basename(src)
        try:
            patching_info = extract_patching_info(os.path.dirname(src))
            if patching_info is None or len(patching_info) != 2:
                raise ValueError("Invalid patching info")
            mag, patch_size = patching_info
            if (mag < 0 or patch_size < 0):
                raise ValueError("Negative patching values")
        except:
            # Set default values if parsing fails
            print(f"Warning: Could not parse patching info from {src}, using defaults")
            mag, patch_size = 20, 256  # Default magnification and patch size
        check_params_same.append([feat_name, mag, patch_size])

        #### parse model name ####
        parsed = parse_model_name(feat_name) 
        parsed.update({'patch_mag': mag, 'patch_size': patch_size})
    
    try:
        check_params_same = pd.DataFrame(check_params_same, columns=['feats_name', 'mag', 'patch_size'])
        assert check_params_same.drop(['feats_name'],axis=1).drop_duplicates().shape[0] == 1
        print("All data sources have the same feature extraction parameters.")
    except:
        print("Data sources do not share the same feature extraction parameters. Exiting...")
        sys.exit()
        
    ### Updated parsed mdoel parameters in args.Namespace ###
    for key, val in parsed.items():
        setattr(args, key, val)
    
    ### setup results dir ###
    if args.exp_code is None:
        if args.model_config == 'PANTHER_default':
            exp_code = f"{args.split_name_clean}::{args.model_config}+{args.emb_model_type}::{args.loss_fn}::{feat_name}"
        else:
            exp_code = f"{args.split_name_clean}::{args.model_config}::{feat_name}"
    else:
        pass
    
    args.results_dir = j_(args.results_dir, 
                          args.task, 
                          f'k={args.split_k}', 
                          str(exp_code), 
                          str(exp_code)+f"::{get_current_time()}")

    os.makedirs(args.results_dir, exist_ok=True)

    print("\n################### Settings ###################")
    for key, val in vars(args).items():
        print("{}:  {}".format(key, val))

    with open(j_(args.results_dir, 'config.json'), 'w') as f:
        f.write(json.dumps(vars(args), sort_keys=True, indent=4))

    #### train ####
    results = main(args)

    print("FINISHED!\n\n\n")

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/main_classification.py
"""
Main entry point for classification downstream tasks
"""

from __future__ import print_function

import argparse
import pdb
import os
from os.path import join as j_
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# internal imports
from utils.file_utils import save_pkl
from utils.utils import (seed_torch, array2list, merge_dict, read_splits, 
                         parse_model_name, get_current_time,
                         extract_patching_info)

from trainer import train
from wsi_datasets import WSIClassificationDataset
from data_factory import tasks, label_dicts

import torch
from torch.utils.data import DataLoader, sampler

import pandas as pd
import numpy as np
import json


PROTO_MODELS = ['PANTHER', 'OT', 'H2T', 'ProtoCount']

def build_sampler(dataset, sampler_type=None):
    data_sampler = None
    if sampler_type is None:
        return data_sampler
    
    assert sampler_type in ['weighted', 'random', 'sequential']
    if sampler_type == 'weighted':
        labels = dataset.get_labels(np.arange(len(dataset)), apply_transform=True)
        uniques, counts = np.unique(labels, return_counts=True)
        weights = {uniques[i]: 1. / counts[i] for i in range(len(uniques))}
        samples_weight = np.array([weights[t] for t in labels])
        samples_weight = torch.from_numpy(samples_weight)
        data_sampler = sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))
    elif sampler_type == 'random':
        data_sampler = sampler.RandomSampler(dataset)
    elif sampler_type == 'sequential':
        data_sampler = sampler.SequentialSampler(dataset)

    return data_sampler

def build_datasets(csv_splits, model_type, batch_size=1, num_workers=2,
                   train_kwargs={}, val_kwargs={}, sampler_types={'train': 'random',
                                                                  'val': 'sequential',
                                                                  'test': 'sequential'}):
    """
    Construct dataloaders from the data splits
    """
    dataset_splits = {}
    for k in csv_splits.keys(): # ['train', 'val', 'test']
        print("\nSPLIT: ", k)
        df = csv_splits[k]
        dataset_kwargs = train_kwargs.copy() if (k == 'train') else val_kwargs.copy()
        print(dataset_kwargs)
        if k == 'test_nlst':
            dataset_kwargs['sample_col'] = 'case_id'
        dataset = WSIClassificationDataset(df, **dataset_kwargs)
        data_sampler = build_sampler(dataset, sampler_type=sampler_types.get(k, 'sequential'))

        # If prototype methods, each WSI will have same feature bag dimension and is batchable
        # Otherwise, we need to use batch size of 1 to accommodate to different bag size for each WSI.
        # Alternatively, we can sample same number of patch features per WSI to have larger batch.
        if model_type not in PROTO_MODELS:
            batch_size = batch_size if dataset_kwargs.get('bag_size', -1) > 0 else 1

        dataloader = DataLoader(dataset, batch_size=batch_size, sampler=data_sampler, num_workers=num_workers)
        dataset_splits[k] = dataloader
        print(f'split: {k}, n: {len(dataset)}')
    return dataset_splits


def main(args):
    if args.train_bag_size == -1:
        args.train_bag_size = args.bag_size
    if args.val_bag_size == -1:
        args.val_bag_size = args.bag_size

    sampler_types = {'train': 'sequential',
                'val': 'sequential',
                'test': 'sequential'}
    train_kwargs = dict(data_source=args.data_source,
                          label_map=args.label_map,
                          target_col=args.target_col,
                          bag_size=args.train_bag_size,
                          shuffle=True)
    
    # use the whole bag at test time
    val_kwargs = dict(data_source=args.data_source,
                          label_map=args.label_map,
                          target_col=args.target_col,
                          bag_size=args.val_bag_size)

    all_results, all_dumps = {}, {}

    # Cross-validation
    seed_torch(args.seed)
    csv_splits = read_splits(args)
    print(csv_splits)
    print('successfully read splits for: ', list(csv_splits.keys()))
    dataset_splits = build_datasets(csv_splits, 
                                    model_type=args.model_type,
                                    batch_size=args.batch_size,
                                    num_workers=args.num_workers,
                                    sampler_types=sampler_types,
                                    train_kwargs=train_kwargs,
                                    val_kwargs=val_kwargs)

    fold_results, fold_dumps = train(dataset_splits, args, mode='classification')

    for split, split_results in fold_results.items():
        all_results[split] = merge_dict({}, split_results) if (split not in all_results.keys()) else merge_dict(all_results[split], split_results)
        save_pkl(j_(args.results_dir, f'{split}_results.pkl'), fold_dumps[split]) # saves per-split, per-fold results to pkl
    
    final_dict = {}
    for split, split_results in all_results.items():
        final_dict.update({f'{metric}_{split}': array2list(val) for metric, val in split_results.items()})
    final_df = pd.DataFrame(final_dict)
    save_name = 'summary.csv'
    final_df.to_csv(j_(args.results_dir, save_name), index=False)
    with open(j_(args.results_dir, save_name + '.json'), 'w') as f:
        f.write(json.dumps(final_dict, sort_keys=True, indent=4))
    
    dump_path = j_(args.results_dir, 'all_dumps.h5')
    fold_dumps.update({'labels': np.array(list(args.label_map.keys()), dtype=np.object_)})
    save_pkl(dump_path, fold_dumps)

    return final_dict


# Generic training settings
parser = argparse.ArgumentParser(description='Configurations for WSI Training')
### optimizer settings ###
parser.add_argument('--max_epochs', type=int, default=10,
                    help='maximum number of epochs to train (default: 20)')
parser.add_argument('--lr', type=float, default=1e-4,
                    help='learning rate')
parser.add_argument('--wd', type=float, default=1e-5,
                    help='weight decay')
parser.add_argument('--accum_steps', type=int, default=1,
                    help='grad accumulation steps')
parser.add_argument('--opt', type=str,
                    choices=['adamW', 'sgd'], default='adamW')
parser.add_argument('--lr_scheduler', type=str,
                    choices=['cosine', 'linear', 'constant'], default='constant')
parser.add_argument('--warmup_steps', type=int,
                    default=-1, help='warmup iterations')
parser.add_argument('--warmup_epochs', type=int,
                    default=-1, help='warmup epochs')
parser.add_argument('--batch_size', type=int, default=1)

### misc ###
parser.add_argument('--print_every', default=100,
                    type=int, help='how often to print')
parser.add_argument('--seed', type=int, default=1,
                    help='random seed for reproducible experiment (default: 1)')
parser.add_argument('--num_workers', type=int, default=2)

### Earlystopper args ###
parser.add_argument('--early_stopping', action='store_true',
                    default=False, help='enable early stopping')
parser.add_argument('--es_min_epochs', type=int, default=15,
                    help='early stopping min epochs')
parser.add_argument('--es_patience', type=int, default=10,
                    help='early stopping min patience')
parser.add_argument('--es_metric', type=str, default='loss',
                    help='early stopping metric')

##
# model / loss fn args ###
parser.add_argument('--model_type', type=str, choices=['H2T', 'ABMIL', 'TransMIL', 'SumMIL', 'OT', 'PANTHER', 'ProtoCount', 'DeepAttnMIL', 'ILRA'],
                    default='ABMIL',
                    help='type of model')
parser.add_argument('--emb_model_type', type=str, default='LinEmb_LR')
parser.add_argument('--ot_eps', default=0.1, type=float,
                    help='Strength for entropic constraint regularization for OT')
parser.add_argument('--model_config', type=str,
                    default='ABMIL_default', help="name of model config file")

parser.add_argument('--in_dim', default=768, type=int,
                    help='dim of input features')
parser.add_argument('--in_dropout', default=0.0, type=float,
                    help='Probability of dropping out input features.')
parser.add_argument('--bag_size', type=int, default=-1)
parser.add_argument('--train_bag_size', type=int, default=-1)
parser.add_argument('--val_bag_size', type=int, default=-1)

parser.add_argument('--train_sampler', type=str, default='random', 
                    choices=['random', 'weighted', 'sequential'])
parser.add_argument('--n_fc_layers', type=int)
parser.add_argument('--em_iter', type=int)
parser.add_argument('--tau', type=float)
parser.add_argument('--out_type', type=str, default='param_cat')

# Prototype related
parser.add_argument('--load_proto', action='store_true', default=False)
parser.add_argument('--proto_path', type=str, default='.')
parser.add_argument('--fix_proto', action='store_true', default=False)
parser.add_argument('--n_proto', type=int)

# experiment task / label args ###
parser.add_argument('--exp_code', type=str,
                    help='experiment code for saving results')
parser.add_argument('--task', type=str, choices=tasks)
parser.add_argument('--loss_fn', type=str, default=None)
parser.add_argument('--target_col', type=str, default='label')

# dataset / split args ###
parser.add_argument('--data_source', type=str, default=None,
                    help='manually specify the data source')
parser.add_argument('--split_dir', type=str, default=None,
                    help='manually specify the set of splits to use')
parser.add_argument('--split_names', type=str, default='train,val,test',
                    help='delimited list for specifying names within each split')
parser.add_argument('--overwrite', action='store_true', default=False,
                    help='overwrite existing results')
# logging args ###
parser.add_argument('--results_dir', default='./results',
                    help='results directory (default: ./results)')
parser.add_argument('--tags', nargs='+', type=str, default=None,
                    help='tags for logging')
args = parser.parse_args()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if __name__ == "__main__":

    args.label_map = label_dicts[args.task]
    print('label map: ', args.label_map)
    args.n_classes = len(set(list(args.label_map.values())))
    print('task: ', args.task)
    args.split_dir = j_('splits', args.split_dir)
    print('split_dir: ', args.split_dir)
    
    split_num = args.split_dir.split('/')[2].split('_k=')
    args.split_name_clean = args.split_dir.split('/')[2].split('_k=')[0]
    if len(split_num) > 1:
        args.split_k = int(split_num[1])
    else:
        args.split_k = 0

    print(args.proto_path)
    if os.path.isfile(args.proto_path):
        args.proto_fname = '/'.join(args.proto_path.split('/')[-2:])

    ### Allows you to pass in multiple data sources (separated by comma). If single data source, no change.
    args.data_source = [src for src in args.data_source.split(',')]
    check_params_same = []
    for src in args.data_source: 
        ### assert data source exists + extract feature name ###
        print('data source: ', src)
        assert os.path.isdir(src), f"data source must be a directory: {src} invalid"

        ### parse patching info ### 
        feat_name = os.path.basename(src)
        print(feat_name)
        #mag, patch_size = extract_patching_info(os.path.dirname(src))       
        mag, patch_size = 0,0
        if (mag < 0 or patch_size < 0):
            raise ValueError(f"invalid patching info parsed for {src}")
        check_params_same.append([feat_name, mag, patch_size])
    
    try:
        check_params_same = pd.DataFrame(check_params_same, columns=['feats_name', 'mag', 'patch_size'])
        print(check_params_same.to_string())
        assert check_params_same.drop(['feats_name'],axis=1).drop_duplicates().shape[0] == 1
        print("All data sources have the same feature extraction parameters.")
    except:
        print("Data sources do not share the same feature extraction parameters. Exiting...")
        sys.exit()
        
    ### Updated parsed mdoel parameters in args.Namespace ###
    #### parse patching info ####
    mag, patch_size = 0,0
    
    #### parse model name ####
    parsed = parse_model_name(feat_name) 
    parsed.update({'patch_mag': mag, 'patch_size': patch_size, 'feat_names': sorted(list(set(check_params_same['feats_name'].tolist())))})
    for key, val in parsed.items():
        setattr(args, key, val)
     
    ### setup results dir ###
    if args.exp_code is None:
        if args.model_config == 'PANTHER_default':
            exp_code = f"{args.split_name_clean}::{args.model_config}+{args.emb_model_type}::{args.loss_fn}::{feat_name}"
        else:
            exp_code = f"{args.split_name_clean}::{args.model_config}::{feat_name}"
    else:
        pass
    
    args.results_dir = j_(args.results_dir, 
                          args.task, 
                          f'k={args.split_k}', 
                          str(exp_code), 
                          str(exp_code)+f"::{get_current_time()}")

    os.makedirs(args.results_dir, exist_ok=True)

    print("\n################### Settings ###################")
    for key, val in vars(args).items():
        print("{}:  {}".format(key, val))

    with open(j_(args.results_dir, 'config.json'), 'w') as f:
        f.write(json.dumps(vars(args), sort_keys=True, indent=4))

    #### train ####
    results = main(args)

    print("FINISHED!\n\n\n")


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/inference.py
import torch
import argparse
from mil_models import create_downstream_model

# --- Configuration ---
# Path to the saved checkpoint file
path_to_checkpoint = 'results/your_model_run/s_checkpoint.pth'
# Path to your new data (e.g., a .pt file with features)
path_to_new_data = 'path/to/your/new_data_features.pt'
# Specify the mode ('survival' or 'classification')
mode = 'survival'

# --- 1. Re-create the Model Architecture ---

# Load the entire checkpoint dictionary
checkpoint = torch.load(path_to_checkpoint, map_location=torch.device('cpu'))

# Get the original training configuration from the checkpoint
config_args = argparse.Namespace(**checkpoint['config'])
print("Re-creating model with the following configuration:")
print(config_args)

# Create a model instance with the same architecture
# [cite_start]The create_downstream_model function builds the model based on the config [cite: 742, 845]
model = create_downstream_model(args=config_args, mode=mode)
print("\nModel architecture created successfully.")


# --- 2. Load the Saved Weights ---

# [cite_start]The saved weights are in the 'model' key of the checkpoint dictionary [cite: 395]
saved_weights = checkpoint['model']

# Load the weights into the model
model.load_state_dict(saved_weights)
print("Saved weights loaded into the model.")

# Set the model to evaluation mode (very important!)
model.eval()
print("Model set to evaluation mode.")


# --- 3. Perform the Forward Pass ---

# Load and prepare your new data as a tensor
# The shape should be (1, N, D) where N is number of instances and D is feature dimension
input_tensor = torch.load(path_to_new_data).unsqueeze(0)
print(f"\nLoaded new data with shape: {input_tensor.shape}")

# Perform inference without calculating gradients
with torch.no_grad():
    # [cite_start]Use the forward_no_loss method for inference [cite: 669, 764]
    # It takes the input tensor and optional embeddings/features
    output_dict = model.forward_no_loss(
        h=input_tensor,
        additional_embeddings=None,  # Provide if your model uses them
        clinical_features=None       # Provide if your model uses them
    )

    # The raw output from the model's final layer
    logits = output_dict['logits']

print(f"Inference complete. Output logits: {logits.numpy()}")

# You can now use these logits as needed (e.g., apply softmax for classification probabilities)

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/trainer.py
import os
from os.path import join as j_
import pdb
import torch.nn.functional as F

import numpy as np
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter

try:
    from sksurv.metrics import concordance_index_censored
except ImportError:
    print('scikit-survival not installed. Exiting...')
    raise

from sklearn.metrics import (roc_auc_score, balanced_accuracy_score,
                             cohen_kappa_score, classification_report, accuracy_score)

from mil_models import create_downstream_model
from utils.losses import NLLSurvLoss, CoxLoss, SurvRankingLoss
from utils.utils import (EarlyStopping, save_checkpoint, AverageMeter,
                         get_optim, print_network, get_lr_scheduler)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


## GENERIC
def log_dict_tensorboard(writer, results, str_prefix, step=0, verbose=False):
    for k, v in results.items():
        if verbose: print(f'{k}: {v:.4f}')
        writer.add_scalar(f'{str_prefix}{k}', v, step)
    return writer


def train(datasets, args, mode='classification'):
    """
    Train for a single fold for classification or suvival
    """
    
    writer_dir = args.results_dir
    if not os.path.isdir(writer_dir):
        os.mkdir(writer_dir)
    writer = SummaryWriter(writer_dir, flush_secs=15)

    assert args.es_metric == 'loss'
    if mode == 'classification':
        loss_fn = nn.CrossEntropyLoss()



    elif mode == 'survival':
        if args.loss_fn == 'nll':
            loss_fn = NLLSurvLoss(alpha=args.nll_alpha)
        elif args.loss_fn == 'cox':
            loss_fn = CoxLoss()
        elif args.loss_fn == 'rank':
            loss_fn = SurvRankingLoss()

    print('\nInit Model...', end=' ')


    model = create_downstream_model(args, mode=mode)
    model.to(device)
    print('Done!')
    print_network(model)

    print('\nInit optimizer ...', end=' ')
    optimizer = get_optim(model=model, args=args)
    print(datasets)
    lr_scheduler = get_lr_scheduler(args, optimizer, datasets['train'])

    if args.early_stopping:
        print('\nSetup EarlyStopping...', end=' ')
        early_stopper = EarlyStopping(save_dir=args.results_dir,
                                      patience=args.es_patience,
                                      min_stop_epoch=args.es_min_epochs,
                                      better='min' if args.es_metric == 'loss' else 'max',
                                      verbose=True)
    else:
        print('\nNo EarlyStopping...', end=' ')
        early_stopper = None
    
    #####################
    # The training loop #
    #####################
    for epoch in range(args.max_epochs):
        step_log = {'epoch': epoch, 'samples_seen': (epoch + 1) * len(datasets['train'].dataset)}

        ### Train Loop
        print('#' * 10, f'TRAIN Epoch: {epoch}', '#' * 10)
        if mode == 'classification':
            train_results = train_loop_classification(model, datasets['train'], optimizer, lr_scheduler, loss_fn,
                                                      in_dropout=args.in_dropout, print_every=args.print_every,
                                                      accum_steps=args.accum_steps)
        elif mode == 'survival':
            train_results = train_loop_survival(model, datasets['train'], optimizer, lr_scheduler, loss_fn,
                                                in_dropout=args.in_dropout, print_every=args.print_every,
                                                accum_steps=args.accum_steps)

        writer = log_dict_tensorboard(writer, train_results, 'train/', epoch)

        ### Validation Loop (Optional)
        if 'val' in datasets.keys():
            print('#' * 11, f'VAL Epoch: {epoch}', '#' * 11)
            if mode == 'classification':
                # During training validation, show per-batch progress
                val_results, _ = validate_classification(model, datasets['val'], loss_fn,
                                                         print_every=args.print_every, verbose=True, show_batch_progress=True)
            elif mode == 'survival':
                # During training validation, show per-batch progress
                val_results, _ = validate_survival(model, datasets['val'], loss_fn,
                                                   print_every=args.print_every, verbose=True, show_batch_progress=True)

            writer = log_dict_tensorboard(writer, val_results, 'val/', epoch)

            ### Check Early Stopping (Optional)
            if early_stopper is not None:
                if args.es_metric == 'loss':
                    score = val_results['loss']

                else:
                    raise NotImplementedError
                save_ckpt_kwargs = dict(config=vars(args),
                                        epoch=epoch,
                                        model=model,
                                        score=score,
                                        fname=f's_checkpoint.pth')
                stop = early_stopper(epoch, score, save_checkpoint, save_ckpt_kwargs)
                if stop:
                    break
        print('#' * (22 + len(f'TRAIN Epoch: {epoch}')), '\n')

    ### End of epoch: Load in the best model (or save the latest model with not early stopping)
    if args.early_stopping:
        model.load_state_dict(torch.load(j_(args.results_dir, f"s_checkpoint.pth"))['model'])
    else:
        torch.save(model.state_dict(), j_(args.results_dir, f"s_checkpoint.pth"))

    ### End of epoch: Evaluate on val and test set
    results, dumps = {}, {}
    for k, loader in datasets.items():
        print(f'End of training. Evaluating on Split {k.upper()}...:')
        if mode == 'classification':
            # Suppress per-batch output during final evaluation, but show final summary
            results[k], dumps[k] = validate_classification(model, loader, loss_fn, print_every=args.print_every,
                                                           dump_results=True, verbose=1, show_batch_progress=False)
        elif mode == 'survival':
            # Suppress per-batch output during final evaluation, but show final summary
            results[k], dumps[k] = validate_survival(model, loader, loss_fn, print_every=args.print_every,
                                                     dump_results=True, verbose=1, show_batch_progress=False)

        # Log all results including train split
        log_dict_tensorboard(writer, results[k], f'final/{k}_', 0, verbose=True)

    writer.close()
    return results, dumps


## CLASSIFICATION
def train_loop_classification(model, loader, optimizer, lr_scheduler, loss_fn=None,
                              in_dropout=0.0, print_every=50,
                              accum_steps=1):
    model.train()
    meters = {'bag_size': AverageMeter(), 'cls_acc': AverageMeter()}
    bag_size_meter = meters['bag_size']
    acc_meter = meters['cls_acc']

    #import pdb; pdb.set_trace()
    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(torch.long).to(device)
        if len(label.shape) == 2 and label.shape[1] == 1:
            label = label.squeeze(dim=-1)

        if in_dropout:
            data = F.dropout(data, p=in_dropout)
        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        model_kwargs = {'attn_mask': attn_mask, 'label': label, 'loss_fn': loss_fn}
        out, log_dict = model(data, model_kwargs)

        # Get loss + backprop
        loss = out['loss']
        loss = loss / accum_steps
        loss.backward()
        if (batch_idx + 1) % accum_steps == 0:
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

        # End of iteration classification-specific metrics to calculate / log
        logits = out['logits']
        acc = (label == logits.argmax(dim=-1)).float().mean()

        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            meters[key].update(val, n=len(data))

        acc_meter.update(acc.item(), n=len(data))
        bag_size_meter.update(data.size(1), n=len(data))

        if ((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch classification-specific metrics to calculate / log
    results = {k: meter.avg for k, meter in meters.items()}
    results['lr'] = optimizer.param_groups[0]['lr']
    return results

@torch.no_grad()
def validate_classification(model, loader,
                            loss_fn=None,
                            print_every=50,
                            dump_results=False,
                            verbose=1,
                            show_batch_progress=True):
    model.eval()
    meters = {'bag_size': AverageMeter(), 'cls_acc': AverageMeter()}
    acc_meter = meters['cls_acc']
    bag_size_meter = meters['bag_size']
    all_probs = []
    all_labels = []
        
    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(torch.long).to(device)
        if len(label.shape) == 2 and label.shape[1] == 1:
            label = label.squeeze(dim=-1)

        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        model_kwargs = {'attn_mask': attn_mask, 'label': label, 'loss_fn': loss_fn}
        out, log_dict = model(data, model_kwargs)
        

        # End of iteration classification-specific metrics to calculate / log
        logits = out['logits']
        acc = (label == logits.argmax(dim=-1)).float().mean()
        acc_meter.update(acc.item(), n=len(data))
        bag_size_meter.update(data.size(1), n=len(data))
        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            meters[key].update(val, n=len(data))
        all_probs.append(torch.softmax(logits, dim=-1).cpu().numpy())
        all_labels.append(label.cpu().numpy())

        # Only show per-batch progress if show_batch_progress is True (i.e., during training validation)
        if verbose and show_batch_progress and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch classification-specific metrics to calculate / log
    n_classes = logits.size(1)
      
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    all_preds = all_probs.argmax(axis=1)

    results = sweep_classification_metrics(all_probs, all_labels, all_preds=all_preds, n_classes=n_classes)
    results.update({k: meter.avg for k, meter in meters.items()})

    if 'report' in results:
        del results['report']

    if verbose:
        msg = [f"{k}: {v:.3f}" for k, v in results.items()]
        print("\t".join(msg))

    dumps = {}
    if dump_results:
        dumps['labels'] = all_labels
        dumps['probs'] = all_probs
        dumps['sample_ids'] = np.array(
            loader.dataset.idx2sample_df['sample_id'])
    return results, dumps


## SURVIVAL
def train_loop_survival(model, loader, optimizer, lr_scheduler, loss_fn=None, in_dropout=0.0, print_every=50,
                        accum_steps=1):
    model.train()
    meters = {'bag_size': AverageMeter()}
    bag_size_meter = meters['bag_size']
    all_risk_scores, all_censorships, all_event_times = [], [], []
    
    # For Cox loss, we need to accumulate multiple samples before computing loss
    is_cox_loss = isinstance(loss_fn, CoxLoss)
    
    if is_cox_loss:
        # Accumulate samples for Cox loss computation
        accumulated_outputs = []
        accumulated_times = []
        accumulated_censorships = []
    
    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(device)

        # Load MRI features if available
        mri_features = None
        if 'mri_feature' in batch:
            mri_features = batch['mri_feature'].to(device)
            
        # Load clinical features if available
        clinical_features = None
        if 'clinical_feature' in batch:
            clinical_features = batch['clinical_feature'].to(device)

        if in_dropout:
            data = F.dropout(data, p=in_dropout)
        event_time = batch['survival_time'].to(device)
        censorship = batch['censorship'].to(device)
        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        
        if is_cox_loss:
            # For Cox loss, get model output without computing loss yet
            model_kwargs = {'attn_mask': attn_mask, 'label': label, 'censorship': censorship, 'loss_fn': None}
            out, log_dict = model(data, additional_embeddings=mri_features, 
                                  clinical_features=clinical_features, model_kwargs=model_kwargs)
            
            # Accumulate outputs for batch processing
            if 'logits' in out:
                accumulated_outputs.append(out['logits'])
            else:
                raise ValueError(f"Model output must contain 'logits', got keys: {list(out.keys())}")
                
            accumulated_times.append(event_time)
            accumulated_censorships.append(censorship)
            
            # Initialize loss tracking - set to None initially for Cox losses during accumulation
            out['loss'] = None
            if 'loss' not in log_dict:
                log_dict['loss'] = 0.0
                
        else:
            # For non-Cox losses, compute normally
            model_kwargs = {'attn_mask': attn_mask, 'label': label, 'censorship': censorship, 'loss_fn': loss_fn}
            out, log_dict = model(data, additional_embeddings=mri_features, 
                                  clinical_features=clinical_features, model_kwargs=model_kwargs)

        # Process accumulated Cox loss every accum_steps or at end
        if is_cox_loss and ((batch_idx + 1) % accum_steps == 0 or batch_idx == len(loader) - 1):
            if accumulated_outputs:
                # Combine accumulated samples
                combined_outputs = torch.cat(accumulated_outputs, dim=0)
                combined_times = torch.cat(accumulated_times, dim=0)
                combined_censorships = torch.cat(accumulated_censorships, dim=0)
                
                # Compute Cox loss on accumulated batch
                cox_loss_dict = loss_fn(logits=combined_outputs, 
                                      times=combined_times, 
                                      censorships=combined_censorships)
                loss = cox_loss_dict['loss']
                
                # Update log_dict with actual loss
                log_dict.update(cox_loss_dict)
                out['loss'] = loss
                
                # Clear accumulation
                accumulated_outputs = []
                accumulated_times = []
                accumulated_censorships = []
            else:
                # No accumulated outputs - this shouldn't happen but handle gracefully
                out['loss'] = torch.tensor(0.0, device=device, requires_grad=True)
        
        # Handle loss computation and backprop
        current_loss = out.get('loss', None)
        if current_loss is not None and current_loss != 0:
            loss = current_loss
            if not is_cox_loss:  # For non-Cox losses, apply accumulation division
                loss = loss / accum_steps
            loss.backward()
            
        # Optimizer step
        if (batch_idx + 1) % accum_steps == 0:
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

        # End of iteration survival-specific metrics to calculate / log
        # Only collect metrics when we have valid output
        if 'risk' in out:
            all_risk_scores.append(out['risk'].detach().cpu().numpy())
        elif 'logits' in out:
            # For Cox loss, logits are log risks, so convert to risk
            all_risk_scores.append(torch.exp(out['logits']).detach().cpu().numpy())
        else:
            # Fallback for accumulation phase
            all_risk_scores.append(torch.zeros(1, 1).numpy())
                
        all_censorships.append(censorship.cpu().numpy())
        all_event_times.append(event_time.cpu().numpy())

        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            if isinstance(val, torch.Tensor):
                val = val.item()
            meters[key].update(val, n=len(data))

        bag_size_meter.update(data.size(1), n=len(data))

        if ((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch survival-specific metrics to calculate / log
    all_risk_scores = np.concatenate(all_risk_scores).squeeze(1)
    all_censorships = np.concatenate(all_censorships).squeeze(1)
    all_event_times = np.concatenate(all_event_times).squeeze(1)
    c_index = concordance_index_censored(
        (1 - all_censorships).astype(bool), all_event_times, all_risk_scores, tied_tol=1e-08)[0]
    results = {k: meter.avg for k, meter in meters.items()}
    results.update({'c_index': c_index})
    results['lr'] = optimizer.param_groups[0]['lr']
    
    return results


@torch.no_grad()
def validate_survival(model, loader,
                      loss_fn=None,
                      print_every=50,
                      dump_results=False,
                      recompute_loss_at_end=True,
                      verbose=1,
                      split_name=None,
                      show_batch_progress=True):
    model.eval()
    meters = {'bag_size': AverageMeter()}
    bag_size_meter = meters['bag_size']
    all_risk_scores, all_censorships, all_event_times = [], [], []


    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(device)

        # Load MRI features if available
        mri_features = None
        if 'mri_feature' in batch:
            mri_features = batch['mri_feature'].to(device)
            
        # Load clinical features if available
        clinical_features = None
        if 'clinical_feature' in batch:
            clinical_features = batch['clinical_feature'].to(device)

        event_time = batch['survival_time'].to(device)
        censorship = batch['censorship'].to(device)
        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        model_kwargs = {'attn_mask': attn_mask, 'label': label, 'censorship': censorship, 'loss_fn': loss_fn}
        out, log_dict = model(data, additional_embeddings=mri_features, 
                             clinical_features=clinical_features, model_kwargs=model_kwargs)

        # End of iteration survival-specific metrics to calculate / log
        bag_size_meter.update(data.size(1), n=len(data))
        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            meters[key].update(val, n=len(data))
        all_risk_scores.append(out['risk'].cpu().numpy())
        all_censorships.append(censorship.cpu().numpy())
        all_event_times.append(event_time.cpu().numpy())

        # Only show per-batch progress if show_batch_progress is True (i.e., during training validation)
        if verbose and show_batch_progress and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch survival-specific metrics to calculate / log
    all_risk_scores = np.concatenate(all_risk_scores).squeeze(1)
    all_censorships = np.concatenate(all_censorships).squeeze(1)
    all_event_times = np.concatenate(all_event_times).squeeze(1)

    c_index = concordance_index_censored(
        (1 - all_censorships).astype(bool), all_event_times, all_risk_scores, tied_tol=1e-08)[0]
    results = {k: meter.avg for k, meter in meters.items()}
    results.update({'c_index': c_index})

    if recompute_loss_at_end and isinstance(loss_fn, CoxLoss):
        surv_loss_dict = loss_fn(logits=torch.tensor(all_risk_scores).unsqueeze(1),
                                 times=torch.tensor(all_event_times).unsqueeze(1),
                                 censorships=torch.tensor(all_censorships).unsqueeze(1))
        results['surv_loss'] = surv_loss_dict['loss'].item()
        results.update({k: v.item() for k, v in surv_loss_dict.items() if isinstance(v, torch.Tensor)})

    if verbose:
        msg = [f"{k}: {v:.3f}" for k, v in results.items()]
        print("\t".join(msg))

    dumps = {}
    if dump_results:
        dumps['all_risk_scores'] = all_risk_scores
        dumps['all_censorships'] = all_censorships
        dumps['all_event_times'] = all_event_times
        dumps['sample_ids'] = np.array(
            loader.dataset.idx2sample_df['sample_id'])
    return results, dumps


@torch.no_grad()
def sweep_classification_metrics(all_probs, all_labels, all_preds=None, n_classes=None):
    if n_classes is None:
        n_classes = all_probs.shape[1]

    if all_preds is None:
        all_preds = all_probs.argmax(axis=1)

    if n_classes == 2:
        all_probs = all_probs[:, 1]
        roc_kwargs = {}
    else:
        roc_kwargs = {'multi_class': 'ovo', 'average': 'macro'}

    bacc = balanced_accuracy_score(all_labels, all_preds)
    kappa = cohen_kappa_score(all_labels, all_preds, weights='quadratic')
    cls_rep = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)
    acc = accuracy_score(all_labels, all_preds)
    roc_auc = roc_auc_score(all_labels, all_probs, **roc_kwargs)

    results = {'acc': acc,
               'bacc': bacc,
               'report': cls_rep,
               'kappa': kappa,
               'roc_auc': roc_auc,
               'weighted_f1': cls_rep['weighted avg']['f1-score']}
    return results

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/update_trainer.py


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/.ipynb_checkpoints/trainer-checkpoint.py
import os
from os.path import join as j_
import pdb
import torch.nn.functional as F

import numpy as np
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter

try:
    from sksurv.metrics import concordance_index_censored
except ImportError:
    print('scikit-survival not installed. Exiting...')
    raise

from sklearn.metrics import (roc_auc_score, balanced_accuracy_score,
                             cohen_kappa_score, classification_report, accuracy_score)

from mil_models.tokenizer import PrototypeTokenizer
from mil_models import create_downstream_model, prepare_emb
from utils.losses import NLLSurvLoss, CoxLoss, SurvRankingLoss
from utils.utils import (EarlyStopping, save_checkpoint, AverageMeter,
                         get_optim, print_network, get_lr_scheduler)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

PROTO_MODELS = ['PANTHER', 'OT', 'H2T', 'ProtoCount']

## GENERIC
def log_dict_tensorboard(writer, results, str_prefix, step=0, verbose=False):
    for k, v in results.items():
        if verbose: print(f'{k}: {v:.4f}')
        writer.add_scalar(f'{str_prefix}{k}', v, step)
    return writer


def train(datasets, args, mode='classification'):
    """
    Train for a single fold for classification or suvival
    """
    
    writer_dir = args.results_dir
    if not os.path.isdir(writer_dir):
        os.mkdir(writer_dir)
    writer = SummaryWriter(writer_dir, flush_secs=15)

    assert args.es_metric == 'loss'
    if mode == 'classification':
        loss_fn = nn.CrossEntropyLoss()



    elif mode == 'survival':
        if args.loss_fn == 'nll':
            loss_fn = NLLSurvLoss(alpha=args.nll_alpha)
        elif args.loss_fn == 'cox':
            loss_fn = CoxLoss()
        elif args.loss_fn == 'rank':
            loss_fn = SurvRankingLoss()

    print('\nInit Model...', end=' ')

    # If prototype-based models, need to create slide-level embeddings
    if args.model_type in PROTO_MODELS:
        datasets, _ = prepare_emb(datasets, args, mode)
        new_in_dim = None
        for k, loader in datasets.items():
            assert loader.dataset.X is not None
            new_in_dim_curr = loader.dataset.X.shape[-1]
            if new_in_dim is None:
                new_in_dim = new_in_dim_curr
            else:
                assert new_in_dim == new_in_dim_curr

            if 'LinearEmb' in args.emb_model_type:
                # Theis emb_model_type doesn't require per-prototype structure (simple linear layer)
                factor = 1
            else:
                # The original embedding is 1-D (long) feature vector
                # Reshape it to (n_proto, -1)
                tokenizer = PrototypeTokenizer(args.model_type, args.out_type, args.n_proto)
                prob, mean, cov = tokenizer(loader.dataset.X)
                loader.dataset.X = torch.cat([torch.Tensor(prob).unsqueeze(dim=-1), torch.Tensor(mean), torch.Tensor(cov)], dim=-1)

                factor = args.n_proto
            
        args.in_dim = new_in_dim // factor

        args.model_type = args.emb_model_type
        args.model_config = args.emb_model_type # actually the config
    else:
        print(f"{args.model_type} doesn't construct unsupervised slide-level embeddings!")

    model = create_downstream_model(args, mode=mode)
    model.to(device)
    print('Done!')
    print_network(model)

    print('\nInit optimizer ...', end=' ')
    optimizer = get_optim(model=model, args=args)
    print('ksjdkjhdskjh')
    print(datasets)
    lr_scheduler = get_lr_scheduler(args, optimizer, datasets['train'])

    if args.early_stopping:
        print('\nSetup EarlyStopping...', end=' ')
        early_stopper = EarlyStopping(save_dir=args.results_dir,
                                      patience=args.es_patience,
                                      min_stop_epoch=args.es_min_epochs,
                                      better='min' if args.es_metric == 'loss' else 'max',
                                      verbose=True)
    else:
        print('\nNo EarlyStopping...', end=' ')
        early_stopper = None
    
    #####################
    # The training loop #
    #####################
    for epoch in range(args.max_epochs):
        step_log = {'epoch': epoch, 'samples_seen': (epoch + 1) * len(datasets['train'].dataset)}

        ### Train Loop
        print('#' * 10, f'TRAIN Epoch: {epoch}', '#' * 10)
        if mode == 'classification':
            train_results = train_loop_classification(model, datasets['train'], optimizer, lr_scheduler, loss_fn,
                                                      in_dropout=args.in_dropout, print_every=args.print_every,
                                                      accum_steps=args.accum_steps)
        elif mode == 'survival':
            train_results = train_loop_survival(model, datasets['train'], optimizer, lr_scheduler, loss_fn,
                                                in_dropout=args.in_dropout, print_every=args.print_every,
                                                accum_steps=args.accum_steps)

        writer = log_dict_tensorboard(writer, train_results, 'train/', epoch)

        ### Validation Loop (Optional)
        if 'val' in datasets.keys():
            print('#' * 11, f'VAL Epoch: {epoch}', '#' * 11)
            if mode == 'classification':
                val_results, _ = validate_classification(model, datasets['val'], loss_fn,
                                                         print_every=args.print_every, verbose=True)
            elif mode == 'survival':
                val_results, _ = validate_survival(model, datasets['val'], loss_fn,
                                                   print_every=args.print_every, verbose=True)

            writer = log_dict_tensorboard(writer, val_results, 'val/', epoch)

            ### Check Early Stopping (Optional)
            if early_stopper is not None:
                if args.es_metric == 'loss':
                    score = val_results['loss']

                else:
                    raise NotImplementedError
                save_ckpt_kwargs = dict(config=vars(args),
                                        epoch=epoch,
                                        model=model,
                                        score=score,
                                        fname=f's_checkpoint.pth')
                stop = early_stopper(epoch, score, save_checkpoint, save_ckpt_kwargs)
                if stop:
                    break
        print('#' * (22 + len(f'TRAIN Epoch: {epoch}')), '\n')

    ### End of epoch: Load in the best model (or save the latest model with not early stopping)
    if args.early_stopping:
        model.load_state_dict(torch.load(j_(args.results_dir, f"s_checkpoint.pth"))['model'])
    else:
        torch.save(model.state_dict(), j_(args.results_dir, f"s_checkpoint.pth"))

    ### End of epoch: Evaluate on val and test set
    results, dumps = {}, {}
    for k, loader in datasets.items():
        print(f'End of training. Evaluating on Split {k.upper()}...:')
        if mode == 'classification':
            results[k], dumps[k] = validate_classification(model, loader, loss_fn, print_every=args.print_every,
                                                           dump_results=True, verbose=False)
        elif mode == 'survival':
            results[k], dumps[k] = validate_survival(model, loader, loss_fn, print_every=args.print_every,
                                                     dump_results=True, verbose=False)

        if k == 'train':
            _ = results.pop('train')  # Train results by default are not saved in the summary, but train dumps are
        else:
            log_dict_tensorboard(writer, results[k], f'final/{k}_', 0, verbose=True)

    writer.close()
    return results, dumps


## CLASSIFICATION
def train_loop_classification(model, loader, optimizer, lr_scheduler, loss_fn=None,
                              in_dropout=0.0, print_every=50,
                              accum_steps=1):
    model.train()
    meters = {'bag_size': AverageMeter(), 'cls_acc': AverageMeter()}
    bag_size_meter = meters['bag_size']
    acc_meter = meters['cls_acc']

    #import pdb; pdb.set_trace()
    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(torch.long).to(device)
        if len(label.shape) == 2 and label.shape[1] == 1:
            label = label.squeeze(dim=-1)

        if in_dropout:
            data = F.dropout(data, p=in_dropout)
        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        model_kwargs = {'attn_mask': attn_mask, 'label': label, 'loss_fn': loss_fn}
        out, log_dict = model(data, model_kwargs)

        # Get loss + backprop
        loss = out['loss']
        loss = loss / accum_steps
        loss.backward()
        if (batch_idx + 1) % accum_steps == 0:
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

        # End of iteration classification-specific metrics to calculate / log
        logits = out['logits']
        acc = (label == logits.argmax(dim=-1)).float().mean()

        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            meters[key].update(val, n=len(data))

        acc_meter.update(acc.item(), n=len(data))
        bag_size_meter.update(data.size(1), n=len(data))

        if ((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch classification-specific metrics to calculate / log
    results = {k: meter.avg for k, meter in meters.items()}
    results['lr'] = optimizer.param_groups[0]['lr']
    return results

@torch.no_grad()
def validate_classification(model, loader,
                            loss_fn=None,
                            print_every=50,
                            dump_results=False,
                            verbose=1):
    model.eval()
    meters = {'bag_size': AverageMeter(), 'cls_acc': AverageMeter()}
    acc_meter = meters['cls_acc']
    bag_size_meter = meters['bag_size']
    all_probs = []
    all_labels = []
        
    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(torch.long).to(device)
        if len(label.shape) == 2 and label.shape[1] == 1:
            label = label.squeeze(dim=-1)

        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        model_kwargs = {'attn_mask': attn_mask, 'label': label, 'loss_fn': loss_fn}
        out, log_dict = model(data, model_kwargs)
        

        # End of iteration classification-specific metrics to calculate / log
        logits = out['logits']
        acc = (label == logits.argmax(dim=-1)).float().mean()
        acc_meter.update(acc.item(), n=len(data))
        bag_size_meter.update(data.size(1), n=len(data))
        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            meters[key].update(val, n=len(data))
        all_probs.append(torch.softmax(logits, dim=-1).cpu().numpy())
        all_labels.append(label.cpu().numpy())

        if verbose and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch classification-specific metrics to calculate / log
    n_classes = logits.size(1)
      
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    all_preds = all_probs.argmax(axis=1)

    results = sweep_classification_metrics(all_probs, all_labels, all_preds=all_preds, n_classes=n_classes)
    results.update({k: meter.avg for k, meter in meters.items()})

    if 'report' in results:
        del results['report']

    if verbose:
        msg = [f"{k}: {v:.3f}" for k, v in results.items()]
        print("\t".join(msg))

    dumps = {}
    if dump_results:
        dumps['labels'] = all_labels
        dumps['probs'] = all_probs
        dumps['sample_ids'] = np.array(
            loader.dataset.idx2sample_df['sample_id'])
    return results, dumps


## SURVIVAL
def train_loop_survival(model, loader, optimizer, lr_scheduler, loss_fn=None, in_dropout=0.0, print_every=50,
                        accum_steps=32):
    model.train()
    meters = {'bag_size': AverageMeter()}
    bag_size_meter = meters['bag_size']
    all_risk_scores, all_censorships, all_event_times = [], [], []
    
    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(device)

        if in_dropout:
            data = F.dropout(data, p=in_dropout)
        event_time = batch['survival_time'].to(device)
        censorship = batch['censorship'].to(device)
        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        model_kwargs = {'attn_mask': attn_mask, 'label': label, 'censorship': censorship, 'loss_fn': loss_fn}
        out, log_dict = model(data, model_kwargs)

        if out['loss'] is None:
            continue

        # Get loss + backprop
        loss = out['loss']
        loss = loss / accum_steps
        loss.backward()
        if (batch_idx + 1) % accum_steps == 0:
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

        # End of iteration survival-specific metrics to calculate / log
        all_risk_scores.append(out['risk'].detach().cpu().numpy())
        all_censorships.append(censorship.cpu().numpy())
        all_event_times.append(event_time.cpu().numpy())

        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            meters[key].update(val, n=len(data))

        bag_size_meter.update(data.size(1), n=len(data))

        if ((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch survival-specific metrics to calculate / log
    all_risk_scores = np.concatenate(all_risk_scores).squeeze(1)
    all_censorships = np.concatenate(all_censorships).squeeze(1)
    all_event_times = np.concatenate(all_event_times).squeeze(1)
    c_index = concordance_index_censored(
        (1 - all_censorships).astype(bool), all_event_times, all_risk_scores, tied_tol=1e-08)[0]
    results = {k: meter.avg for k, meter in meters.items()}
    results.update({'c_index': c_index})
    results['lr'] = optimizer.param_groups[0]['lr']
    return results


@torch.no_grad()
def validate_survival(model, loader,
                      loss_fn=None,
                      print_every=50,
                      dump_results=False,
                      recompute_loss_at_end=True,
                      verbose=1):
    model.eval()
    meters = {'bag_size': AverageMeter()}
    bag_size_meter = meters['bag_size']
    all_risk_scores, all_censorships, all_event_times = [], [], []


    for batch_idx, batch in enumerate(loader):
        data = batch['img'].to(device)
        label = batch['label'].to(device)

        event_time = batch['survival_time'].to(device)
        censorship = batch['censorship'].to(device)
        attn_mask = batch['attn_mask'].to(device) if ('attn_mask' in batch) else None
        model_kwargs = {'attn_mask': attn_mask, 'label': label, 'censorship': censorship, 'loss_fn': loss_fn}
        out, log_dict = model(data, model_kwargs)

        # End of iteration survival-specific metrics to calculate / log
        bag_size_meter.update(data.size(1), n=len(data))
        for key, val in log_dict.items():
            if key not in meters:
                meters[key] = AverageMeter()
            meters[key].update(val, n=len(data))
        all_risk_scores.append(out['risk'].cpu().numpy())
        all_censorships.append(censorship.cpu().numpy())
        all_event_times.append(event_time.cpu().numpy())

        if verbose and (((batch_idx + 1) % print_every == 0) or (batch_idx == len(loader) - 1)):
            msg = [f"avg_{k}: {meter.avg:.4f}" for k, meter in meters.items()]
            msg = f"batch {batch_idx}\t" + "\t".join(msg)
            print(msg)

    # End of epoch survival-specific metrics to calculate / log
    all_risk_scores = np.concatenate(all_risk_scores).squeeze(1)
    all_censorships = np.concatenate(all_censorships).squeeze(1)
    all_event_times = np.concatenate(all_event_times).squeeze(1)

    c_index = concordance_index_censored(
        (1 - all_censorships).astype(bool), all_event_times, all_risk_scores, tied_tol=1e-08)[0]
    results = {k: meter.avg for k, meter in meters.items()}
    results.update({'c_index': c_index})

    if recompute_loss_at_end and isinstance(loss_fn, CoxLoss):
        surv_loss_dict = loss_fn(logits=torch.tensor(all_risk_scores).unsqueeze(1),
                                 times=torch.tensor(all_event_times).unsqueeze(1),
                                 censorships=torch.tensor(all_censorships).unsqueeze(1))
        results['surv_loss'] = surv_loss_dict['loss'].item()
        results.update({k: v.item() for k, v in surv_loss_dict.items() if isinstance(v, torch.Tensor)})

    if verbose:
        msg = [f"{k}: {v:.3f}" for k, v in results.items()]
        print("\t".join(msg))

    dumps = {}
    if dump_results:
        dumps['all_risk_scores'] = all_risk_scores
        dumps['all_censorships'] = all_censorships
        dumps['all_event_times'] = all_event_times
        dumps['sample_ids'] = np.array(
            loader.dataset.idx2sample_df['sample_id'])
    return results, dumps


@torch.no_grad()
def sweep_classification_metrics(all_probs, all_labels, all_preds=None, n_classes=None):
    if n_classes is None:
        n_classes = all_probs.shape[1]

    if all_preds is None:
        all_preds = all_probs.argmax(axis=1)

    if n_classes == 2:
        all_probs = all_probs[:, 1]
        roc_kwargs = {}
    else:
        roc_kwargs = {'multi_class': 'ovo', 'average': 'macro'}

    bacc = balanced_accuracy_score(all_labels, all_preds)
    kappa = cohen_kappa_score(all_labels, all_preds, weights='quadratic')
    cls_rep = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)
    acc = accuracy_score(all_labels, all_preds)
    roc_auc = roc_auc_score(all_labels, all_probs, **roc_kwargs)

    results = {'acc': acc,
               'bacc': bacc,
               'report': cls_rep,
               'kappa': kappa,
               'roc_auc': roc_auc,
               'weighted_f1': cls_rep['weighted avg']['f1-score']}
    return results

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/.ipynb_checkpoints/main_classification-checkpoint.py
"""
Main entry point for classification downstream tasks
"""

from __future__ import print_function

import argparse
import pdb
import os
from os.path import join as j_
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# internal imports
from utils.file_utils import save_pkl
from utils.utils import (seed_torch, array2list, merge_dict, read_splits, 
                         parse_model_name, get_current_time,
                         extract_patching_info)

from trainer import train
from wsi_datasets import WSIClassificationDataset
from data_factory import tasks, label_dicts

import torch
from torch.utils.data import DataLoader, sampler

import pandas as pd
import numpy as np
import json


PROTO_MODELS = ['PANTHER', 'OT', 'H2T', 'ProtoCount']

def build_sampler(dataset, sampler_type=None):
    data_sampler = None
    if sampler_type is None:
        return data_sampler
    
    assert sampler_type in ['weighted', 'random', 'sequential']
    if sampler_type == 'weighted':
        labels = dataset.get_labels(np.arange(len(dataset)), apply_transform=True)
        uniques, counts = np.unique(labels, return_counts=True)
        weights = {uniques[i]: 1. / counts[i] for i in range(len(uniques))}
        samples_weight = np.array([weights[t] for t in labels])
        samples_weight = torch.from_numpy(samples_weight)
        data_sampler = sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))
    elif sampler_type == 'random':
        data_sampler = sampler.RandomSampler(dataset)
    elif sampler_type == 'sequential':
        data_sampler = sampler.SequentialSampler(dataset)

    return data_sampler

def build_datasets(csv_splits, model_type, batch_size=1, num_workers=2,
                   train_kwargs={}, val_kwargs={}, sampler_types={'train': 'random',
                                                                  'val': 'sequential',
                                                                  'test': 'sequential'}):
    """
    Construct dataloaders from the data splits
    """
    dataset_splits = {}
    for k in csv_splits.keys(): # ['train', 'val', 'test']
        print("\nSPLIT: ", k)
        df = csv_splits[k]
        dataset_kwargs = train_kwargs.copy() if (k == 'train') else val_kwargs.copy()
        print(dataset_kwargs)
        if k == 'test_nlst':
            dataset_kwargs['sample_col'] = 'case_id'
        dataset = WSIClassificationDataset(df, **dataset_kwargs)
        data_sampler = build_sampler(dataset, sampler_type=sampler_types.get(k, 'sequential'))

        # If prototype methods, each WSI will have same feature bag dimension and is batchable
        # Otherwise, we need to use batch size of 1 to accommodate to different bag size for each WSI.
        # Alternatively, we can sample same number of patch features per WSI to have larger batch.
        if model_type not in PROTO_MODELS:
            batch_size = batch_size if dataset_kwargs.get('bag_size', -1) > 0 else 1

        dataloader = DataLoader(dataset, batch_size=batch_size, sampler=data_sampler, num_workers=num_workers)
        dataset_splits[k] = dataloader
        print(f'split: {k}, n: {len(dataset)}')
    return dataset_splits


def main(args):
    if args.train_bag_size == -1:
        args.train_bag_size = args.bag_size
    if args.val_bag_size == -1:
        args.val_bag_size = args.bag_size

    sampler_types = {'train': 'sequential',
                'val': 'sequential',
                'test': 'sequential'}
    train_kwargs = dict(data_source=args.data_source,
                          label_map=args.label_map,
                          target_col=args.target_col,
                          bag_size=args.train_bag_size,
                          shuffle=True)
    
    # use the whole bag at test time
    val_kwargs = dict(data_source=args.data_source,
                          label_map=args.label_map,
                          target_col=args.target_col,
                          bag_size=args.val_bag_size)

    all_results, all_dumps = {}, {}

    # Cross-validation
    seed_torch(args.seed)
    csv_splits = read_splits(args)
    print(csv_splits)
    print('successfully read splits for: ', list(csv_splits.keys()))
    dataset_splits = build_datasets(csv_splits, 
                                    model_type=args.model_type,
                                    batch_size=args.batch_size,
                                    num_workers=args.num_workers,
                                    sampler_types=sampler_types,
                                    train_kwargs=train_kwargs,
                                    val_kwargs=val_kwargs)

    fold_results, fold_dumps = train(dataset_splits, args, mode='classification')

    for split, split_results in fold_results.items():
        all_results[split] = merge_dict({}, split_results) if (split not in all_results.keys()) else merge_dict(all_results[split], split_results)
        save_pkl(j_(args.results_dir, f'{split}_results.pkl'), fold_dumps[split]) # saves per-split, per-fold results to pkl
    
    final_dict = {}
    for split, split_results in all_results.items():
        final_dict.update({f'{metric}_{split}': array2list(val) for metric, val in split_results.items()})
    final_df = pd.DataFrame(final_dict)
    save_name = 'summary.csv'
    final_df.to_csv(j_(args.results_dir, save_name), index=False)
    with open(j_(args.results_dir, save_name + '.json'), 'w') as f:
        f.write(json.dumps(final_dict, sort_keys=True, indent=4))
    
    dump_path = j_(args.results_dir, 'all_dumps.h5')
    fold_dumps.update({'labels': np.array(list(args.label_map.keys()), dtype=np.object_)})
    save_pkl(dump_path, fold_dumps)

    return final_dict


# Generic training settings
parser = argparse.ArgumentParser(description='Configurations for WSI Training')
### optimizer settings ###
parser.add_argument('--max_epochs', type=int, default=10,
                    help='maximum number of epochs to train (default: 20)')
parser.add_argument('--lr', type=float, default=1e-4,
                    help='learning rate')
parser.add_argument('--wd', type=float, default=1e-5,
                    help='weight decay')
parser.add_argument('--accum_steps', type=int, default=1,
                    help='grad accumulation steps')
parser.add_argument('--opt', type=str,
                    choices=['adamW', 'sgd'], default='adamW')
parser.add_argument('--lr_scheduler', type=str,
                    choices=['cosine', 'linear', 'constant'], default='constant')
parser.add_argument('--warmup_steps', type=int,
                    default=-1, help='warmup iterations')
parser.add_argument('--warmup_epochs', type=int,
                    default=-1, help='warmup epochs')
parser.add_argument('--batch_size', type=int, default=1)

### misc ###
parser.add_argument('--print_every', default=100,
                    type=int, help='how often to print')
parser.add_argument('--seed', type=int, default=1,
                    help='random seed for reproducible experiment (default: 1)')
parser.add_argument('--num_workers', type=int, default=2)

### Earlystopper args ###
parser.add_argument('--early_stopping', action='store_true',
                    default=False, help='enable early stopping')
parser.add_argument('--es_min_epochs', type=int, default=15,
                    help='early stopping min epochs')
parser.add_argument('--es_patience', type=int, default=10,
                    help='early stopping min patience')
parser.add_argument('--es_metric', type=str, default='loss',
                    help='early stopping metric')

##
# model / loss fn args ###
parser.add_argument('--model_type', type=str, choices=['H2T', 'ABMIL', 'TransMIL', 'SumMIL', 'OT', 'PANTHER', 'ProtoCount', 'DeepAttnMIL', 'ILRA'],
                    default='ABMIL',
                    help='type of model')
parser.add_argument('--emb_model_type', type=str, default='LinEmb_LR')
parser.add_argument('--ot_eps', default=0.1, type=float,
                    help='Strength for entropic constraint regularization for OT')
parser.add_argument('--model_config', type=str,
                    default='ABMIL_default', help="name of model config file")

parser.add_argument('--in_dim', default=768, type=int,
                    help='dim of input features')
parser.add_argument('--in_dropout', default=0.0, type=float,
                    help='Probability of dropping out input features.')
parser.add_argument('--bag_size', type=int, default=-1)
parser.add_argument('--train_bag_size', type=int, default=-1)
parser.add_argument('--val_bag_size', type=int, default=-1)

parser.add_argument('--train_sampler', type=str, default='random', 
                    choices=['random', 'weighted', 'sequential'])
parser.add_argument('--n_fc_layers', type=int)
parser.add_argument('--em_iter', type=int)
parser.add_argument('--tau', type=float)
parser.add_argument('--out_type', type=str, default='param_cat')

# Prototype related
parser.add_argument('--load_proto', action='store_true', default=False)
parser.add_argument('--proto_path', type=str, default='.')
parser.add_argument('--fix_proto', action='store_true', default=False)
parser.add_argument('--n_proto', type=int)

# experiment task / label args ###
parser.add_argument('--exp_code', type=str,
                    help='experiment code for saving results')
parser.add_argument('--task', type=str, choices=tasks)
parser.add_argument('--loss_fn', type=str, default=None)
parser.add_argument('--target_col', type=str, default='label')

# dataset / split args ###
parser.add_argument('--data_source', type=str, default=None,
                    help='manually specify the data source')
parser.add_argument('--split_dir', type=str, default=None,
                    help='manually specify the set of splits to use')
parser.add_argument('--split_names', type=str, default='train,val,test',
                    help='delimited list for specifying names within each split')
parser.add_argument('--overwrite', action='store_true', default=False,
                    help='overwrite existing results')
# logging args ###
parser.add_argument('--results_dir', default='./results',
                    help='results directory (default: ./results)')
parser.add_argument('--tags', nargs='+', type=str, default=None,
                    help='tags for logging')
args = parser.parse_args()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if __name__ == "__main__":

    args.label_map = label_dicts[args.task]
    print('label map: ', args.label_map)
    args.n_classes = len(set(list(args.label_map.values())))
    print('task: ', args.task)
    args.split_dir = j_('splits', args.split_dir)
    print('split_dir: ', args.split_dir)
    
    split_num = args.split_dir.split('/')[2].split('_k=')
    args.split_name_clean = args.split_dir.split('/')[2].split('_k=')[0]
    if len(split_num) > 1:
        args.split_k = int(split_num[1])
    else:
        args.split_k = 0

    print(args.proto_path)
    if os.path.isfile(args.proto_path):
        args.proto_fname = '/'.join(args.proto_path.split('/')[-2:])

    ### Allows you to pass in multiple data sources (separated by comma). If single data source, no change.
    args.data_source = [src for src in args.data_source.split(',')]
    check_params_same = []
    for src in args.data_source: 
        ### assert data source exists + extract feature name ###
        print('data source: ', src)
        assert os.path.isdir(src), f"data source must be a directory: {src} invalid"

        ### parse patching info ### 
        feat_name = os.path.basename(src)
        print(feat_name)
        #mag, patch_size = extract_patching_info(os.path.dirname(src))       
        mag, patch_size = 0,0
        if (mag < 0 or patch_size < 0):
            raise ValueError(f"invalid patching info parsed for {src}")
        check_params_same.append([feat_name, mag, patch_size])
    
    try:
        check_params_same = pd.DataFrame(check_params_same, columns=['feats_name', 'mag', 'patch_size'])
        print(check_params_same.to_string())
        assert check_params_same.drop(['feats_name'],axis=1).drop_duplicates().shape[0] == 1
        print("All data sources have the same feature extraction parameters.")
    except:
        print("Data sources do not share the same feature extraction parameters. Exiting...")
        sys.exit()
        
    ### Updated parsed mdoel parameters in args.Namespace ###
    #### parse patching info ####
    mag, patch_size = 0,0
    
    #### parse model name ####
    parsed = parse_model_name(feat_name) 
    parsed.update({'patch_mag': mag, 'patch_size': patch_size, 'feat_names': sorted(list(set(check_params_same['feats_name'].tolist())))})
    for key, val in parsed.items():
        setattr(args, key, val)
     
    ### setup results dir ###
    if args.exp_code is None:
        if args.model_config == 'PANTHER_default':
            exp_code = f"{args.split_name_clean}::{args.model_config}+{args.emb_model_type}::{args.loss_fn}::{feat_name}"
        else:
            exp_code = f"{args.split_name_clean}::{args.model_config}::{feat_name}"
    else:
        pass
    
    args.results_dir = j_(args.results_dir, 
                          args.task, 
                          f'k={args.split_k}', 
                          str(exp_code), 
                          str(exp_code)+f"::{get_current_time()}")

    os.makedirs(args.results_dir, exist_ok=True)

    print("\n################### Settings ###################")
    for key, val in vars(args).items():
        print("{}:  {}".format(key, val))

    with open(j_(args.results_dir, 'config.json'), 'w') as f:
        f.write(json.dumps(vars(args), sort_keys=True, indent=4))

    #### train ####
    results = main(args)

    print("FINISHED!\n\n\n")


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/.ipynb_checkpoints/main_survival-checkpoint.py
"""
Main entry point for survival downstream tasks
"""

from __future__ import print_function

import argparse
import pdb
import os
from os.path import join as j_
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
# internal imports
from utils.file_utils import save_pkl
from utils.utils import (seed_torch, array2list, merge_dict, read_splits, 
                         parse_model_name, get_current_time, extract_patching_info)

from trainer import train
from wsi_datasets import WSISurvivalDataset
# pytorch imports
import torch
from torch.utils.data import DataLoader

import pandas as pd
import json

PROTO_MODELS = ['PANTHER', 'OT', 'H2T', 'ProtoCount']

def build_datasets(csv_splits, model_type, batch_size=1, num_workers=2, train_kwargs={}, val_kwargs={}):
    """
    Construct dataloaders from the data splits
    """
    dataset_splits = {}
    label_bins = None
    for k in csv_splits.keys(): # ['train', 'val', 'test']
        df = csv_splits[k]
        dataset_kwargs = train_kwargs.copy() if (k == 'train') else val_kwargs.copy()
        dataset_kwargs['label_bins'] = label_bins
        dataset = WSISurvivalDataset(df, **dataset_kwargs)

        # If prototype methods, each WSI will have same feature bag dimension and is batchable
        # Otherwise, we need to use batch size of 1 to accommodate to different bag size for each WSI.
        # Alternatively, we can sample same number of patch features per WSI to have larger batch.
        if model_type not in PROTO_MODELS:
            batch_size = batch_size if dataset_kwargs.get('bag_size', -1) > 0 else 1

        dataloader = DataLoader(dataset, batch_size=batch_size,
                                shuffle=dataset_kwargs['shuffle'], num_workers=num_workers)
        dataset_splits[k] = dataloader
        print(f'split: {k}, n: {len(dataset)}')
        if (args.loss_fn == 'nll') and (k == 'train'):
            label_bins = dataset.get_label_bins()
    return dataset_splits


def main(args):
    if args.train_bag_size == -1:
        args.train_bag_size = args.bag_size
    if args.val_bag_size == -1:
        args.val_bag_size = args.bag_size
    if args.loss_fn != 'nll':
        args.n_label_bins = 0

    censorship_col = args.target_col.split('_')[0] + '_censorship'

    train_kwargs = dict(data_source=args.data_source,
                        survival_time_col=args.target_col,
                        censorship_col=censorship_col,
                        n_label_bins=args.n_label_bins,
                        label_bins=None,
                        bag_size=args.train_bag_size,
                        shuffle=True,
                        )

    # use the whole bag at test time
    val_kwargs = dict(data_source=args.data_source,
                      survival_time_col=args.target_col,
                      censorship_col=censorship_col,
                      n_label_bins=args.n_label_bins,
                      label_bins=None,
                      bag_size=args.val_bag_size,
                      shuffle=False,
                      )

    all_results, all_dumps = {}, {}

    # Cross-validation
    seed_torch(args.seed)
    csv_splits = read_splits(args)
    print('successfully read splits for: ', list(csv_splits.keys()))
    dataset_splits = build_datasets(csv_splits, 
                                    model_type=args.model_type,
                                    batch_size=args.batch_size,
                                    num_workers=args.num_workers,
                                    train_kwargs=train_kwargs,
                                    val_kwargs=val_kwargs)

    fold_results, fold_dumps = train(dataset_splits, args, mode='survival')

    for split, split_results in fold_results.items():
        all_results[split] = merge_dict({}, split_results) if (split not in all_results.keys()) else merge_dict(all_results[split], split_results)
        save_pkl(j_(args.results_dir, f'{split}_results.pkl'), fold_dumps[split]) # saves per-split, per-fold results to pkl
    
    final_dict = {}
    for split, split_results in all_results.items():
        final_dict.update({f'{metric}_{split}': array2list(val) for metric, val in split_results.items()})
    final_df = pd.DataFrame(final_dict)
    save_name = 'summary.csv'
    final_df.to_csv(j_(args.results_dir, save_name), index=False)
    with open(j_(args.results_dir, save_name + '.json'), 'w') as f:
        f.write(json.dumps(final_dict, sort_keys=True, indent=4))
    
    dump_path = j_(args.results_dir, 'all_dumps.h5')
    save_pkl(dump_path, fold_dumps)

    return final_dict

# Generic training settings
parser = argparse.ArgumentParser(description='Configurations for WSI Training')
### optimizer settings ###
parser.add_argument('--max_epochs', type=int, default=20,
                    help='maximum number of epochs to train (default: 20)')
parser.add_argument('--lr', type=float, default=1e-4,
                    help='learning rate')
parser.add_argument('--wd', type=float, default=1e-5,
                    help='weight decay')
parser.add_argument('--accum_steps', type=int, default=1,
                    help='grad accumulation steps')
parser.add_argument('--opt', type=str, default='adamW',
                    choices=['adamW', 'sgd', 'RAdam'])
parser.add_argument('--lr_scheduler', type=str,
                    choices=['cosine', 'linear', 'constant'], default='constant')
parser.add_argument('--warmup_steps', type=int,
                    default=-1, help='warmup iterations')
parser.add_argument('--warmup_epochs', type=int,
                    default=-1, help='warmup epochs')
parser.add_argument('--batch_size', type=int, default=1)

### misc ###
parser.add_argument('--print_every', default=100,
                    type=int, help='how often to print')
parser.add_argument('--seed', type=int, default=1,
                    help='random seed for reproducible experiment (default: 1)')
parser.add_argument('--num_workers', type=int, default=2)

### Earlystopper args ###
parser.add_argument('--early_stopping', type=int,
                    default=0, help='enable early stopping')
parser.add_argument('--es_min_epochs', type=int, default=3,
                    help='early stopping min epochs')
parser.add_argument('--es_patience', type=int, default=5,
                    help='early stopping min patience')
parser.add_argument('--es_metric', type=str, default='loss',
                    help='early stopping metric')

### model args ###
parser.add_argument('--model_type', type=str, choices=['H2T', 'ABMIL', 'TransMIL', 'SumMIL', 'OT', 'PANTHER', 'ProtoCount', 'DeepAttnMIL', 'ILRA'],
                    default='ABMIL',
                    help='type of model')
parser.add_argument('--emb_model_type', type=str, default='LinEmb_LR')
parser.add_argument('--ot_eps', default=0.1, type=float,
                    help='Strength for entropic constraint regularization for OT')
parser.add_argument('--model_config', type=str,
                    default='ABMIL_default', help="name of model config file")
parser.add_argument('--n_fc_layers', type=int)
parser.add_argument('--em_iter', type=int)
parser.add_argument('--tau', type=float)
parser.add_argument('--out_type', type=str, default='param_cat')

# Prototype related
parser.add_argument('--load_proto', action='store_true', default=False)
parser.add_argument('--proto_path', type=str)
parser.add_argument('--fix_proto', action='store_true', default=False)
parser.add_argument('--n_proto', type=int)

parser.add_argument('--in_dim', default=768, type=int,
                    help='dim of input features')
parser.add_argument('--in_dropout', default=0.1, type=float,
                    help='Probability of dropping out input features.')
parser.add_argument('--bag_size', type=int, default=-1)
parser.add_argument('--train_bag_size', type=int, default=-1)
parser.add_argument('--val_bag_size', type=int, default=-1)
parser.add_argument('--loss_fn', type=str, default='nll', choices=['nll', 'cox', 'sumo', 'ipcwls', 'rank'],
                    help='which loss function to use')
parser.add_argument('--nll_alpha', type=float, default=0,
                    help='Balance between censored / uncensored loss')

# experiment task / label args ###
parser.add_argument('--exp_code', type=str, default=None,
                    help='experiment code for saving results')
parser.add_argument('--task', type=str, default='unspecified_survival_task')
parser.add_argument('--target_col', type=str, default='os_survival_days')
parser.add_argument('--n_label_bins', type=int, default=4,
                    help='number of bins for event time discretization')

# dataset / split args ###
parser.add_argument('--data_source', type=str, default=None,
                    help='manually specify the data source')
parser.add_argument('--split_dir', type=str, default=None,
                    help='manually specify the set of splits to use')
parser.add_argument('--split_names', type=str, default='train,val,test',
                    help='delimited list for specifying names within each split')
parser.add_argument('--overwrite', action='store_true', default=False,
                    help='overwrite existing results')

# logging args ###
parser.add_argument('--results_dir', default='./results',
                    help='results directory (default: ./results)')
parser.add_argument('--tags', nargs='+', type=str, default=None,
                    help='tags for logging')
args = parser.parse_args()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if __name__ == "__main__":

    print('task: ', args.task)
    args.split_dir = j_('splits', args.split_dir)
    print('split_dir: ', args.split_dir)
    split_num = args.split_dir.split('/')[2].split('_k=')
    args.split_name_clean = args.split_dir.split('/')[2].split('_k=')[0]
    if len(split_num) > 1:
        args.split_k = int(split_num[1])
    else:
        args.split_k = 0

    if args.load_proto:
        assert os.path.isfile(args.proto_path), f"Path {args.proto_path} doesn't exist!"
        args.proto_fname = '/'.join(args.proto_path.split('/')[-2:])
        proto_fname_clean = '--'.join(args.proto_fname[:-4].split('/'))

    ### Allows you to pass in multiple data sources (separated by comma). If single data source, no change.
    args.data_source = [src for src in args.data_source.split(',')]
    check_params_same = []
    for src in args.data_source: 
        ### assert data source exists + extract feature name ###
        print('data source: ', src)
        assert os.path.isdir(src), f"data source must be a directory: {src} invalid"

        ### parse patching info ###
        feat_name = os.path.basename(src)
        mag, patch_size = extract_patching_info(os.path.dirname(src))
        if (mag < 0 or patch_size < 0):
            raise ValueError(f"invalid patching info parsed for {src}")
        check_params_same.append([feat_name, mag, patch_size])

        #### parse model name ####
        parsed = parse_model_name(feat_name) 
        parsed.update({'patch_mag': mag, 'patch_size': patch_size})
    
    try:
        check_params_same = pd.DataFrame(check_params_same, columns=['feats_name', 'mag', 'patch_size'])
        assert check_params_same.drop(['feats_name'],axis=1).drop_duplicates().shape[0] == 1
        print("All data sources have the same feature extraction parameters.")
    except:
        print("Data sources do not share the same feature extraction parameters. Exiting...")
        sys.exit()
        
    ### Updated parsed mdoel parameters in args.Namespace ###
    for key, val in parsed.items():
        setattr(args, key, val)
    
    ### setup results dir ###
    if args.exp_code is None:
        if args.model_config == 'PANTHER_default':
            exp_code = f"{args.split_name_clean}::{args.model_config}+{args.emb_model_type}::{args.loss_fn}::{feat_name}"
        else:
            exp_code = f"{args.split_name_clean}::{args.model_config}::{feat_name}"
    else:
        pass
    
    args.results_dir = j_(args.results_dir, 
                          args.task, 
                          f'k={args.split_k}', 
                          str(exp_code), 
                          str(exp_code)+f"::{get_current_time()}")

    os.makedirs(args.results_dir, exist_ok=True)

    print("\n################### Settings ###################")
    for key, val in vars(args).items():
        print("{}:  {}".format(key, val))

    with open(j_(args.results_dir, 'config.json'), 'w') as f:
        f.write(json.dumps(vars(args), sort_keys=True, indent=4))

    #### train ####
    results = main(args)

    print("FINISHED!\n\n\n")

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/.ipynb_checkpoints/main_prototype-checkpoint.py
"""
This will perform K-means clustering on the training data

Good reference for clustering
https://github.com/facebookresearch/faiss/wiki/FAQ#questions-about-training
"""

from __future__ import print_function

import argparse
import torch
from torch.utils.data import DataLoader
from wsi_datasets import WSIProtoDataset
from utils.utils import seed_torch, read_splits
from utils.file_utils import save_pkl
from utils.proto_utils import cluster

import os
from os.path import join as j_

def build_datasets(csv_splits, batch_size=1, num_workers=2, train_kwargs={}):
    dataset_splits = {}
    for k in csv_splits.keys(): # ['train']
        df = csv_splits[k]
        dataset_kwargs = train_kwargs.copy()
        dataset = WSIProtoDataset(df, **dataset_kwargs)

        batch_size = 1
        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)
        dataset_splits[k] = dataloader
        print(f'split: {k}, n: {len(dataset)}')

    return dataset_splits


def main(args):
    
    train_kwargs = dict(data_source=args.data_source)
       
    seed_torch(args.seed)
    csv_splits = read_splits(args)
    print('\nsuccessfully read splits for: ', list(csv_splits.keys()))

    dataset_splits = build_datasets(csv_splits,
                                    batch_size=1,
                                    num_workers=args.num_workers,
                                    train_kwargs=train_kwargs)

    print('\nInit Datasets...', end=' ')
    os.makedirs(j_(args.split_dir, 'prototypes'), exist_ok=True)

    loader_train = dataset_splits['train']
    
    _, weights = cluster(loader_train,
                            n_proto=args.n_proto,
                            n_iter=args.n_iter,
                            n_init=args.n_init,
                            feature_dim=args.in_dim,
                            mode=args.mode,
                            n_proto_patches=args.n_proto_patches,
                            use_cuda=True if torch.cuda.is_available() else False)
    

    save_fpath = j_(args.split_dir,
                    'prototypes',
                    f"prototypes_c{args.n_proto}_{args.data_source[0].split('/')[-2]}_{args.mode}_num_{args.n_proto_patches:.1e}.pkl")

    save_pkl(save_fpath, {'prototypes': weights})


# Generic training settings
parser = argparse.ArgumentParser(description='Configurations for WSI Training')
parser.add_argument('--seed', type=int, default=1,
                    help='random seed for reproducible experiment (default: 1)')
# model / loss fn args ###
parser.add_argument('--n_proto', type=int, help='Number of prototypes')
parser.add_argument('--n_proto_patches', type=int, default=10000,
                    help='Number of patches per prototype to use. Total patches = n_proto * n_proto_patches')
parser.add_argument('--n_init', type=int, default=5,
                    help='Number of different KMeans initialization (for FAISS)')
parser.add_argument('--n_iter', type=int, default=50,
                    help='Number of iterations for Kmeans clustering')
parser.add_argument('--in_dim', type=int)
parser.add_argument('--mode', type=str, choices=['kmeans', 'faiss'], default='kmeans')

# dataset / split args ###
parser.add_argument('--data_source', type=str, default=None,
                    help='manually specify the data source')
parser.add_argument('--split_dir', type=str, default=None,
                    help='manually specify the set of splits to use')
parser.add_argument('--split_names', type=str, default='train,val,test',
                    help='delimited list for specifying names within each split')
parser.add_argument('--num_workers', type=int, default=8)

args = parser.parse_args()

if __name__ == "__main__":
    args.split_dir = j_('splits', args.split_dir)
    args.split_name = os.path.basename(args.split_dir)
    print('split_dir: ', args.split_dir)

    args.data_source = [src for src in args.data_source.split(',')]
    results = main(args)

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/training/.ipynb_checkpoints/main_embedding-checkpoint.py
"""
This will construct unsupervised slide embedding

Good reference for clustering
https://github.com/facebookresearch/faiss/wiki/FAQ#questions-about-training
"""

from __future__ import print_function

import argparse
from torch.utils.data import DataLoader
from wsi_datasets import WSIProtoDataset
from utils.utils import seed_torch, read_splits
from utils.file_utils import save_pkl
from mil_models import prepare_emb
from mil_models import PrototypeTokenizer

import numpy as np

import pdb
import os
from os.path import join as j_

def build_datasets(csv_splits, batch_size=1, num_workers=2, train_kwargs={}):
    dataset_splits = {}
    for k in csv_splits.keys(): # ['train']
        df = csv_splits[k]
        dataset_kwargs = train_kwargs.copy()
        dataset = WSIProtoDataset(df, **dataset_kwargs)

        batch_size = 1
        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)
        dataset_splits[k] = dataloader
        print(f'split: {k}, n: {len(dataset)}')

    return dataset_splits


def main(args):
    
    train_kwargs = dict(data_source=args.data_source)
       
    seed_torch(args.seed)
    csv_splits = read_splits(args)
    print('\nsuccessfully read splits for: ', list(csv_splits.keys()))

    dataset_splits = build_datasets(csv_splits,
                                    batch_size=1,
                                    num_workers=args.num_workers,
                                    train_kwargs=train_kwargs)

    print('\nInit Datasets...', end=' ')
    os.makedirs(j_(args.split_dir, 'embeddings'), exist_ok=True)
    
    # Construct unsupervised slide-level embedding
    datasets, fpath = prepare_emb(dataset_splits, args, mode='emb')

    # Construct tokenized slide-level embedding
    if args.out_type == 'allcat':
        print("Generting Tokenized slide embeddings..")
        tokenizer = PrototypeTokenizer(args.model_type, args.out_type, args.n_proto)
        embeddings = {}
        for k, loader in datasets.items():
            prob, mean, cov = tokenizer(loader.dataset.X)
            embeddings[k] = {'prob': prob, 'mean': mean, 'cov': cov}
        fpath_new = fpath.rsplit('.', 1)[0] + '_tokenized.pkl'
        save_pkl(fpath_new, embeddings)


    print("\nSlide embedding construction finished!")



# Generic training settings
parser = argparse.ArgumentParser(description='Configurations for WSI Training')
parser.add_argument('--seed', type=int, default=1,
                    help='random seed for reproducible experiment (default: 1)')
# model / loss fn args ###
parser.add_argument('--n_proto', type=int, help='Number of prototypes')
parser.add_argument('--in_dim', type=int)
parser.add_argument('--model_type', type=str, choices=['H2T', 'OT', 'PANTHER', 'ProtoCount'],
                    help='type of embedding model')
parser.add_argument('--em_iter', type=int)
parser.add_argument('--tau', type=float)
parser.add_argument('--out_type', type=str)
parser.add_argument('--ot_eps', default=0.1, type=float,
                    help='Strength for entropic constraint regularization for OT')
parser.add_argument('--model_config', type=str,
                    default='ABMIL_default', help="name of model config file")

# dataset / split args ###
parser.add_argument('--data_source', type=str, default=None,
                    help='manually specify the data source')
parser.add_argument('--split_dir', type=str, default=None,
                    help='manually specify the set of splits to use')
parser.add_argument('--split_names', type=str, default='train,val,test',
                    help='delimited list for specifying names within each split')
parser.add_argument('--num_workers', type=int, default=8)

# Prototype related
parser.add_argument('--load_proto', action='store_true', default=False)
parser.add_argument('--proto_path', type=str)
parser.add_argument('--fix_proto', action='store_true', default=False)

args = parser.parse_args()

if __name__ == "__main__":
    args.split_dir = j_('splits', args.split_dir)
    args.split_name = os.path.basename(args.split_dir)
    print('split_dir: ', args.split_dir)

    args.data_source = [src for src in args.data_source.split(',')]

    if args.load_proto:
        assert os.path.exists(args.proto_path), f"The proto path {args.proto_path} doesn't exist!"

    results = main(args)

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/pandas_helper_funcs.py
import os
from os.path import join as j_
from os import listdir as ldir_
from os import scandir as sdir_
import shutil

from tqdm import tqdm

import numpy as np
import pandas
import pandas as pd
from pandas import Series

def series_int(s1: pandas.Series, s2: pandas.Series, dtype='O'):
    """
    Returns set intersection of two pd.Series (resets index).

    Args:
        s1 (pandas.Series): Series (of strings).
        s2 (pandas.Series): Series (of strings).

    Returns:
        (pandas.Series): set interesection of s1 and s2
    """
    return pd.Series(list(set(s1) & set(s2)), dtype=dtype)

def df_loc_col(df1: pandas.DataFrame, s1: pandas.Series, col_name: str, apply_sint=True, drop_orig_index=False):
    """
    Performs pandas.loc with column <col_name> as index of df1.

    Args:
        df1 (pandas.DataFrame): Dataframe.
        s1 (pandas.Series): Series (of strings).
        col_name (str): column to use as index for pandas.loc
        apply_sint (bool): Whether to take series intersection first.
        drop_orig_index (bool): Drops original index.

    Returns:
       (pandas.DataFrame): df1 subsetted by s1 using column <col_name>.
    """
    return df1.reset_index(drop=drop_orig_index).set_index(col_name).loc[series_int(df1[col_name], s1) if apply_sint else s1].reset_index(drop=False)

def series_ldir_int(path1, path2, exts=['.', '.'], add_ext=False):
    """
    Gets intersection of fnames (accounting for differing exts) in path1 and path2.

    Args:
        path1 (_type_): Path to directory of fnames.
        path2 (_type_): Path to directory of fnames.
        exts (list): Which exts to split for fnames in path1 and path2. Defaults to ['.', '.'].
        add_ext (bool, optional): Whether to add back in the extension. 
            If True, defaults to using extension of path1 (order matters). Defaults to False.

    Returns:
        (pd.Series): Intersection of fnames (accounting for differing exts) in path1 and path2
    """
    df1 = pd.Series(ldir_(path1)).str.rsplit(pat=exts[0], n=1, expand=True).set_axis(['fname', 'ext'], axis=1)
    df2 = pd.Series(ldir_(path2)).str.rsplit(pat=exts[1], n=1, expand=True).set_axis(['fname', 'ext'], axis=1)
    df = df_loc_col(df1, df2['fname'], col_name='fname', apply_sint=True, drop_orig_index=True)
    return df['fname']+exts[0]+df['ext'] if add_ext else df['fname']

def df_sdir(dataroot: str, cols=['fpath', 'fname', 'slide_id']):
    """
    Returns pd.DataFrame of the file paths and fnames of contents in dataroot.

    Args:
        dataroot (str): path to files.

    Returns:
        (pandas.Dataframe): pd.DataFrame of the file paths and fnames of contents in dataroot (make default cols: ['fpath', 'fname_ext', 'fname_noext']?)
    """
    return pd.DataFrame([(e.path, e.name, os.path.splitext(e.name)[0]) for e in sdir_(dataroot)], columns=cols)


# TODO: Fix doc + also make function for ldir_diff
def series_diff(s1, s2, dtype='O'):
    r"""
    Returns set difference of two pd.Series.
    """
    return pd.Series(list(set(s1).difference(set(s2))), dtype=dtype)


def transfer_dir2dir_shutil(dataroot: str, saveroot: str, subset_fnames:str=None, lim: int=None):
    r"""
    Transfer files from dir2dir
    
    Args:
    - dataroot (str): Source folder from which you want to transfer files from
    - subset_fnames (list): List of filenames
    - saveroot (str): Destination folder from which you want to transfer files to
    
    Return:
    - None
    """
    from tqdm import tqdm
    from os.path import join as j_
    if lim == None:
        pbar = tqdm(os.listdir(dataroot))
    else:
        pbar = tqdm(os.listdir(dataroot)[:lim])
    
    missing = []
    for fname in pbar:
        pbar.set_description(f'Copying: {fname}')

        src = j_(dataroot, fname)
        dst = j_(saveroot, fname)

        if not os.path.isfile(src):
            missing.append(fname)
        elif os.path.isfile(dst):
            continue
        else:
            shutil.copyfile(src=src, dst=dst)
        pass
    
    print('Num Missing:', len(missing))
    print("Missing Files:", missing)

series_intersection = series_int
series_difference = series_diff

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/utils.py
import pdb
import math
import os
from os.path import join as j_
import pickle
import pandas as pd
import datetime
import torch
import numpy as np
import torch.nn as nn
from torch.utils.data import DataLoader, sampler
import torch.optim as optim
import logging

from transformers import (get_constant_schedule_with_warmup, 
                         get_linear_schedule_with_warmup, 
                         get_cosine_schedule_with_warmup)

import re
import torch.nn.functional as F



def get_current_time():
    now = datetime.datetime.now()
    year = now.year % 100  # convert to 2-digit year
    month = now.month
    day = now.day
    hour = now.hour
    minute = now.minute
    second = now.second
    return f"{year:02d}-{month:02d}-{day:02d}-{hour:02d}-{minute:02d}-{second:02d}"

def extract_patching_info(s):
    match = re.search(r"extracted_mag(\d+)x_patch(\d+)_fp", s)
    mag, patch_size = -1, -1
    if match:
        mag = int(match.group(1))
        patch_size = int(match.group(2))
        return mag, patch_size


def parse_model_name(model_name, ckpt=None, inference_prec=None):
    # 'extracted-vit_base_patch16_224.ibot.mgb100m20X_bs1024_cropadjust_opnorm_wd0.04_0012_fp16'

    # get inference precision
    if inference_prec is None:
        inference_prec = 'fp32'
        if model_name.endswith('_fp16'):
            inference_prec = 'fp16'
            model_name = model_name[:-len('_fp16')]

    model_name = model_name.replace('extracted-', '')
    parsed = model_name.split('.', maxsplit=2)
    enc = model_name
    algo = ''
    exp = ''
    if len(parsed) >= 3:
        enc = parsed[0]
        algo = parsed[1]
        exp = '.'.join(parsed[2:])

    # get ckpt
    if ckpt is None:
        exp_parsed = exp.split('.')
        ckpt = exp_parsed[-1].split('_')[-1]
        if ckpt.isnumeric():
            ckpt = int(ckpt)
            exp = '.'.join(exp_parsed[:-1]) + '_'.join(exp_parsed[-1].split('_')[:-1])
        else:
            ckpt = -1
    else:
        if str(ckpt).isnumeric():
            ckpt = int(ckpt)
        else:
            ckpt = -1
    return dict(pretrain_enc=enc, 
                pretrain_algo=algo, 
                pretrain_exp=exp, 
                pretrain_ckpt=ckpt,
                inference_prec=inference_prec)

def merge_dict(main_dict, new_dict):
    for k, v in new_dict.items():
        if k not in main_dict:
            main_dict[k] = []
        main_dict[k].append(v)
    return main_dict


def array2list(x):
    if isinstance(x, np.ndarray):
        return x.tolist()
    return list(x)

def summarize_reulsts(results_dict, ignore_keys = ['folds']):
    summary = {}
    for k, v in results_dict.items():
        if k in ignore_keys: continue
        summary[f"{k}_avg"] = np.mean(v)
        # summary[f"{k}_std"] = np.std(v)
    return summary


def seed_torch(seed=7):
    import random
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if device.type == 'cuda':
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def read_splits(args, fold_idx=None):
    splits_csvs = {}
    split_names = args.split_names.split(',')
    print(f"Using the following split names: {split_names}")
    for split in split_names:
        print(args.split_dir, f'{split}.csv')
        if fold_idx is not None:
            split_path = j_(args.split_dir, f'{split}_{fold_idx}.csv')
        else:
            split_path = j_(args.split_dir, f'{split}.csv')
            print('hhh',split_path)
        
        if os.path.isfile(split_path):
            df = pd.read_csv(split_path)#.sample(frac=1, random_state=0).head(25).reset_index(drop=True)
            assert 'Unnamed: 0' not in df.columns
            splits_csvs[split] = df
    print(splits_csvs)
    return splits_csvs


class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, name='unk', fmt=':f'):
        self.name = name
        self.fmt = fmt
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

    def __str__(self):
        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
        return fmtstr.format(**self.__dict__)
    
def get_lr_scheduler(args, optimizer, dataloader):
    scheduler_name = args.lr_scheduler
    warmup_steps = args.warmup_steps
    warmup_epochs = args.warmup_epochs
    epochs = args.max_epochs if hasattr(args, 'max_epochs') else args.epochs
    assert not (warmup_steps > 0 and warmup_epochs > 0), "Cannot have both warmup steps and epochs"
    accum_steps = args.accum_steps
    if warmup_steps > 0:
        warmup_steps = warmup_steps
    elif warmup_epochs > 0:
        warmup_steps = warmup_epochs * (len(dataloader) // accum_steps)
    else:
        warmup_steps = 0
    if scheduler_name=='constant':
        lr_scheduler = get_constant_schedule_with_warmup(optimizer=optimizer,
        num_warmup_steps=warmup_steps)
    elif scheduler_name=='cosine':
        lr_scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=(len(dataloader) // accum_steps * epochs),
        )
    elif scheduler_name=='linear':
        lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=(len(dataloader) // accum_steps) * epochs,
        )
    return lr_scheduler


def get_optim(args, model=None, parameters=None):
    def exclude(
        n, p): return p.ndim < 2 or "bn" in n or "ln" in n or "bias" in n or 'logit_scale' in n

    def include(n, p): return not exclude(n, p)

    if parameters is None:
        named_parameters = list(model.named_parameters())
        gain_or_bias_params = [
            p for n, p in named_parameters if exclude(n, p) and p.requires_grad]
        rest_params = [p for n, p in named_parameters if include(
            n, p) and p.requires_grad]
        parameters = [
            {"params": gain_or_bias_params, "weight_decay": 0.},
            {"params": rest_params, "weight_decay": args.wd},
        ]

    if args.opt == "adamW":
        optimizer = optim.AdamW(parameters, lr=args.lr)
    elif args.opt == 'sgd':
        optimizer = optim.SGD(parameters, lr=args.lr, momentum=0.9)
    elif args.opt == 'RAdam':
        optimizer = optim.RAdam(parameters, lr=args.lr)
    else:
        raise NotImplementedError
    return optimizer
 

def print_network(net):
    num_params = 0
    num_params_train = 0

    logging.info(str(net))
    # print(str(net))
    for param in net.parameters():
        n = param.numel()
        num_params += n
        if param.requires_grad:
            num_params_train += n

    logging.info(f'Total number of parameters: {num_params}')
    logging.info(f'Total number of trainable parameters: {num_params_train}')

    # print('Total number of parameters: %d' % num_params)
    # print('Total number of trainable parameters: %d' % num_params_train)


class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, save_dir, patience=20, min_stop_epoch=50, verbose=False, better='min'):
        """
        train_args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            min_stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement. 
                            Default: False
        """
        self.patience = patience
        self.patience_counter = 0
        self.min_stop_epoch = min_stop_epoch
        self.better = better
        self.verbose = verbose
        self.best_score = None
        self.save_dir = save_dir

        if better == 'min':
            self.best_score = np.Inf
        else:
            self.best_score = -np.Inf
        self.early_stop = False
        self.counter = 0

    def is_new_score_better(self, score):
        if self.better == 'min':
            return score < self.best_score
        else:
            return score > self.best_score

    def __call__(self, epoch, score, save_ckpt_fn, save_ckpt_kwargs):
        is_better = self.is_new_score_better(score)
        if is_better:
            print(
                f'score improved ({self.best_score:.6f} --> {score:.6f}).  Saving model ...')
            self.save_checkpoint(save_ckpt_fn, save_ckpt_kwargs)
            self.counter = 0
            self.best_score = score
        else:
            self.counter += 1
            print(
                f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience and epoch >= (self.min_stop_epoch - 1):
                self.early_stop = True
        return self.early_stop

    def save_checkpoint(self, save_ckpt_fn, save_ckpt_kwargs):
        '''Saves model when score improves.'''
        if 'save_dir' in save_ckpt_kwargs:
            save_ckpt_fn(**save_ckpt_kwargs)
        else:
            save_ckpt_fn(save_dir=self.save_dir, **save_ckpt_kwargs)


def save_checkpoint(config, epoch, model, score, save_dir, fname=None):
    save_state = {'model': model.state_dict(),
                  'score': score,
                  'epoch': epoch,
                  'config': config}

    if fname is None:
        save_path = j_(save_dir, f'ckpt_epoch_{epoch}.pth')
    else:
        save_path = j_(save_dir, fname)

    torch.save(save_state, save_path)


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/file_utils.py
import pickle
import h5py
from os.path import join as j_
import numpy as np
import pandas as pd
import pdb

def save_pkl(filename, save_object):
	with open(filename,'wb') as f:
	    pickle.dump(save_object, f)

def load_pkl(filename):
	loader = open(filename,'rb')
	file = pickle.load(loader)
	loader.close()
	return file

def save_hdf5(output_path, asset_dict, attr_dict= None, mode='a'):
    file = h5py.File(output_path, mode)
    for key, val in asset_dict.items():
        data_shape = val.shape
        if key not in file:
            data_type = val.dtype
            if data_type == np.object_: data_type = h5py.string_dtype(encoding='utf-8')
            chunk_shape = (1, ) + data_shape[1:]
            maxshape = (None, ) + data_shape[1:]

            try:
                dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)
                dset[:] = val
                if attr_dict is not None:
                    if key in attr_dict.keys():
                        for attr_key, attr_val in attr_dict[key].items():
                            dset.attrs[attr_key] = attr_val
            except:
                 print(f"Error encoding {key} of dtype {data_type} into hdf5")
        else:
            dset = file[key]
            dset.resize(len(dset) + data_shape[0], axis=0)
            dset[-data_shape[0]:] = val
    file.close()
    return output_path

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/losses.py
import torch.nn as nn
import torch
import numpy as np
import pdb
from itertools import combinations
import torch.nn.functional as F

# Optimal Transport loss
d_cosine = nn.CosineSimilarity(dim=-1, eps=1e-8)

# Survival loss
class NLLSurvLoss(nn.Module):
    """
    The negative log-likelihood loss function for the discrete time to event model (Zadeh and Schmid, 2020).
    Code borrowed from https://github.com/mahmoodlab/Patch-GCN/blob/master/utils/utils.py
    Parameters
    ----------
    alpha: float
        TODO: document
    eps: float
        Numerical constant; lower bound to avoid taking logs of tiny numbers.
    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']
    """
    def __init__(self, alpha=0.0, eps=1e-7, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.eps = eps
        self.reduction = reduction

    def __call__(self, logits, times, censorships):
        """
        Parameters
        ----------
        h: (n_batches, n_classes)
            The neural network output discrete survival predictions such that hazards = sigmoid(h).
        y_c: (n_batches, 2) or (n_batches, 3)
            The true time bin label (first column) and censorship indicator (second column).
        """

        return nll_loss(logits=logits, y=times, c=censorships,
                        alpha=self.alpha, eps=self.eps,
                        reduction=self.reduction)


# TODO: document better and clean up
def nll_loss(logits, y, c, alpha=0.0, eps=1e-7, reduction='mean'):
    """
    The negative log-likelihood loss function for the discrete time to event model (Zadeh and Schmid, 2020).
    Code borrowed from https://github.com/mahmoodlab/Patch-GCN/blob/master/utils/utils.py
    Parameters
    ----------
    logits: (n_batches, n_classes)
        The neural network output discrete survival predictions such that hazards = sigmoid(logits).
    y: (n_batches, )
        The true time bin index label.
    c: (n_batches, )
        The censoring status indicator.
    alpha: float
        TODO: document
    eps: float
        Numerical constant; lower bound to avoid taking logs of tiny numbers.
    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']
    References
    ----------
    Zadeh, S.G. and Schmid, M., 2020. Bias in cross-entropy-based training of deep survival networks. IEEE transactions on pattern analysis and machine intelligence.
    """
    # print("h shape", h.shape)

    # make sure these are ints
    y = y.long()
    c = c.long()

    hazards = torch.sigmoid(logits)

    S = torch.cumprod(1 - hazards, dim=1)

    S_padded = torch.cat([torch.ones_like(c), S], 1)
    # S(-1) = 0, all patients are alive from (-inf, 0) by definition
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    # hazards[y] = hazards(1)
    # S[1] = S(1)

    s_prev = torch.gather(S_padded, dim=1, index=y).clamp(min=eps)
    h_this = torch.gather(hazards, dim=1, index=y).clamp(min=eps)
    s_this = torch.gather(S_padded, dim=1, index=y+1).clamp(min=eps)

    uncensored_loss = -(1 - c) * (torch.log(s_prev) + torch.log(h_this))
    censored_loss = - c * torch.log(s_this)

    neg_l = censored_loss + uncensored_loss
    
    if alpha is not None:
        loss = (1 - alpha) * neg_l + alpha * uncensored_loss

    if reduction == 'mean':
        loss = loss.mean()
        censored_loss = censored_loss.mean()
        uncensored_loss = uncensored_loss.mean()
    elif reduction == 'sum':
        loss = loss.sum()
        censored_loss = censored_loss.sum()
        uncensored_loss = uncensored_loss.sum()
    else:
        raise ValueError("Bad input for reduction: {}".format(reduction))

    return {'loss': loss, 'uncensored_loss': uncensored_loss, 'censored_loss': censored_loss}

def partial_ll_loss(lrisks, survival_times, event_indicators):
    """
    lrisks: log risks, B x 1
    survival_times: time bin, B x 1
    event_indicators: event indicator, B x 1
    """    
    num_uncensored = torch.sum(event_indicators, 0)
    if num_uncensored.item() == 0:
        return {'loss': torch.sum(lrisks) * 0}
    
    survival_times = survival_times.squeeze(1)
    event_indicators = event_indicators.squeeze(1)
    lrisks = lrisks.squeeze(1)

    sindex = torch.argsort(-survival_times)
    survival_times = survival_times[sindex]
    event_indicators = event_indicators[sindex]
    lrisks = lrisks[sindex]

    log_risk_stable = torch.logcumsumexp(lrisks, 0)

    likelihood = lrisks - log_risk_stable
    uncensored_likelihood = likelihood * event_indicators
    logL = -torch.sum(uncensored_likelihood)
    # negative average log-likelihood
    return {'loss': logL / num_uncensored}







class CoxLoss(nn.Module):
    """
    """
    def __init__(self):
        super().__init__()

    def __call__(self, logits, times, censorships):
        return partial_ll_loss(lrisks = logits, survival_times=times, event_indicators=(1-censorships).float())


class SurvRankingLoss(nn.Module):
    """
    Implements the surivival ranking loss which approximates the negaive c-index; see Section 3.2 of (Luck et al, 2018) -- but be careful of the typo in their c-index formula.

    The c-index for risk scores z_1, ..., z_n is given by

    c_index = sum_{(a, b) are comparable} 1(z_a > z_b)

    where (a, b) are comparable if and only if a's event is observed and a has a strictly lower survival time than b. This ignores ties.

    We replace the indicator with a continous approximation

    1(z_a - z_b > 0 ) ~= phi(z_a - z_b)

    e.g. where phi(r) is a Relu or sigmoid function.

    The loss function we want to minimize is then

    - sum_{(a, b) are comparable} phi(z_a - z_b)

    where z_a, z_b are the risk scores output by the network.

    Parameters
    ----------
    phi: str
        Which indicator approximation to use. Must be one of ['relu', 'sigmoid'].

    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']

    References
    ----------
    Luck, M., Sylvain, T., Cohen, J.P., Cardinal, H., Lodi, A. and Bengio, Y., 2018. Learning to rank for censored survival data. arXiv preprint arXiv:1806.01984.
    """

    def __init__(self, phi='sigmoid', reduction='mean'):
        super().__init__()

        assert phi in ['sigmoid', 'relu']
        assert reduction in ['mean', 'sum']
        self.phi = phi
        self.reduction = reduction

    def forward(self, z, times, censorships):
        """
        Parameters
        ----------
        z: (batch_size, 1)
            The predicted risk scores.

        c_t: (batch_size, 2)
            first element: censorship
            second element: survival time
        """
        batch_size = z.shape[0]
        if batch_size == 1:
            # raise NotImplementedError("Batch size must be at least 2")
            return {'loss': torch.tensor(-1e5)}

        # censorship, times = c_t[:, 0], c_t[:, 1]
        events = 1 - censorships

        ##############################
        # determine comparable pairs #
        ##############################
        Z_more_risky = []
        Z_less_risky = []
        for (idx_a, idx_b) in combinations(range(batch_size), 2):
            time_a, event_a = times[idx_a], events[idx_a]
            time_b, event_b = times[idx_b], events[idx_b]

            if time_a < time_b and event_a:
                # a and b are comparable, a is more risky
                Z_more_risky.append(z[idx_a])
                Z_less_risky.append(z[idx_b])

            elif time_b < time_a and event_b:
                # a and b are comparable, b is more risky
                Z_more_risky.append(z[idx_b])
                Z_less_risky.append(z[idx_a])

        # if there are no comparable pairs then just return zero
        if len(Z_less_risky) == 0:
            # TODO: perhaps return None?
            return {'loss': None}

        Z_more_risky = torch.stack(Z_more_risky)
        Z_less_risky = torch.stack(Z_less_risky)

        # compute approximate c indices
        r = Z_more_risky - Z_less_risky
        if self.phi == 'sigmoid':
            approx_c_indices = torch.sigmoid(r)

        elif self.phi == 'relu':
            approx_c_indices = torch.relu(r)

        # negative mean/sum of c-indices
        if self.reduction == 'mean':
            return {'loss': - approx_c_indices.mean()}
        if self.reduction == 'sum':
            return {'loss': -approx_c_indices.sum()}

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/scheduler.py
import numpy as np


def assign_learning_rate(optimizer, new_lr):
    for param_group in optimizer.param_groups:
        param_group["lr"] = new_lr


def _warmup_lr(base_lr, warmup_length, step):
    return base_lr * (step + 1) / warmup_length


def const_lr(optimizer, base_lr, warmup_length, steps):
    def _lr_adjuster(step):
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            lr = base_lr
        assign_learning_rate(optimizer, lr)
        return lr
    return _lr_adjuster


def const_lr_cooldown(optimizer, base_lr, warmup_length, steps, cooldown_steps, cooldown_power=1.0, cooldown_end_lr=0.):
    def _lr_adjuster(step):
        start_cooldown_step = steps - cooldown_steps
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            if step < start_cooldown_step:
                lr = base_lr
            else:
                e = step - start_cooldown_step
                es = steps - start_cooldown_step
                # linear decay if power == 1; polynomial decay otherwise;
                decay = (1 - (e/es)) ** cooldown_power
                lr = decay * (base_lr - cooldown_end_lr) + cooldown_end_lr
        assign_learning_rate(optimizer, lr)
        return lr
    return _lr_adjuster


def cosine_lr(optimizer, base_lr, warmup_length, steps):
    def _lr_adjuster(step):
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            e = step - warmup_length
            es = steps - warmup_length
            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * base_lr
        assign_learning_rate(optimizer, lr)
        return lr
    return _lr_adjuster


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/.ipynb_checkpoints/losses-checkpoint.py
import torch.nn as nn
import torch
import numpy as np
import pdb
from itertools import combinations
import torch.nn.functional as F

# Optimal Transport loss
d_cosine = nn.CosineSimilarity(dim=-1, eps=1e-8)

# Survival loss
class NLLSurvLoss(nn.Module):
    """
    The negative log-likelihood loss function for the discrete time to event model (Zadeh and Schmid, 2020).
    Code borrowed from https://github.com/mahmoodlab/Patch-GCN/blob/master/utils/utils.py
    Parameters
    ----------
    alpha: float
        TODO: document
    eps: float
        Numerical constant; lower bound to avoid taking logs of tiny numbers.
    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']
    """
    def __init__(self, alpha=0.0, eps=1e-7, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.eps = eps
        self.reduction = reduction

    def __call__(self, logits, times, censorships):
        """
        Parameters
        ----------
        h: (n_batches, n_classes)
            The neural network output discrete survival predictions such that hazards = sigmoid(h).
        y_c: (n_batches, 2) or (n_batches, 3)
            The true time bin label (first column) and censorship indicator (second column).
        """

        return nll_loss(logits=logits, y=times, c=censorships,
                        alpha=self.alpha, eps=self.eps,
                        reduction=self.reduction)


# TODO: document better and clean up
def nll_loss(logits, y, c, alpha=0.0, eps=1e-7, reduction='mean'):
    """
    The negative log-likelihood loss function for the discrete time to event model (Zadeh and Schmid, 2020).
    Code borrowed from https://github.com/mahmoodlab/Patch-GCN/blob/master/utils/utils.py
    Parameters
    ----------
    logits: (n_batches, n_classes)
        The neural network output discrete survival predictions such that hazards = sigmoid(logits).
    y: (n_batches, )
        The true time bin index label.
    c: (n_batches, )
        The censoring status indicator.
    alpha: float
        TODO: document
    eps: float
        Numerical constant; lower bound to avoid taking logs of tiny numbers.
    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']
    References
    ----------
    Zadeh, S.G. and Schmid, M., 2020. Bias in cross-entropy-based training of deep survival networks. IEEE transactions on pattern analysis and machine intelligence.
    """
    # print("h shape", h.shape)

    # make sure these are ints
    y = y.long()
    c = c.long()

    hazards = torch.sigmoid(logits)

    S = torch.cumprod(1 - hazards, dim=1)

    S_padded = torch.cat([torch.ones_like(c), S], 1)
    # S(-1) = 0, all patients are alive from (-inf, 0) by definition
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    # hazards[y] = hazards(1)
    # S[1] = S(1)

    s_prev = torch.gather(S_padded, dim=1, index=y).clamp(min=eps)
    h_this = torch.gather(hazards, dim=1, index=y).clamp(min=eps)
    s_this = torch.gather(S_padded, dim=1, index=y+1).clamp(min=eps)

    uncensored_loss = -(1 - c) * (torch.log(s_prev) + torch.log(h_this))
    censored_loss = - c * torch.log(s_this)

    neg_l = censored_loss + uncensored_loss
    
    if alpha is not None:
        loss = (1 - alpha) * neg_l + alpha * uncensored_loss

    if reduction == 'mean':
        loss = loss.mean()
        censored_loss = censored_loss.mean()
        uncensored_loss = uncensored_loss.mean()
    elif reduction == 'sum':
        loss = loss.sum()
        censored_loss = censored_loss.sum()
        uncensored_loss = uncensored_loss.sum()
    else:
        raise ValueError("Bad input for reduction: {}".format(reduction))

    return {'loss': loss, 'uncensored_loss': uncensored_loss, 'censored_loss': censored_loss}

def partial_ll_loss(lrisks, survival_times, event_indicators):
    """
    lrisks: log risks, B x 1
    survival_times: time bin, B x 1
    event_indicators: event indicator, B x 1
    """    
    num_uncensored = torch.sum(event_indicators, 0)
    if num_uncensored.item() == 0:
        return {'loss': torch.sum(lrisks) * 0}
    
    survival_times = survival_times.squeeze(1)
    event_indicators = event_indicators.squeeze(1)
    lrisks = lrisks.squeeze(1)

    sindex = torch.argsort(-survival_times)
    survival_times = survival_times[sindex]
    event_indicators = event_indicators[sindex]
    lrisks = lrisks[sindex]

    log_risk_stable = torch.logcumsumexp(lrisks, 0)

    likelihood = lrisks - log_risk_stable
    uncensored_likelihood = likelihood * event_indicators
    logL = -torch.sum(uncensored_likelihood)
    # negative average log-likelihood
    return {'loss': logL / num_uncensored}







class CoxLoss(nn.Module):
    """
    """
    def __init__(self):
        super().__init__()

    def __call__(self, logits, times, censorships):
        return partial_ll_loss(lrisks = logits, survival_times=times, event_indicators=(1-censorships).float())


class SurvRankingLoss(nn.Module):
    """
    Implements the surivival ranking loss which approximates the negaive c-index; see Section 3.2 of (Luck et al, 2018) -- but be careful of the typo in their c-index formula.

    The c-index for risk scores z_1, ..., z_n is given by

    c_index = sum_{(a, b) are comparable} 1(z_a > z_b)

    where (a, b) are comparable if and only if a's event is observed and a has a strictly lower survival time than b. This ignores ties.

    We replace the indicator with a continous approximation

    1(z_a - z_b > 0 ) ~= phi(z_a - z_b)

    e.g. where phi(r) is a Relu or sigmoid function.

    The loss function we want to minimize is then

    - sum_{(a, b) are comparable} phi(z_a - z_b)

    where z_a, z_b are the risk scores output by the network.

    Parameters
    ----------
    phi: str
        Which indicator approximation to use. Must be one of ['relu', 'sigmoid'].

    reduction: str
        Do we sum or average the loss function over the batches. Must be one of ['mean', 'sum']

    References
    ----------
    Luck, M., Sylvain, T., Cohen, J.P., Cardinal, H., Lodi, A. and Bengio, Y., 2018. Learning to rank for censored survival data. arXiv preprint arXiv:1806.01984.
    """

    def __init__(self, phi='sigmoid', reduction='mean'):
        super().__init__()

        assert phi in ['sigmoid', 'relu']
        assert reduction in ['mean', 'sum']
        self.phi = phi
        self.reduction = reduction

    def forward(self, z, times, censorships):
        """
        Parameters
        ----------
        z: (batch_size, 1)
            The predicted risk scores.

        c_t: (batch_size, 2)
            first element: censorship
            second element: survival time
        """
        batch_size = z.shape[0]
        if batch_size == 1:
            # raise NotImplementedError("Batch size must be at least 2")
            return {'loss': torch.tensor(-1e5)}

        # censorship, times = c_t[:, 0], c_t[:, 1]
        events = 1 - censorships

        ##############################
        # determine comparable pairs #
        ##############################
        Z_more_risky = []
        Z_less_risky = []
        for (idx_a, idx_b) in combinations(range(batch_size), 2):
            time_a, event_a = times[idx_a], events[idx_a]
            time_b, event_b = times[idx_b], events[idx_b]

            if time_a < time_b and event_a:
                # a and b are comparable, a is more risky
                Z_more_risky.append(z[idx_a])
                Z_less_risky.append(z[idx_b])

            elif time_b < time_a and event_b:
                # a and b are comparable, b is more risky
                Z_more_risky.append(z[idx_b])
                Z_less_risky.append(z[idx_a])

        # if there are no comparable pairs then just return zero
        if len(Z_less_risky) == 0:
            # TODO: perhaps return None?
            return {'loss': None}

        Z_more_risky = torch.stack(Z_more_risky)
        Z_less_risky = torch.stack(Z_less_risky)

        # compute approximate c indices
        r = Z_more_risky - Z_less_risky
        if self.phi == 'sigmoid':
            approx_c_indices = torch.sigmoid(r)

        elif self.phi == 'relu':
            approx_c_indices = torch.relu(r)

        # negative mean/sum of c-indices
        if self.reduction == 'mean':
            return {'loss': - approx_c_indices.mean()}
        if self.reduction == 'sum':
            return {'loss': -approx_c_indices.sum()}

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/.ipynb_checkpoints/proto_utils-checkpoint.py
"""
All the functions related to clustering and slide embedding construction
"""

import pdb
import os
from utils.file_utils import save_pkl, load_pkl
import numpy as np
import time
from sklearn.cluster import KMeans
from tqdm import tqdm
import torch


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def cluster(data_loader, n_proto, n_iter, n_init=5, feature_dim=1024, n_proto_patches=50000, mode='kmeans', use_cuda=False):
    """
    K-Means clustering on embedding space

    For further details on FAISS,
    https://github.com/facebookresearch/faiss/wiki/Faiss-building-blocks:-clustering,-PCA,-quantization
    """
    n_patches = 0

    n_total = n_proto * n_proto_patches

    # Sample equal number of patch features from each WSI
    try:
        n_patches_per_batch = (n_total + len(data_loader) - 1) // len(data_loader)
    except:
        n_patches_per_batch = 1000

    print(f"Sampling maximum of {n_proto * n_proto_patches} patches: {n_patches_per_batch} each from {len(data_loader)}")

    patches = torch.Tensor(n_total, feature_dim)

    for batch in tqdm(data_loader):
        if n_patches >= n_total:
            continue

        data = batch['img'] # (n_batch, n_instances, instance_dim)

        with torch.no_grad():
            out = data.reshape(-1, data.shape[-1])[:n_patches_per_batch]  # Remove batch dim

        size = out.size(0)
        if n_patches + size > n_total:
            size = n_total - n_patches
            out = out[:size]
        patches[n_patches: n_patches + size] = out
        n_patches += size

    print(f"\nTotal of {n_patches} patches aggregated")

    s = time.time()
    if mode == 'kmeans':
        print("\nUsing Kmeans for clustering...")
        print(f"\n\tNum of clusters {n_proto}, num of iter {n_iter}")
        kmeans = KMeans(n_clusters=n_proto, max_iter=n_iter)
        kmeans.fit(patches[:n_patches].cpu())
        weight = kmeans.cluster_centers_[np.newaxis, ...]

    elif mode == 'faiss':
        assert use_cuda, f"FAISS requires access to GPU. Please enable use_cuda"
        try:
            import faiss
        except ImportError:
            print("FAISS not installed. Please use KMeans option!")
            raise
        
        numOfGPUs = torch.cuda.device_count()
        print(f"\nUsing Faiss Kmeans for clustering with {numOfGPUs} GPUs...")
        print(f"\tNum of clusters {n_proto}, num of iter {n_iter}")

        kmeans = faiss.Kmeans(patches.shape[1], 
                              n_proto, 
                              niter=n_iter, 
                              nredo=n_init,
                              verbose=True, 
                              max_points_per_centroid=n_proto_patches,
                              gpu=numOfGPUs)
        kmeans.train(patches[:n_patches].numpy())
        weight = kmeans.centroids[np.newaxis, ...]

    else:
        raise NotImplementedError(f"Clustering not implemented for {mode}!")

    e = time.time()
    print(f"\nClustering took {e-s} seconds!")

    return n_patches, weight

def check_prototypes(n_proto, embed_dim, load_proto, proto_path):
    """
    Check validity of the prototypes
    """
    print(proto_path)
    print(proto_path.endswith)
    if load_proto:
        assert os.path.exists(proto_path), "{} does not exist!".format(proto_path)
        
        if proto_path.endswith('pkl'):
            prototypes = load_pkl(proto_path)['prototypes'].squeeze()
            print(prototypes.shape[0])

        elif proto_path.endswith('npy'):
            prototypes = np.load(proto_path)
            print(prototypes.shape[0])
        assert (n_proto == prototypes.shape[0]) and (embed_dim == prototypes.shape[1]),\
            "Prototype dimensions do not match! Params: ({}, {}) Suplied: ({}, {})".format(n_proto,
                                                                                           embed_dim,
                                                                                           prototypes.shape[0],
                                                                                           prototypes.shape[1])



# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/.ipynb_checkpoints/utils-checkpoint.py
import pdb
import math
import os
from os.path import join as j_
import pickle
import pandas as pd
import datetime
import torch
import numpy as np
import torch.nn as nn
from torch.utils.data import DataLoader, sampler
import torch.optim as optim
import logging

from transformers import (get_constant_schedule_with_warmup, 
                         get_linear_schedule_with_warmup, 
                         get_cosine_schedule_with_warmup)

import re
import torch.nn.functional as F



def get_current_time():
    now = datetime.datetime.now()
    year = now.year % 100  # convert to 2-digit year
    month = now.month
    day = now.day
    hour = now.hour
    minute = now.minute
    second = now.second
    return f"{year:02d}-{month:02d}-{day:02d}-{hour:02d}-{minute:02d}-{second:02d}"

def extract_patching_info(s):
    match = re.search(r"extracted_mag(\d+)x_patch(\d+)_fp", s)
    mag, patch_size = -1, -1
    if match:
        mag = int(match.group(1))
        patch_size = int(match.group(2))
        return mag, patch_size


def parse_model_name(model_name, ckpt=None, inference_prec=None):
    # 'extracted-vit_base_patch16_224.ibot.mgb100m20X_bs1024_cropadjust_opnorm_wd0.04_0012_fp16'

    # get inference precision
    if inference_prec is None:
        inference_prec = 'fp32'
        if model_name.endswith('_fp16'):
            inference_prec = 'fp16'
            model_name = model_name[:-len('_fp16')]

    model_name = model_name.replace('extracted-', '')
    parsed = model_name.split('.', maxsplit=2)
    enc = model_name
    algo = ''
    exp = ''
    if len(parsed) >= 3:
        enc = parsed[0]
        algo = parsed[1]
        exp = '.'.join(parsed[2:])

    # get ckpt
    if ckpt is None:
        exp_parsed = exp.split('.')
        ckpt = exp_parsed[-1].split('_')[-1]
        if ckpt.isnumeric():
            ckpt = int(ckpt)
            exp = '.'.join(exp_parsed[:-1]) + '_'.join(exp_parsed[-1].split('_')[:-1])
        else:
            ckpt = -1
    else:
        if str(ckpt).isnumeric():
            ckpt = int(ckpt)
        else:
            ckpt = -1
    return dict(pretrain_enc=enc, 
                pretrain_algo=algo, 
                pretrain_exp=exp, 
                pretrain_ckpt=ckpt,
                inference_prec=inference_prec)

def merge_dict(main_dict, new_dict):
    for k, v in new_dict.items():
        if k not in main_dict:
            main_dict[k] = []
        main_dict[k].append(v)
    return main_dict


def array2list(x):
    if isinstance(x, np.ndarray):
        return x.tolist()
    return list(x)

def summarize_reulsts(results_dict, ignore_keys = ['folds']):
    summary = {}
    for k, v in results_dict.items():
        if k in ignore_keys: continue
        summary[f"{k}_avg"] = np.mean(v)
        # summary[f"{k}_std"] = np.std(v)
    return summary


def seed_torch(seed=7):
    import random
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if device.type == 'cuda':
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def read_splits(args, fold_idx=None):
    splits_csvs = {}
    split_names = args.split_names.split(',')
    print(f"Using the following split names: {split_names}")
    for split in split_names:
        print(args.split_dir, f'{split}.csv')
        if fold_idx is not None:
            split_path = j_(args.split_dir, f'{split}_{fold_idx}.csv')
        else:
            split_path = j_(args.split_dir, f'{split}.csv')
            print('hhh',split_path)
        
        if os.path.isfile(split_path):
            df = pd.read_csv(split_path)#.sample(frac=1, random_state=0).head(25).reset_index(drop=True)
            assert 'Unnamed: 0' not in df.columns
            splits_csvs[split] = df
    print(splits_csvs)
    return splits_csvs


class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, name='unk', fmt=':f'):
        self.name = name
        self.fmt = fmt
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

    def __str__(self):
        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
        return fmtstr.format(**self.__dict__)
    
def get_lr_scheduler(args, optimizer, dataloader):
    scheduler_name = args.lr_scheduler
    warmup_steps = args.warmup_steps
    warmup_epochs = args.warmup_epochs
    epochs = args.max_epochs if hasattr(args, 'max_epochs') else args.epochs
    assert not (warmup_steps > 0 and warmup_epochs > 0), "Cannot have both warmup steps and epochs"
    accum_steps = args.accum_steps
    if warmup_steps > 0:
        warmup_steps = warmup_steps
    elif warmup_epochs > 0:
        warmup_steps = warmup_epochs * (len(dataloader) // accum_steps)
    else:
        warmup_steps = 0
    if scheduler_name=='constant':
        lr_scheduler = get_constant_schedule_with_warmup(optimizer=optimizer,
        num_warmup_steps=warmup_steps)
    elif scheduler_name=='cosine':
        lr_scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=(len(dataloader) // accum_steps * epochs),
        )
    elif scheduler_name=='linear':
        lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=(len(dataloader) // accum_steps) * epochs,
        )
    return lr_scheduler


def get_optim(args, model=None, parameters=None):
    def exclude(
        n, p): return p.ndim < 2 or "bn" in n or "ln" in n or "bias" in n or 'logit_scale' in n

    def include(n, p): return not exclude(n, p)

    if parameters is None:
        named_parameters = list(model.named_parameters())
        gain_or_bias_params = [
            p for n, p in named_parameters if exclude(n, p) and p.requires_grad]
        rest_params = [p for n, p in named_parameters if include(
            n, p) and p.requires_grad]
        parameters = [
            {"params": gain_or_bias_params, "weight_decay": 0.},
            {"params": rest_params, "weight_decay": args.wd},
        ]

    if args.opt == "adamW":
        optimizer = optim.AdamW(parameters, lr=args.lr)
    elif args.opt == 'sgd':
        optimizer = optim.SGD(parameters, lr=args.lr, momentum=0.9)
    elif args.opt == 'RAdam':
        optimizer = optim.RAdam(parameters, lr=args.lr)
    else:
        raise NotImplementedError
    return optimizer
 

def print_network(net):
    num_params = 0
    num_params_train = 0

    logging.info(str(net))
    # print(str(net))
    for param in net.parameters():
        n = param.numel()
        num_params += n
        if param.requires_grad:
            num_params_train += n

    logging.info(f'Total number of parameters: {num_params}')
    logging.info(f'Total number of trainable parameters: {num_params_train}')

    # print('Total number of parameters: %d' % num_params)
    # print('Total number of trainable parameters: %d' % num_params_train)


class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, save_dir, patience=20, min_stop_epoch=50, verbose=False, better='min'):
        """
        train_args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            min_stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement. 
                            Default: False
        """
        self.patience = patience
        self.patience_counter = 0
        self.min_stop_epoch = min_stop_epoch
        self.better = better
        self.verbose = verbose
        self.best_score = None
        self.save_dir = save_dir

        if better == 'min':
            self.best_score = np.Inf
        else:
            self.best_score = -np.Inf
        self.early_stop = False
        self.counter = 0

    def is_new_score_better(self, score):
        if self.better == 'min':
            return score < self.best_score
        else:
            return score > self.best_score

    def __call__(self, epoch, score, save_ckpt_fn, save_ckpt_kwargs):
        is_better = self.is_new_score_better(score)
        if is_better:
            print(
                f'score improved ({self.best_score:.6f} --> {score:.6f}).  Saving model ...')
            self.save_checkpoint(save_ckpt_fn, save_ckpt_kwargs)
            self.counter = 0
            self.best_score = score
        else:
            self.counter += 1
            print(
                f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience and epoch >= (self.min_stop_epoch - 1):
                self.early_stop = True
        return self.early_stop

    def save_checkpoint(self, save_ckpt_fn, save_ckpt_kwargs):
        '''Saves model when score improves.'''
        if 'save_dir' in save_ckpt_kwargs:
            save_ckpt_fn(**save_ckpt_kwargs)
        else:
            save_ckpt_fn(save_dir=self.save_dir, **save_ckpt_kwargs)


def save_checkpoint(config, epoch, model, score, save_dir, fname=None):
    save_state = {'model': model.state_dict(),
                  'score': score,
                  'epoch': epoch,
                  'config': config}

    if fname is None:
        save_path = j_(save_dir, f'ckpt_epoch_{epoch}.pth')
    else:
        save_path = j_(save_dir, fname)

    torch.save(save_state, save_path)


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/.ipynb_checkpoints/pandas_helper_funcs-checkpoint.py
import os
from os.path import join as j_
from os import listdir as ldir_
from os import scandir as sdir_
import shutil

from tqdm import tqdm

import numpy as np
import pandas
import pandas as pd
from pandas import Series

def series_int(s1: pandas.Series, s2: pandas.Series, dtype='O'):
    """
    Returns set intersection of two pd.Series (resets index).

    Args:
        s1 (pandas.Series): Series (of strings).
        s2 (pandas.Series): Series (of strings).

    Returns:
        (pandas.Series): set interesection of s1 and s2
    """
    return pd.Series(list(set(s1) & set(s2)), dtype=dtype)

def df_loc_col(df1: pandas.DataFrame, s1: pandas.Series, col_name: str, apply_sint=True, drop_orig_index=False):
    """
    Performs pandas.loc with column <col_name> as index of df1.

    Args:
        df1 (pandas.DataFrame): Dataframe.
        s1 (pandas.Series): Series (of strings).
        col_name (str): column to use as index for pandas.loc
        apply_sint (bool): Whether to take series intersection first.
        drop_orig_index (bool): Drops original index.

    Returns:
       (pandas.DataFrame): df1 subsetted by s1 using column <col_name>.
    """
    return df1.reset_index(drop=drop_orig_index).set_index(col_name).loc[series_int(df1[col_name], s1) if apply_sint else s1].reset_index(drop=False)

def series_ldir_int(path1, path2, exts=['.', '.'], add_ext=False):
    """
    Gets intersection of fnames (accounting for differing exts) in path1 and path2.

    Args:
        path1 (_type_): Path to directory of fnames.
        path2 (_type_): Path to directory of fnames.
        exts (list): Which exts to split for fnames in path1 and path2. Defaults to ['.', '.'].
        add_ext (bool, optional): Whether to add back in the extension. 
            If True, defaults to using extension of path1 (order matters). Defaults to False.

    Returns:
        (pd.Series): Intersection of fnames (accounting for differing exts) in path1 and path2
    """
    df1 = pd.Series(ldir_(path1)).str.rsplit(pat=exts[0], n=1, expand=True).set_axis(['fname', 'ext'], axis=1)
    df2 = pd.Series(ldir_(path2)).str.rsplit(pat=exts[1], n=1, expand=True).set_axis(['fname', 'ext'], axis=1)
    df = df_loc_col(df1, df2['fname'], col_name='fname', apply_sint=True, drop_orig_index=True)
    return df['fname']+exts[0]+df['ext'] if add_ext else df['fname']

def df_sdir(dataroot: str, cols=['fpath', 'fname', 'slide_id']):
    print('ffsdfg',dataroot)
    """
    Returns pd.DataFrame of the file paths and fnames of contents in dataroot.

    Args:
        dataroot (str): path to files.

    Returns:
        (pandas.Dataframe): pd.DataFrame of the file paths and fnames of contents in dataroot (make default cols: ['fpath', 'fname_ext', 'fname_noext']?)
    """
    return pd.DataFrame([(e.path, e.name, os.path.splitext(e.name)[0]) for e in sdir_(dataroot)], columns=cols)


# TODO: Fix doc + also make function for ldir_diff
def series_diff(s1, s2, dtype='O'):
    r"""
    Returns set difference of two pd.Series.
    """
    return pd.Series(list(set(s1).difference(set(s2))), dtype=dtype)


def transfer_dir2dir_shutil(dataroot: str, saveroot: str, subset_fnames:str=None, lim: int=None):
    r"""
    Transfer files from dir2dir
    
    Args:
    - dataroot (str): Source folder from which you want to transfer files from
    - subset_fnames (list): List of filenames
    - saveroot (str): Destination folder from which you want to transfer files to
    
    Return:
    - None
    """
    from tqdm import tqdm
    from os.path import join as j_
    if lim == None:
        pbar = tqdm(os.listdir(dataroot))
    else:
        pbar = tqdm(os.listdir(dataroot)[:lim])
    
    missing = []
    for fname in pbar:
        pbar.set_description(f'Copying: {fname}')

        src = j_(dataroot, fname)
        dst = j_(saveroot, fname)

        if not os.path.isfile(src):
            missing.append(fname)
        elif os.path.isfile(dst):
            continue
        else:
            shutil.copyfile(src=src, dst=dst)
        pass
    
    print('Num Missing:', len(missing))
    print("Missing Files:", missing)

series_intersection = series_int
series_difference = series_diff

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/utils/.ipynb_checkpoints/scheduler-checkpoint.py
import numpy as np


def assign_learning_rate(optimizer, new_lr):
    for param_group in optimizer.param_groups:
        param_group["lr"] = new_lr


def _warmup_lr(base_lr, warmup_length, step):
    return base_lr * (step + 1) / warmup_length


def const_lr(optimizer, base_lr, warmup_length, steps):
    def _lr_adjuster(step):
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            lr = base_lr
        assign_learning_rate(optimizer, lr)
        return lr
    return _lr_adjuster


def const_lr_cooldown(optimizer, base_lr, warmup_length, steps, cooldown_steps, cooldown_power=1.0, cooldown_end_lr=0.):
    def _lr_adjuster(step):
        start_cooldown_step = steps - cooldown_steps
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            if step < start_cooldown_step:
                lr = base_lr
            else:
                e = step - start_cooldown_step
                es = steps - start_cooldown_step
                # linear decay if power == 1; polynomial decay otherwise;
                decay = (1 - (e/es)) ** cooldown_power
                lr = decay * (base_lr - cooldown_end_lr) + cooldown_end_lr
        assign_learning_rate(optimizer, lr)
        return lr
    return _lr_adjuster


def cosine_lr(optimizer, base_lr, warmup_length, steps):
    def _lr_adjuster(step):
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            e = step - warmup_length
            es = steps - warmup_length
            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * base_lr
        assign_learning_rate(optimizer, lr)
        return lr
    return _lr_adjuster


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/configs/H2T_default/config.json
{
  "in_dim": 768,
  "n_classes": 2,
  "out_size": 8,
  "load_proto": false,
  "proto_path": ".",
  "fix_proto": false
}


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/configs/H2T_default/.ipynb_checkpoints/config-checkpoint.json
{
  "in_dim": 768,
  "n_classes": 2,
  "out_size": 8,
  "load_proto": false,
  "proto_path": ".",
  "fix_proto": false
}


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/configs/ABMIL_tiny/config.json
{
  "gate": true,
  "in_dim": 64,
  "n_classes": 2,
  "embed_dim": 64,
  "attn_dim": 64,
  "n_fc_layers": 1,
  "dropout": 0.25
}


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/configs/ABMIL_tiny/.ipynb_checkpoints/config-checkpoint.json
{
  "gate": true,
  "in_dim": 64,
  "n_classes": 2,
  "embed_dim": 64,
  "attn_dim": 64,
  "n_fc_layers": 1,
  "dropout": 0.25
}


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/configs/ABMIL_default/config.json
{
  "gate": true,
  "in_dim": 1024,
  "n_classes": 2,
  "embed_dim": 512,
  "attn_dim": 384,
  "n_fc_layers": 1,
  "dropout": 0.25,
  "fusion_dim": 320,
  "clinical_dim": 64,
  "clinical_hidden_dim": 64,
  "clinical_layers": 2,
  "post_attention_dim": 128
}

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/configs/ABMIL_default/.ipynb_checkpoints/config-checkpoint.json
{
  "gate": true,
  "in_dim": 768,
  "n_classes": 2,
  "embed_dim": 512,
  "attn_dim": 384,
  "n_fc_layers": 1,
  "dropout": 0.25
}


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/wsi_datasets/wsi_survival.py
from __future__ import print_function, division
import os
from os.path import join as j_
import torch
import numpy as np
import pandas as pd
import math
import re
import pdb
import pickle
import sys

from torch.utils.data import Dataset
import h5py
from .dataset_utils import apply_sampling
from .clinical_processor import ClinicalDataProcessor
sys.path.append('../')
from utils.pandas_helper_funcs import df_sdir, series_diff

class WSISurvivalDataset(Dataset):
    """WSI Survival Dataset."""

    def __init__(self,
                 df,
                 data_source,
                 target_transform=None,
                 sample_col='case_id',
                 slide_col='slide_id',
                 survival_time_col='os_survival_days',
                 censorship_col='os_censorship',
                 n_label_bins=4,
                 label_bins=None,
                 bag_size=0,
                 include_surv_t0=True,
                 mri_feature_path=None,
                 clinical_data_path=None,
                 clinical_processor=None,
                 **kwargs):
        """
        Args:
        """
        self.data_source = []
        for src in data_source:
            assert os.path.basename(src) in ['feats_h5', 'feats_pt', 'features']
            self.use_h5 = True if os.path.basename(src) == 'feats_h5' else False
            self.data_source.append(src)

        self.data_df = df
        assert 'Unnamed: 0' not in self.data_df.columns
        self.sample_col = sample_col
        self.slide_col = slide_col
        self.target_col = survival_time_col
        self.survival_time_col = survival_time_col
        self.censorship_col = censorship_col
        self.include_surv_t0 = include_surv_t0
        self.mri_feature_path = mri_feature_path
        self.clinical_data_path = clinical_data_path
        self.clinical_processor = clinical_processor

        # Initialize clinical processor if provided with a path but no processor
        if self.clinical_data_path is not None and self.clinical_processor is None:
            self.clinical_processor = ClinicalDataProcessor(clinical_data_path=self.clinical_data_path)
            # We'll fit the processor in the __getitem__ method when needed

        is_nan_censorship = self.data_df[self.censorship_col].isna()
        if sum(is_nan_censorship) > 0:
            print('# of NaNs in Censorship col, dropping:', sum(is_nan_censorship))
            self.data_df = self.data_df[~is_nan_censorship]

        is_nan_survival = self.data_df[self.survival_time_col].isna()
        if sum(is_nan_survival) > 0:
            print('# of NaNs in Survival time col, dropping:', sum(is_nan_survival))
            self.data_df = self.data_df[~is_nan_survival]

        if (self.data_df[self.survival_time_col] < 0).sum() > 0 and (not self.include_surv_t0):
            self.data_df = self.data_df[self.data_df[self.survival_time_col] > 0]

        censorship_vals = self.data_df[self.censorship_col].value_counts().index
        if set(censorship_vals) != set([0,1]):
            print('Censorship values must be binary integers, found:', censorship_vals)
            sys.exit()

        self.target_transform = target_transform
        self.n_label_bins = n_label_bins
        self.label_bins = None
        self.bag_size = bag_size

        self.validate_survival_dataset()
        self.idx2sample_df = pd.DataFrame({'sample_id': self.data_df[sample_col].astype(str).unique()})
        self.set_feat_paths_in_df()
        self.data_df.index = self.data_df[sample_col].astype(str)
        self.data_df.index.name = 'sample_id'
        self.X = None
        self.y = None
        
        if 'disc_label' in self.data_df.columns:
            self.data_df = self.data_df.drop('disc_label', axis=1)
        
        if self.n_label_bins > 0:
            disc_labels, label_bins = compute_discretization(df=self.data_df,
                                                             survival_time_col=self.survival_time_col,
                                                             censorship_col=self.censorship_col,
                                                             n_label_bins=self.n_label_bins,
                                                             label_bins=label_bins)
            self.data_df = self.data_df.join(disc_labels)
            self.label_bins = label_bins
            self.target_col = disc_labels.name
            assert self.data_df.index.nunique() == self.idx2sample_df.index.nunique()

        self.survival_time_labels = []
        self.censorship_labels = []
        self.disc_labels = []
        for idx in self.idx2sample_df.index:
            survival_time, censorship, disc_label = self.get_labels(idx)
            self.survival_time_labels.append(survival_time)
            self.censorship_labels.append(censorship)
            self.disc_labels.append(disc_label)

        self.survival_time_labels = torch.tensor(self.survival_time_labels)
        self.censorship_labels = torch.tensor(self.censorship_labels)
        self.disc_labels = torch.tensor(self.disc_labels)

    def __len__(self):
        return len(self.idx2sample_df)

    def set_feat_paths_in_df(self):
        """
        Sets the feature path (for each slide id) in self.data_df. At the same time, checks that all slides 
        specified in the split (or slides for the cases specified in the split) exist within data source.
        """
        self.feats_df = pd.concat([df_sdir(feats_dir, cols=['fpath', 'fname', self.slide_col]) for feats_dir in self.data_source]).drop(['fname'], axis=1).reset_index(drop=True)
        missing_feats_in_split = series_diff(self.data_df[self.slide_col], self.feats_df[self.slide_col])

        ### Assertion to make sure that there are not any missing slides that were specified in your split csv file
        try:
            assert len(missing_feats_in_split) == 0
        except:
            print(f"Missing Features in Split:\n{missing_feats_in_split}")
            sys.exit()

        ### Assertion to make sure that all slide ids to feature paths have a one-to-one mapping (no duplicated features).
        try:
            self.data_df = self.data_df.merge(self.feats_df, how='left', on=self.slide_col, validate='1:1')
            assert self.feats_df[self.slide_col].duplicated().sum() == 0
        except:
            print("Features duplicated in data source(s). List of duplicated features (and their paths):")
            print(self.feats_df[self.feats_df[self.slide_col].duplicated()].to_string())
            sys.exit()

        self.data_df = self.data_df[list(self.data_df.columns[-1:]) + list(self.data_df.columns[:-1])]

    def validate_survival_dataset(self):
        """Validate that the survival dataset is valid."""
        # check that each case_id has only one survival value
        num_unique_surv_times = self.data_df.groupby(self.sample_col)[self.survival_time_col].unique().apply(len)
        try:
            assert (num_unique_surv_times == 1).all()
        except AssertionError:
            print('Each case_id must have only one unique survival value.')
            raise

        # check that all survival values are numeric
        try:
            assert not pd.to_numeric(self.data_df[self.survival_time_col], errors='coerce').isna().any()
        except AssertionError:
            print('Survival values must be numeric.')
            raise

        # check that all survival values are positive
        try:
            assert (self.data_df[self.survival_time_col] >= 0).all()
            if not self.include_surv_t0:
                assert (self.data_df[self.survival_time_col] > 0).all()
        except AssertionError:
            print('Survival values must be positive.')
            raise

        # check that all censorship values are binary integers
        try:
            assert self.data_df[self.censorship_col].isin([0, 1]).all()
        except AssertionError:
            print('Censorship values must be binary integers.')
            raise

    def get_sample_id(self, idx):
        return self.idx2sample_df.loc[idx]['sample_id']

    def get_mri_features(self, idx):
        """Load MRI features for a given sample."""
        if self.mri_feature_path is None:
            return None
        
        sample_id = self.get_sample_id(idx)
        # Construct MRI feature file path: [case_id]_0001.pt
        mri_file_path = os.path.join(self.mri_feature_path, f"{sample_id}_0001_features.pt")
        
        if os.path.exists(mri_file_path):
            try:
                mri_features = torch.load(mri_file_path)
                # Ensure the features are in the correct shape (320,)
                if len(mri_features.shape) > 1:
                    mri_features = mri_features.squeeze()
                return mri_features
            except Exception as e:
                print(f"Error loading MRI features for {sample_id}: {e}")
                return None
        else:
            print(f"MRI feature file not found: {mri_file_path}")
            return None

    def get_clinical_features(self, idx):
        """Load clinical features for a given sample."""
        if self.clinical_processor is None:
            return None
        
        sample_id = self.get_sample_id(idx)
        
        # Transform the clinical data for this sample
        try:
            clinical_features = self.clinical_processor.transform(sample_id)
            return clinical_features
        except Exception as e:
            print(f"Error getting clinical features for {sample_id}: {e}")
            # Return zeros with the right dimension if transformation fails
            if hasattr(self.clinical_processor, 'output_dim') and self.clinical_processor.output_dim is not None:
                return torch.zeros(self.clinical_processor.output_dim)
            return None

    def get_feat_paths(self, idx):
        feat_paths = self.data_df.loc[self.get_sample_id(idx), 'fpath']
        if isinstance(feat_paths, str):
            feat_paths = [feat_paths]
        return feat_paths

    def get_labels(self, idx):
        labels = self.data_df.loc[self.get_sample_id(idx), [self.survival_time_col, self.censorship_col, self.target_col]]
        if isinstance(labels, pd.Series):
            labels = list(labels)
        elif isinstance(labels, pd.DataFrame):
            labels = list(labels.iloc[0])
        return labels

    def __getitem__from_emb__(self, idx):
        out = {'img': self.X[idx],
            'coords': [],
            'survival_time': torch.tensor([self.survival_time_labels[idx]]),
            'censorship': torch.tensor([self.censorship_labels[idx]]),
            'label': torch.tensor([self.disc_labels[idx]])}
        return out

    def __getitem__(self, idx):
        if self.X is not None:
            return self.__getitem__from_emb__(idx)
        
        survival_time, censorship, label = self.get_labels(idx)
        # Read features (and coordinates, Optional) from pt/h5 file
        all_features = []
        all_coords = []

        feat_paths = self.get_feat_paths(idx)
        for feat_path in feat_paths:
            if self.use_h5:
                with h5py.File(feat_path, 'r') as f:
                    features = f['features'][:]
                    coords = f['coords'][:]
                all_coords.append(coords)
            else:
                features = torch.load(feat_path)

            if len(features.shape) > 2:
                assert features.shape[0] == 1, f'{features.shape} is not compatible! It has to be (1, numOffeats, feat_dim) or (numOffeats, feat_dim)'
                features = np.squeeze(features, axis=0)

            all_features.append(features)
        all_features = torch.from_numpy(np.concatenate(all_features, axis=0))
        if len(all_coords) > 0:
            all_coords = np.concatenate(all_coords, axis=0)

        # apply sampling if needed, return attention mask if sampling is applied else None
        all_features, all_coords, attn_mask = apply_sampling(self.bag_size, all_features, all_coords)

        # Load MRI features
        mri_features = self.get_mri_features(idx)

        out = {'img': all_features,
            'coords': all_coords,
            'survival_time': torch.Tensor([survival_time]),
            'censorship': torch.Tensor([censorship]),
            'label': torch.Tensor([label])}

        # Add MRI features if available
        if mri_features is not None:
            out['mri_feature'] = mri_features

        if attn_mask is not None:
            out['attn_mask'] = attn_mask

        # Get clinical features
        clinical_features = self.get_clinical_features(idx)
        out['clinical_feature'] = clinical_features

        return out
    
    def get_label_bins(self):
        return self.label_bins


def compute_discretization(df, survival_time_col='os_survival_days', censorship_col='os_censorship', n_label_bins=4, label_bins=None):
    df = df[~df['case_id'].duplicated()] # make sure that we compute discretization on unique cases

    if label_bins is not None:
        assert len(label_bins) == n_label_bins + 1
        q_bins = label_bins
    else:
        uncensored_df = df[df[censorship_col] == 0]
        disc_labels, q_bins = pd.qcut(uncensored_df[survival_time_col], q=n_label_bins, retbins=True, labels=False)
        q_bins[-1] = 1e6  # set rightmost edge to be infinite
        q_bins[0] = -1e-6  # set leftmost edge to be 0

    disc_labels, q_bins = pd.cut(df[survival_time_col], bins=q_bins,
                                retbins=True, labels=False,
                                include_lowest=True)
    assert isinstance(disc_labels, pd.Series) and (disc_labels.index.name == df.index.name)
    disc_labels.name = 'disc_label'
    return disc_labels, q_bins


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/wsi_datasets/__init__.py
from .wsi_classification import WSIClassificationDataset
from .wsi_survival import WSISurvivalDataset


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/wsi_datasets/dataset_utils.py
import numpy as np
import torch

def apply_sampling(target_bag_size, all_features, all_coords):
    attn_mask = None
    if target_bag_size > 0:
        bag_size = all_features.size(0)
        attn_mask = torch.ones(bag_size)
        if bag_size < target_bag_size:
            sampled_features = torch.cat([all_features, torch.zeros(
                (target_bag_size - bag_size, all_features.shape[1]))], dim=0)
            attn_mask = torch.cat(
                [attn_mask, torch.zeros((target_bag_size - bag_size))])
            if len(all_coords) > 0:
                all_coords = np.concatenate(
                    [all_coords, np.zeros((target_bag_size - bag_size, 2))], axis=0)
        else:
            sampled_patch_ids = np.random.choice(
                np.arange(bag_size), target_bag_size, replace=False)
            sampled_features = all_features[sampled_patch_ids, :]
            attn_mask = attn_mask[:target_bag_size]
            if len(all_coords) > 0:
                all_coords = all_coords[sampled_patch_ids, :]
        all_features = sampled_features
    return all_features, all_coords, attn_mask

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/wsi_datasets/clinical_processor.py
import os
import json
import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

class ClinicalDataProcessor:
    """
    Processor for clinical data. Handles loading, preprocessing, and feature extraction
    from clinical JSON files.
    """
    def __init__(self, clinical_data_path=None):
        """
        Initialize the clinical data processor
        
        Args:
            clinical_data_path: Path to directory containing clinical JSON files
        """
        self.clinical_data_path = clinical_data_path
        self.preprocessor = None
        
        # Define features to use from clinical data
        self.numerical_features = [
            'age_at_prostatectomy',
            'primary_gleason',
            'secondary_gleason',
            'tertiary_gleason',
            'ISUP',
            'pre_operative_PSA'
        ]
        
        self.categorical_features = [
            'pT_stage',
            'positive_lymph_nodes',
            'capsular_penetration',
            'positive_surgical_margins',
            'invasion_seminal_vesicles',
            'lymphovascular_invasion',
            'earlier_therapy'
        ]
        
        # Default values for missing data
        self.default_values = {
            'age_at_prostatectomy': 0,
            'primary_gleason': 0,
            'secondary_gleason': 0,
            'tertiary_gleason': 0,
            'ISUP': 1,
            'pre_operative_PSA': 0.0,
            'pT_stage': 'x',
            'positive_lymph_nodes': 'x',
            'capsular_penetration': 'x',
            'positive_surgical_margins': 'x',
            'invasion_seminal_vesicles': 'x',
            'lymphovascular_invasion': 'x',
            'earlier_therapy': 'none'
        }
        
        # Output dimension (will be set after fitting)
        self.output_dim = None
    
    def fit(self, case_ids):
        """
        Fit the preprocessor on clinical data for the given case IDs
        
        Args:
            case_ids: List of case IDs to fit on
        
        Returns:
            self
        """
        if self.clinical_data_path is None:
            print("Warning: No clinical data path specified. Processor will return default values.")
            self.output_dim = self._get_default_output_dim()
            return self
        
        # Load clinical data for all case IDs
        clinical_data = []
        for case_id in case_ids:
            data = self._load_clinical_data(case_id)
            clinical_data.append(data)
        
        # Create a DataFrame from the clinical data
        clinical_df = pd.DataFrame(clinical_data)
        
        # Create preprocessing pipelines
        numerical_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ])
        
        categorical_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        # Create column transformer
        self.preprocessor = ColumnTransformer(
            transformers=[
                ('num', numerical_pipeline, self.numerical_features),
                ('cat', categorical_pipeline, self.categorical_features)
            ],
            remainder='drop'  # Drop any columns not specified in the transformers
        )
        
        # Fit the preprocessor
        self.preprocessor.fit(clinical_df)
        
        # Store the output dimension
        self.output_dim = self._get_output_dim(clinical_df)
        print(f"Clinical preprocessor fitted. Output dimension: {self.output_dim}")
        
        return self
    
    def transform(self, case_id):
        """
        Transform clinical data for a single case ID
        
        Args:
            case_id: Case ID to transform
        
        Returns:
            Tensor of processed clinical features
        """
        if self.preprocessor is None or self.clinical_data_path is None:
            # Return zero vector with correct dimension if no preprocessor or clinical data path
            zeros = torch.zeros(self._get_default_output_dim())
            return zeros
        
        # Load clinical data
        data = self._load_clinical_data(case_id)
        
        # Create a DataFrame with a single row
        df = pd.DataFrame([data])
        
        # Transform the data
        try:
            processed_features = self.preprocessor.transform(df)
            # Convert to tensor
            return torch.tensor(processed_features, dtype=torch.float32).squeeze(0)
        except Exception as e:
            print(f"Error transforming clinical data for case {case_id}: {e}")
            # Return zeros as fallback
            return torch.zeros(self.output_dim)
    
    def _get_default_output_dim(self):
        """
        Get the default output dimension for when no preprocessor is available
        
        Returns:
            Default output dimension
        """
        if hasattr(self, 'output_dim') and self.output_dim is not None:
            return self.output_dim
        else:
            # Estimate output dimension based on feature counts
            num_features = len(self.numerical_features)
            
            # Estimate categorical dimensions
            cat_features = 0
            for feature in self.categorical_features:
                if feature == 'pT_stage':
                    cat_features += 9  # Assuming all pT_stage values
                elif feature == 'earlier_therapy':
                    cat_features += 5  # Assuming all therapy types
                else:
                    cat_features += 3  # Assuming binary + unknown
            
            return num_features + cat_features
    
    def _get_output_dim(self, example_df):
        """
        Get the output dimension by transforming an example dataframe
        
        Args:
            example_df: Example DataFrame to transform
        
        Returns:
            Output dimension
        """
        # Transform the example dataframe and get the shape
        transformed = self.preprocessor.transform(example_df.iloc[:1])
        return transformed.shape[1]
    
    def _load_clinical_data(self, case_id):
        """
        Load clinical data for a single case ID
        
        Args:
            case_id: Case ID to load
        
        Returns:
            Dictionary of clinical data
        """
        if self.clinical_data_path is None:
            return self.default_values
        
        json_path = os.path.join(self.clinical_data_path, f"{case_id}.json")
        
        if not os.path.exists(json_path):
            print(f"Clinical data file not found for case {case_id}: {json_path}")
            return self.default_values
        
        try:
            with open(json_path, 'r') as f:
                data = json.load(f)
            
            # Filter to include only the features we want and provide defaults for missing values
            filtered_data = {}
            for feature in self.numerical_features + self.categorical_features:
                value = data.get(feature, self.default_values[feature])
                # --- Custom preprocessing for each feature ---
                if feature in ['primary_gleason', 'secondary_gleason', 'tertiary_gleason', 'ISUP']:
                    # Ordinal integer, treat 'x' or missing as 0 (or np.nan if you prefer)
                    try:
                        value = int(value)
                    except Exception:
                        value = 0
                elif feature == 'pT_stage':
                    # Map to ordinal integer (example mapping, adjust as needed)
                    pt_stage_map = {
                        '2': 0, '2a': 1, '2b': 2, '2c': 3,
                        '3': 4, '3a': 5, '3b': 6,
                        '4': 7, '4a': 8, '4b': 9, 'x': -1
                    }
                    value = pt_stage_map.get(str(value).lower(), -1)
                elif feature in ['positive_lymph_nodes', 'capsular_penetration', 'invasion_seminal_vesicles']:
                    # Binary with 'x' for unknown
                    if str(value) == '1':
                        value = 1
                    elif str(value) == '0':
                        value = 0
                    else:
                        value = -1
                elif feature == 'positive_surgical_margins':
                    # Binary with 'x' for unknown
                    try:
                        value = int(value)
                        if value not in [0, 1]:
                            value = -1
                    except Exception:
                        value = -1
                elif feature == 'lymphovascular_invasion':
                    # Binary with 'x' for unknown, sometimes float string
                    if str(value) in ['1', '1.0']:
                        value = 1
                    elif str(value) in ['0', '0.0']:
                        value = 0
                    else:
                        value = -1
                # earlier_therapy: keep as string for one-hot
                filtered_data[feature] = value
            
            return filtered_data
        except Exception as e:
            print(f"Error loading clinical data for case {case_id}: {e}")
            return self.default_values


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/wsi_datasets/wsi_classification.py
from __future__ import print_function, division
import os
from os.path import join as j_
import torch
import numpy as np
import pandas as pd
import math
import re
import pdb
import pickle
import sys

from torch.utils.data import Dataset
import h5py
from .dataset_utils import apply_sampling
sys.path.append('../')
from utils.pandas_helper_funcs import df_sdir, series_diff

class WSIClassificationDataset(Dataset):
    """WSI Classification Dataset."""

    def __init__(self,
                 df,
                 data_source,
                 target_transform=None,
                 sample_col='slide_id',
                 slide_col='slide_id',
                 target_col='label',
                 label_map=None,
                 bag_size=0,
                 **kwargs):
        """
        Args:
        """
        self.data_source = []
        for src in data_source:
            self.data_source.append(src)

        self.use_h5 = False

        self.data_df = df
        assert 'Unnamed: 0' not in self.data_df.columns
        self.sample_col = sample_col
        self.slide_col = slide_col
        self.target_col = target_col
        self.target_transform = target_transform
        self.label_map = label_map
        self.bag_size = bag_size
        self.data_df[sample_col] = self.data_df[sample_col].astype(str)
        self.data_df[slide_col] = self.data_df[slide_col].astype(str)
        self.X = None
        self.y = None

        self.validate_classification_dataset()
        self.idx2sample_df = pd.DataFrame({'sample_id': self.data_df[sample_col].astype(str).unique()})
        self.set_feat_paths_in_df()
        self.data_df.index = self.data_df[sample_col].astype(str)
        self.data_df.index.name = 'sample_id'
        print(self.data_df.groupby([target_col])[sample_col].count().to_string())

        self.labels = []
        for idx in self.idx2sample_df.index:
            self.labels.append(self.get_labels(idx, apply_transform=True))

        self.labels = torch.Tensor(self.labels).type(torch.long)

    def __len__(self):
        return len(self.idx2sample_df)

    def set_feat_paths_in_df(self):
        """
        Sets the feature path (for each slide id) in self.data_df. At the same time, checks that all slides 
        specified in the split (or slides for the cases specified in the split) exist within data source.
        """
        self.feats_df = pd.concat([df_sdir(feats_dir, cols=['fpath', 'fname', self.slide_col]) for feats_dir in self.data_source]).drop(['fname'], axis=1).reset_index(drop=True)
        missing_feats_in_split = series_diff(self.data_df[self.slide_col], self.feats_df[self.slide_col])

        ### Assertion to make sure there  are no unexpected labels in split
        try:
            self.data_df[self.target_col].map(self.label_map)
        except:
            print(f"Unexpected labels in split:\n{self.data_df[self.target_col].unique()}")
            sys.exit()

        ### Assertion to make sure that there are not any missing slides that were specified in your split csv file
        try:
            assert len(missing_feats_in_split) == 0
        except:
            print(f"Missing Features in Split:\n{missing_feats_in_split}")
            sys.exit()

        ### Assertion to make sure that all slide ids to feature paths have a one-to-one mapping (no duplicated features).
        try:
            self.data_df = self.data_df.merge(self.feats_df, how='left', on=self.slide_col, validate='1:1')
            assert self.feats_df[self.slide_col].duplicated().sum() == 0
        except:
            print("Features duplicated in data source(s). List of duplicated features (and their paths):")
            print(self.feats_df[self.feats_df[self.slide_col].duplicated()].to_string())
            sys.exit()

        self.data_df = self.data_df[list(self.data_df.columns[-1:]) + list(self.data_df.columns[:-1])]

    def validate_classification_dataset(self):
        """
        - Why is this needed? For ebrains, slides for a single case have different disease diagnoses (often the case for temporal data, or patients who undergo multiple resections).
        """
        num_unique_target_labels = self.data_df.groupby(self.sample_col)[self.target_col].unique().apply(len)
        try:
            assert (num_unique_target_labels == 1).all()
        except AssertionError:
            print('Each case_id must have only one unique survival value.')
            raise

    def get_sample_id(self, idx):
        return self.idx2sample_df.loc[idx]['sample_id']

    def get_feat_paths(self, idx):
        feat_paths = self.data_df.loc[self.get_sample_id(idx), 'fpath']
        if isinstance(feat_paths, str):
            feat_paths = [feat_paths]
        return feat_paths

    def get_labels(self, idx, apply_transform=False):
        if isinstance(idx, int):
            idx = [idx]
        labels = self.data_df.loc[self.get_sample_id(idx), self.target_col]
        if isinstance(labels, pd.Series):
            labels = labels.values.tolist()
        if apply_transform:
            if self.label_map is not None:
                labels = [self.label_map[label] for label in labels]
            if self.target_transform is not None:
                labels = [self.target_transform(label) for label in labels]
        
        if len(idx) == 1:
            labels = labels[0]
        return labels

    def __getitem__from_emb__(self, idx):
        out = {'img': self.X[idx],
               'coords': [],
               'label': torch.Tensor([self.labels[idx]])}

        return out

    def __getitem__(self, idx):
        if self.X is not None:
            return self.__getitem__from_emb__(idx)
        
        feat_paths = self.get_feat_paths(idx)
        label = self.get_labels(idx, apply_transform=True)

        # Read features (and coordinates, Optional) from pt/h5 file
        all_features = []
        all_coords = []
        for feat_path in feat_paths:
            if self.use_h5:
                with h5py.File(feat_path, 'r') as f:
                    features = f['features'][:]
                    coords = f['coords'][:]
                all_coords.append(coords)
            else:
                features = torch.load(feat_path)

            if len(features.shape) > 2:
                assert features.shape[0] == 1, f'{features.shape} is not compatible! It has to be (1, numOffeats, feat_dim) or (numOffeats, feat_dim)'
                features = np.squeeze(features, axis=0)
            all_features.append(features)
            
        all_features = torch.from_numpy(np.concatenate(all_features, axis=0))
        if len(all_coords) > 0:
            all_coords = np.concatenate(all_coords, axis=0)

        # apply sampling if needed, return attention mask if sampling is applied else None
        all_features, all_coords, attn_mask = apply_sampling(self.bag_size, all_features, all_coords)

        out = {'img': all_features,
               'coords': all_coords,
               'label': label}
        if attn_mask is not None:
            out['attn_mask'] = attn_mask

        return out


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/wsi_datasets/.ipynb_checkpoints/__init__-checkpoint.py
from .wsi_classification import WSIClassificationDataset
from .wsi_survival import WSISurvivalDataset
from .wsi_prototype import WSIProtoDataset

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/wsi_datasets/.ipynb_checkpoints/wsi_classification-checkpoint.py
from __future__ import print_function, division
import os
from os.path import join as j_
import torch
import numpy as np
import pandas as pd
import math
import re
import pdb
import pickle
import sys

from torch.utils.data import Dataset
import h5py
from .dataset_utils import apply_sampling
sys.path.append('../')
from utils.pandas_helper_funcs import df_sdir, series_diff

class WSIClassificationDataset(Dataset):
    """WSI Classification Dataset."""

    def __init__(self,
                 df,
                 data_source,
                 target_transform=None,
                 sample_col='slide_id',
                 slide_col='slide_id',
                 target_col='label',
                 label_map=None,
                 bag_size=0,
                 **kwargs):
        """
        Args:
        """
        self.data_source = []
        for src in data_source:
            self.data_source.append(src)

        use_h5 = True

        self.data_df = df
        assert 'Unnamed: 0' not in self.data_df.columns
        self.sample_col = sample_col
        self.slide_col = slide_col
        self.target_col = target_col
        self.target_transform = target_transform
        self.label_map = label_map
        self.bag_size = bag_size
        self.data_df[sample_col] = self.data_df[sample_col].astype(str)
        self.data_df[slide_col] = self.data_df[slide_col].astype(str)
        self.X = None
        self.y = None

        self.validate_classification_dataset()
        self.idx2sample_df = pd.DataFrame({'sample_id': self.data_df[sample_col].astype(str).unique()})
        self.set_feat_paths_in_df()
        self.data_df.index = self.data_df[sample_col].astype(str)
        self.data_df.index.name = 'sample_id'
        print(self.data_df.groupby([target_col])[sample_col].count().to_string())

        self.labels = []
        for idx in self.idx2sample_df.index:
            self.labels.append(self.get_labels(idx, apply_transform=True))

        self.labels = torch.Tensor(self.labels).type(torch.long)

    def __len__(self):
        return len(self.idx2sample_df)

    def set_feat_paths_in_df(self):
        """
        Sets the feature path (for each slide id) in self.data_df. At the same time, checks that all slides 
        specified in the split (or slides for the cases specified in the split) exist within data source.
        """
        self.feats_df = pd.concat([df_sdir(feats_dir, cols=['fpath', 'fname', self.slide_col]) for feats_dir in self.data_source]).drop(['fname'], axis=1).reset_index(drop=True)
        print(self.feats_df)
        missing_feats_in_split = series_diff(self.data_df[self.slide_col], self.feats_df[self.slide_col])

        ### Assertion to make sure there  are no unexpected labels in split
        try:
            self.data_df[self.target_col].map(self.label_map)
        except:
            print(f"Unexpected labels in split:\n{self.data_df[self.target_col].unique()}")
            sys.exit()

        ### Assertion to make sure that there are not any missing slides that were specified in your split csv file
        try:
            assert len(missing_feats_in_split) == 0
        except:
            print(f"Missing Features in Split:\n{missing_feats_in_split}")
            sys.exit()

        ### Assertion to make sure that all slide ids to feature paths have a one-to-one mapping (no duplicated features).
        try:
            self.data_df = self.data_df.merge(self.feats_df, how='left', on=self.slide_col, validate='1:1')
            assert self.feats_df[self.slide_col].duplicated().sum() == 0
        except:
            print("Features duplicated in data source(s). List of duplicated features (and their paths):")
            print(self.feats_df[self.feats_df[self.slide_col].duplicated()].to_string())
            sys.exit()

        self.data_df = self.data_df[list(self.data_df.columns[-1:]) + list(self.data_df.columns[:-1])]

    def validate_classification_dataset(self):
        """
        - Why is this needed? For ebrains, slides for a single case have different disease diagnoses (often the case for temporal data, or patients who undergo multiple resections).
        """
        num_unique_target_labels = self.data_df.groupby(self.sample_col)[self.target_col].unique().apply(len)
        try:
            assert (num_unique_target_labels == 1).all()
        except AssertionError:
            print('Each case_id must have only one unique survival value.')
            raise

    def get_sample_id(self, idx):
        return self.idx2sample_df.loc[idx]['sample_id']

    def get_feat_paths(self, idx):
        feat_paths = self.data_df.loc[self.get_sample_id(idx), 'fpath']
        if isinstance(feat_paths, str):
            feat_paths = [feat_paths]
        return feat_paths

    def get_labels(self, idx, apply_transform=False):
        if isinstance(idx, int):
            idx = [idx]
        labels = self.data_df.loc[self.get_sample_id(idx), self.target_col]
        if isinstance(labels, pd.Series):
            labels = labels.values.tolist()
        if apply_transform:
            if self.label_map is not None:
                labels = [self.label_map[label] for label in labels]
            if self.target_transform is not None:
                labels = [self.target_transform(label) for label in labels]
        
        if len(idx) == 1:
            labels = labels[0]
        return labels

    def __getitem__from_emb__(self, idx):
        out = {'img': self.X[idx],
               'coords': [],
               'label': torch.Tensor([self.labels[idx]])}

        return out

    def __getitem__(self, idx):
        if self.X is not None:
            return self.__getitem__from_emb__(idx)
        
        feat_paths = self.get_feat_paths(idx)
        label = self.get_labels(idx, apply_transform=True)

        # Read features (and coordinates, Optional) from pt/h5 file
        all_features = []
        all_coords = []
        for feat_path in feat_paths:
            if self.use_h5:
                with h5py.File(feat_path, 'r') as f:
                    features = f['features'][:]
                    coords = f['coords'][:]
                all_coords.append(coords)
            else:
                features = torch.load(feat_path)

            if len(features.shape) > 2:
                assert features.shape[0] == 1, f'{features.shape} is not compatible! It has to be (1, numOffeats, feat_dim) or (numOffeats, feat_dim)'
                features = np.squeeze(features, axis=0)
            all_features.append(features)
            
        all_features = torch.from_numpy(np.concatenate(all_features, axis=0))
        if len(all_coords) > 0:
            all_coords = np.concatenate(all_coords, axis=0)

        # apply sampling if needed, return attention mask if sampling is applied else None
        all_features, all_coords, attn_mask = apply_sampling(self.bag_size, all_features, all_coords)

        out = {'img': all_features,
               'coords': all_coords,
               'label': label}
        if attn_mask is not None:
            out['attn_mask'] = attn_mask

        return out


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/model_h2t.py
"""
Hard-clustering-based aggregation

Ref:
    Vu, Quoc Dang, et al. "Handcrafted Histological Transformer (H2T): Unsupervised representation of whole slide images." Medical image analysis 85 (2023): 102743.
"""

import torch
import torch.nn as nn
import os
import numpy as np
import pdb

from tqdm import tqdm
from .components import predict_clf, predict_surv, predict_emb
from utils.file_utils import save_pkl, load_pkl

class H2T(nn.Module):
    """
    WSI is represented as a prototype-count vector
    """
    def __init__(self, config, mode):
        super().__init__()

        assert config.load_proto, "Prototypes must be loaded!"
        assert os.path.exists(config.proto_path), "Path {} doesn't exist!".format(config.proto_path)

        self.config = config
        self.mode = mode
        proto_path = config.proto_path

        if proto_path.endswith('pkl'):
            weights = load_pkl(proto_path)['prototypes'].squeeze()
        elif proto_path.endswith('npy'):
            weights = np.load(proto_path)

        self.n_proto = config.out_size
        self.prototypes = torch.from_numpy(weights).float()
        self.prototypes = self.prototypes / torch.norm(self.prototypes, dim=1).unsqueeze(1)

        emb_dim = config.in_dim

    def representation(self, x):
        """
        Construct unsupervised slide representation
        """
        self.prototypes = self.prototypes.to(x.device)

        x = x / torch.norm(x, dim=-1).unsqueeze(2)
        dist = torch.cdist(self.prototypes, x, p=2) # (1 x n_proto x n_instances)
        c_identity = torch.argmin(dist.squeeze(), dim=0) # (n_instances)

        feats = []
        for idx in range(self.n_proto):
            indices = torch.nonzero(c_identity == idx)
            if len(indices) != 0:
                feat = torch.mean(x[:, indices, :], dim=1)
            else:
                feat = torch.zeros((1,1,x.shape[-1])).to(x.device)
            feats.append(feat)
        out = torch.cat(feats, dim=1)
        out = out.reshape(x.shape[0], -1)
        return {'repr': out}

    def forward(self, x):
        out = self.representation(x)
        return out['repr']

    def predict(self, data_loader, use_cuda=True):
        if self.mode == 'classification':
            output, y = predict_clf(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'survival':
            output, y = predict_surv(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'emb':
            output = predict_emb(self, data_loader.dataset, use_cuda=use_cuda)
            y = None
        else:
            raise NotImplementedError(f"Not implemented for {self.mode}!")
        
        return output, y

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/model_abmil_fusion.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from .components import Attn_Net, Attn_Net_Gated, create_mlp, process_clf, process_surv

class ABMIL(nn.Module):
    def __init__(self, config, mode):
        super().__init__()
        self.config = config
        self.mode = mode
        
        # --- Model dimensions from config ---
        self.fusion_dim = getattr(config, 'fusion_dim', 0)
        self.clinical_dim = getattr(config, 'clinical_dim', 0)
        self.clinical_hidden_dim = getattr(config, 'clinical_hidden_dim', 64)
        self.clinical_layers = getattr(config, 'clinical_layers', 2)
        self.post_attention_dim = getattr(config, 'post_attention_dim', 128)  # NEW: output dim after attention

        print(f"Model initialized with fusion_dim: {self.fusion_dim}, clinical_dim: {self.clinical_dim}, post_attention_dim: {self.post_attention_dim}")

        # --- Clinical MLP branch ---
        if self.clinical_dim > 0:
            clinical_layer_dims = []
            current_dim = self.clinical_dim
            for i in range(self.clinical_layers - 1):
                next_dim = max(self.clinical_hidden_dim // (2**i), 32)
                clinical_layer_dims.append(next_dim)
                current_dim = next_dim
            self.clinical_mlp = create_mlp(
                in_dim=self.clinical_dim,
                hid_dims=clinical_layer_dims,
                dropout=config.dropout,
                out_dim=self.clinical_hidden_dim,
                end_with_fc=True
            )
            print(f"Clinical MLP created with layers: {self.clinical_dim} -> {clinical_layer_dims} -> {self.clinical_hidden_dim}")
        else:
            self.clinical_mlp = None

        # --- (Optional) MLP for WSI features before attention ---
        self.mlp = create_mlp(
            in_dim=config.in_dim,
            hid_dims=[config.embed_dim] * (config.n_fc_layers - 1),
            dropout=config.dropout,
            out_dim=config.embed_dim,
            end_with_fc=False
        )

        # --- Attention network ---
        if config.gate:
            self.attention_net = Attn_Net_Gated(
                L=config.in_dim,
                D=config.attn_dim,
                dropout=config.dropout,
                n_classes=1
            )
        else:
            self.attention_net = Attn_Net(
                L=config.in_dim,
                D=config.attn_dim,
                dropout=config.dropout,
                n_classes=1
            )
        
        # --- NEW: Linear layer to reduce attention output dimension ---
        self.post_attention_fc = nn.Linear(config.in_dim, self.post_attention_dim)

        # --- Update classifier input dimension for fusion ---
        classifier_input_dim = self.post_attention_dim + self.fusion_dim
        if self.clinical_dim > 0:
            classifier_input_dim += self.clinical_hidden_dim

        print(f"Classifier input dimension: {classifier_input_dim} (post_attention_dim: {self.post_attention_dim} + fusion_dim: {self.fusion_dim} + clinical_hidden_dim: {self.clinical_hidden_dim if self.clinical_dim > 0 else 0})")
        self.classifier = nn.Linear(classifier_input_dim, config.n_classes)
        print(f"Classifier created: input={self.classifier.in_features}, output={self.classifier.out_features}")
        self.n_classes = config.n_classes

    def forward_attention(self, h, attn_only=False):
        # h: (B, N, in_dim)
        A = self.attention_net(h)  # (B, N, K)
        A = torch.transpose(A, -2, -1)  # (B, K, N)
        if attn_only:
            return A
        else:
            return h, A

    def forward_no_loss(self, h, additional_embeddings=None, clinical_features=None, attn_mask=None):
        h, A = self.forward_attention(h)
        if attn_mask is not None:
            A = A + (1 - attn_mask).unsqueeze(dim=1) * torch.finfo(A.dtype).min
        A = F.softmax(A, dim=-1)
        M = torch.bmm(A, h).squeeze(dim=1)  # (B, in_dim)
        # --- Reduce dimension after attention ---
        M = self.post_attention_fc(M)  # (B, post_attention_dim)

        # --- FUSION OF FEATURES (WSI + MRI + CLINICAL) ---
        fusion_features = [M]
        if additional_embeddings is not None:
            fusion_features.append(additional_embeddings)
        elif self.fusion_dim > 0:
            zeros = torch.zeros(M.size(0), self.fusion_dim, device=M.device, dtype=M.dtype)
            fusion_features.append(zeros)
        if clinical_features is not None and self.clinical_mlp is not None:
            clinical_embedding = self.clinical_mlp(clinical_features)
            fusion_features.append(clinical_embedding)
        elif self.clinical_dim > 0 and self.clinical_mlp is not None:
            zeros = torch.zeros(M.size(0), self.clinical_hidden_dim, device=M.device, dtype=M.dtype)
            fusion_features.append(zeros)
        M = torch.cat(fusion_features, dim=-1)
        logits = self.classifier(M)
        out = {
            'logits': logits,
            'attn': A,
            'feats': h,
            'feats_agg': M
        }
        return out

    def forward(self, h, additional_embeddings=None, clinical_features=None, model_kwargs={}):
        if self.mode == 'classification':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            loss_fn = model_kwargs['loss_fn']
            out = self.forward_no_loss(h, additional_embeddings=additional_embeddings, 
                                       clinical_features=clinical_features, attn_mask=attn_mask)
            logits = out['logits']
            results_dict, log_dict = process_clf(logits, label, loss_fn)
        elif self.mode == 'survival':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            censorship = model_kwargs['censorship']
            loss_fn = model_kwargs['loss_fn']
            out = self.forward_no_loss(h, additional_embeddings=additional_embeddings, 
                                       clinical_features=clinical_features, attn_mask=attn_mask)
            logits = out['logits']
            results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)
        else:
            raise NotImplementedError("Mode not implemented!")
        return results_dict, log_dict

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/model_linear.py
import torch
import torch.nn as nn
import pdb

from .components import create_mlp, create_mlp_with_dropout, process_surv, process_clf

class LinearEmb(nn.Module):
    """
    Linear fully-connected layer from slide representation to output
    """
    def __init__(self, config, mode):
        super().__init__()
        self.config = config
        self.classifier = nn.Linear(config.in_dim, config.n_classes, bias=False)
        self.n_classes = config.n_classes
        self.mode = mode

    def forward_no_loss(self, h, attn_mask=None):
        logits = self.classifier(h)
        out = {'logits': logits}
        return out
    
    def forward(self, h, model_kwargs={}):
        if self.mode == 'classification':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_clf(logits, label, loss_fn)
        elif self.mode == 'survival':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            censorship = model_kwargs['censorship']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)
        else:
            raise NotImplementedError("Not Implemented!")
        
        return results_dict, log_dict


#
# MLP per prototype
#
class IndivMLPEmb(nn.Module):
    """
    Comprised of three MLP (in sequence), each of which can be enabled/disabled and configured accordingly
    - Shared: Shared MLP across prototypes for feature dimension reduction
    - Indiv: Individual MLP per prototype
    - Post: Shared MLP across prototypes for final feature dimension reduction
    """
    def __init__(self, config, mode):
        super().__init__()
        self.config = config
        self.n_classes = config.n_classes
        self.p = config.p
        self.mode = mode

        mlp_func = create_mlp_with_dropout

        if config.shared_mlp:
            self.shared_mlp = mlp_func(in_dim=config.in_dim,
                                    hid_dims=[config.shared_embed_dim] *
                                            (config.n_fc_layers - 1),
                                    dropout=config.shared_dropout,
                                    out_dim=config.shared_embed_dim,
                                    end_with_fc=False)
            next_in_dim = config.shared_embed_dim
        else:
            self.shared_mlp = nn.Identity()
            next_in_dim = config.in_dim
            
        if config.indiv_mlps:
            self.indiv_mlps = nn.ModuleList([mlp_func(in_dim=next_in_dim,
                                hid_dims=[config.indiv_embed_dim] *
                                        (config.n_fc_layers - 1),
                                dropout=config.indiv_dropout,
                                out_dim=config.indiv_embed_dim,
                                end_with_fc=False) for i in range(config.p)])
            next_in_dim = config.p * config.indiv_embed_dim
        else:
            self.indiv_mlps = nn.ModuleList([nn.Identity() for i in range (config.p)])
            next_in_dim = config.p * next_in_dim

        if config.postcat_mlp:
            self.postcat_mlp = mlp_func(in_dim=next_in_dim,
                                    hid_dims=[config.postcat_embed_dim] *
                                            (config.n_fc_layers - 1),
                                    dropout=config.postcat_dropout,
                                    out_dim=config.postcat_embed_dim,
                                    end_with_fc=False)
            next_in_dim = config.postcat_embed_dim
        else:
            self.postcat_mlp = nn.Identity()
        
        self.classifier = nn.Linear(next_in_dim,
                                    config.n_classes,
                                    bias=False)

    def forward_no_loss(self, h, attn_mask=None):
        h = self.shared_mlp(h)
        h = torch.stack([self.indiv_mlps[idx](h[:, idx, :]) for idx in range(self.p)], dim=1)
        h = h.reshape(h.shape[0], -1)   # (n_samples, n_proto * config.indiv_embed_dim)
        h = self.postcat_mlp(h)
        logits = self.classifier(h)
        out = {'logits': logits}
        return out
        
    def forward(self, h, model_kwargs={}):
        if self.mode == 'classification':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_clf(logits, label, loss_fn)
        elif self.mode == 'survival':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            censorship = model_kwargs['censorship']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)
        else:
            raise NotImplementedError("Not Implemented!")
        
        return results_dict, log_dict

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/__init__.py
from .model_abmil import ABMIL
from .model_abmil_fusion import ABMIL as ABMIL_FUSION
from .model_configs import PretrainedConfig, ABMILConfig
from .model_factory import create_downstream_model, prepare_emb

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/model_OT.py
"""
Optimal transport (OT)-based aggregation

Ref:
    Mialon, GrÃ©goire, et al. "A trainable optimal transport embedding for feature aggregation and its relationship to attention." arXiv preprint arXiv:2006.12065 (2020).
    https://github.com/claying/OTK
"""

from .components import create_mlp, predict_surv, predict_clf, predict_emb
from .OT.otk.layers import OTKernel
from utils.proto_utils import check_prototypes
from utils.file_utils import save_pkl, load_pkl

import torch
from torch import nn
import numpy as np
import pdb

class OT(nn.Module):
    """
    OTK method without bells and whistles (no convolutional kernel & no bioembedding)
    """

    def __init__(self, config, mode):
        super().__init__()
        self.config = config

        self.attention = OTKernel(in_dim=config.in_dim, 
                                  out_size=config.out_size, 
                                  distance=config.distance,
                                  heads=config.heads, 
                                  max_iter=config.max_iter, 
                                  eps=config.ot_eps)

        self.out_type = config.out_type
        self.mode = mode

        if self.out_type == 'allcat':
            self.out_features = config.in_dim * config.out_size * config.heads
        elif self.out_type == 'weight_avg_mean':
            self.out_features = config.in_dim * config.heads
        else:
            raise NotImplementedError(f"OT Not implemented for {self.out_type}!")

        self.nclass = config.n_classes

        check_prototypes(config.out_size, config.in_dim, config.load_proto, config.proto_path)

        if config.load_proto:
            if config.proto_path.endswith('pkl'):
                weights = load_pkl(config.proto_path)['prototypes'].squeeze()
            elif config.proto_path.endswith('npy'):
                weights = np.load(config.proto_path)
            weights = torch.from_numpy(weights)
            self.attention.weight.data.copy_(weights)

    def representation(self, x):
        """
        Construct unsupervised slide representation
        """
        B = x.shape[0]
        # out = self.attention(x.permute(0, 2, 1))    # (batch_size, out_size, in_dim)
        out = self.attention(x)    # (batch_size, out_size, in_dim)

        if self.out_type == 'allcat':
            out = out.reshape(B, -1)
        elif self.out_type == 'weight_avg_mean':
            out = torch.mean(out, dim=1)
        else:
            raise NotImplementedError(f"OTK Not implemented for {self.out_type}!")
        
        return {'repr': out}

    def forward(self, x):
        out = self.representation(x)
        return out['repr']
    
    def predict(self, data_loader, use_cuda=True):
        if self.mode == 'classification':
            output, y = predict_clf(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'survival':
            output, y = predict_surv(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'emb':
            output = predict_emb(self, data_loader.dataset, use_cuda=use_cuda)
            y = None
        else:
            raise NotImplementedError(f"Not implemented for {self.mode}!")
        
        return output, y

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/model_configs.py
from dataclasses import dataclass, asdict
from typing import Optional, Union, Callable
import logging
import json
import os
logger = logging.getLogger(__name__)


@dataclass
class PretrainedConfig:
    def to_json_file(self, json_file_path: Union[str, os.PathLike]):
        """
        Save this instance to a JSON file.
        Args:
            json_file_path: Path to the JSON file in which this configuration instance's parameters will be saved.
        """
        config_dict = {k: v for k, v in asdict(self).items()}
        with open(json_file_path, "w", encoding="utf-8") as writer:
            writer.write(json.dumps(
                config_dict, indent=2, sort_keys=False) + "\n")

    @classmethod
    def from_pretrained(cls, config_path: Union[str, os.PathLike], update_dict={}):
        config_dict = json.load(open(config_path))
        for key in update_dict:
            config_dict[key] = update_dict[key]  # Always update, regardless of whether key exists
        config = cls(**config_dict)
        return config

    def save_pretrained(self, save_directory: Union[str, os.PathLike]):
        """
        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the
        [`~PretrainedConfig.from_pretrained`] class method.
        Args:
            save_directory (`str` or `os.PathLike`):
                Directory where the configuration JSON file will be saved (will be created if it does not exist).
        """
        if os.path.isfile(save_directory):
            raise AssertionError(
                f"Provided path ({save_directory}) should be a directory, not a file")

        os.makedirs(save_directory, exist_ok=True)

        # If we save using the predefined names, we can load using `from_pretrained`
        output_config_file = os.path.join(save_directory, "config.json")

        self.to_json_file(output_config_file)
        logger.info(f"Configuration saved in {output_config_file}")

@dataclass
class ABMILConfig(PretrainedConfig):
    gate: bool = True
    in_dim: int = 768
    n_classes: int = 2
    embed_dim: int = 512
    attn_dim: int = 384
    n_fc_layers: int = 1
    dropout: float = 0.25
    fusion_dim: int = 0  # Dimension for fusion features (e.g., MRI features)
    clinical_dim: int = 0  # Dimension for clinical features
    clinical_hidden_dim: int = 128  # Hidden dimension for clinical MLP
    clinical_layers: int = 2  # Number of layers in clinical MLP

@dataclass
class OTConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    n_filters: int = 2048
    len_motifs: int = 1
    subsamplings: int = 1
    kernel_args: int = 0.4
    weight_decay: float = 0.0001
    ot_eps: float = 3.0
    heads: int = 1
    out_size: int = 3
    out_type: str = 'param_cat'
    max_iter: int = 100
    distance: str = 'euclidean'
    fit_bias: bool = False
    alternating: bool = False
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True


@dataclass
class PANTHERConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    heads: int = 1
    em_iter: int = 3
    tau: float = 0.001
    embed_dim: int = 512
    ot_eps: int = 0.1
    n_fc_layers: int = 1
    dropout: float = 0.
    out_type: str = 'param_cat'
    out_size: int = 3
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True


@dataclass
class ProtoCountConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    out_size: int = 3
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True

@dataclass
class H2TConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    out_size: int = 3
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True

@dataclass
class LinearEmbConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2


@dataclass
class IndivMLPEmbConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    embed_dim: int = 128
    n_fc_layers: int = 2
    dropout: float = 0.25
    proto_model_type: str = 'DIEM'
    p: int = 32
    out_type: str = 'param_cat'

@dataclass
class IndivMLPEmbConfig_Shared(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = True
    indiv_mlps: bool = False
    postcat_mlp: bool = False
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32

@dataclass
class IndivMLPEmbConfig_Indiv(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = False
    indiv_mlps: bool = True
    postcat_mlp: bool = False
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32

@dataclass
class IndivMLPEmbConfig_SharedPost(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = True
    indiv_mlps: bool = False
    postcat_mlp: bool = True
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32

@dataclass
class IndivMLPEmbConfig_IndivPost(PretrainedConfig):
    in_dim: int = 2049
    n_classes: int = 4
    shared_embed_dim: int = 256
    indiv_embed_dim: int = 128
    postcat_embed_dim: int = 1024
    shared_mlp: bool = False
    indiv_mlps: bool = True
    postcat_mlp: bool = True
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 16
    use_snn: bool = False

@dataclass
class IndivMLPEmbConfig_SharedIndiv(PretrainedConfig):
    in_dim: int = 2049
    n_classes: int = 4
    shared_embed_dim: int = 256
    indiv_embed_dim: int = 128
    postcat_embed_dim: int = 1024
    shared_mlp: bool = True
    indiv_mlps: bool = True
    postcat_mlp: bool = False
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 16
    use_snn: bool = False

@dataclass
class IndivMLPEmbConfig_SharedIndivPost(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = True
    indiv_mlps: bool = True
    postcat_mlp: bool = True
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/components.py
import torch.nn as nn
import torch
from tqdm import tqdm

from utils.losses import NLLSurvLoss, CoxLoss, SurvRankingLoss
from sksurv.util import Surv

def create_mlp(in_dim=None, hid_dims=[], act=nn.ReLU(), dropout=0.,
               out_dim=None, end_with_fc=True, bias=True):

    layers = []
    if len(hid_dims) < 0:
        mlp = nn.Identity()
    elif len(hid_dims) >= 0:
        if len(hid_dims) > 0:
            for hid_dim in hid_dims:
                layers.append(nn.Linear(in_dim, hid_dim, bias=bias))
                layers.append(act)
                layers.append(nn.Dropout(dropout))
                in_dim = hid_dim
        layers.append(nn.Linear(in_dim, out_dim))
        if not end_with_fc:
            layers.append(act)
        mlp = nn.Sequential(*layers)
    return mlp

def create_mlp_with_dropout(in_dim=None, hid_dims=[], act=nn.ReLU(), dropout=0.,
               out_dim=None, end_with_fc=True, bias=True):

    layers = []
    if len(hid_dims) < 0:
        mlp = nn.Identity()
    elif len(hid_dims) >= 0:
        if len(hid_dims) > 0:
            for hid_dim in hid_dims:
                layers.append(nn.Linear(in_dim, hid_dim, bias=bias))
                layers.append(act)
                layers.append(nn.Dropout(dropout))
                in_dim = hid_dim
        layers.append(nn.Linear(in_dim, out_dim))
        if not end_with_fc:
            layers.append(act)
            layers.append(nn.Dropout(dropout))
        mlp = nn.Sequential(*layers)
    return mlp

#
# Model processing
#
def predict_emb(self, dataset, use_cuda=True, permute=False):
    """
    Create prototype-based slide representation

    Returns
    - X (torch.Tensor): (n_data x output_set_dim)
    - y (torch.Tensor): (n_data)
    """

    X = []

    for i in tqdm(range(len(dataset))):
        batch = dataset.__getitem__(i)
        data = batch['img'].unsqueeze(dim=0)
        if use_cuda:
            data = data.cuda()
        
        with torch.no_grad():
            out = self.representation(data)
            out = out['repr'].data.detach().cpu()

        X.append(out)

    X = torch.cat(X)

    return X

def predict_clf(self, dataset, use_cuda=True, permute=False):
    """
    Create prototype-based slide representation

    Returns
    - X (torch.Tensor): (n_data x output_set_dim)
    - y (torch.Tensor): (n_data)
    """

    X, y = [], []

    for i in tqdm(range(len(dataset))):
        batch = dataset.__getitem__(i)
        data = batch['img'].unsqueeze(dim=0)
        label = batch['label']
        if use_cuda:
            data = data.cuda()
        
        with torch.no_grad():
            out = self.representation(data)
            out = out['repr'].data.detach().cpu()

        X.append(out)
        y.append(label)

    X = torch.cat(X)
    y = torch.Tensor(y).to(torch.long)

    return X, y

def process_clf(logits, label, loss_fn):
    results_dict = {'logits': logits}
    log_dict = {}

    if loss_fn is not None and label is not None:
        cls_loss = loss_fn(logits, label)
        loss = cls_loss
        log_dict.update({
            'cls_loss': cls_loss.item(),
            'loss': loss.item()})
        results_dict.update({'loss': loss})
    
    return results_dict, log_dict

def predict_surv(self, dataset,  use_cuda=True, permute=False):
    """
    Create prototype-based slide representation
    """

    output = []
    label_output = []
    censor_output = []
    time_output = []

    for i in tqdm(range(len(dataset))):
        batch = dataset.__getitem__(i)
        data, label, censorship, time = batch['img'].unsqueeze(dim=0), batch['label'].unsqueeze(dim=0), batch['censorship'].unsqueeze(dim=0), batch['survival_time'].unsqueeze(dim=0)
        batch_size = data.shape[0]

        if use_cuda:
            data = data.cuda()

        with torch.no_grad():
            batch_out = self.representation(data)
            batch_out = batch_out['repr'].data.cpu()

        output.append(batch_out)
        label_output.append(label)
        censor_output.append(censorship)
        time_output.append(time)

    output = torch.cat(output)
    label_output = torch.cat(label_output)
    censor_output = torch.cat(censor_output)
    time_output = torch.cat(time_output)

    y = Surv.from_arrays(~censor_output.numpy().astype('bool').squeeze(),
                            time_output.numpy().squeeze()
                            )
    
    return output, y


def process_surv(logits, label, censorship, loss_fn=None):
    results_dict = {'logits': logits}
    log_dict = {}

    if loss_fn is not None and label is not None:
        if isinstance(loss_fn, NLLSurvLoss):
            surv_loss_dict = loss_fn(logits=logits, times=label, censorships=censorship)
            hazards = torch.sigmoid(logits)
            survival = torch.cumprod(1 - hazards, dim=1)
            risk = -torch.sum(survival, dim=1).unsqueeze(dim=1)
            results_dict.update({'hazards': hazards,
                                    'survival': survival,
                                    'risk': risk})
        elif isinstance(loss_fn, CoxLoss):
            # logits is log risk
            surv_loss_dict = loss_fn(logits=logits, times=label, censorships=censorship)
            risk = torch.exp(logits)
            results_dict['risk'] = risk

        elif isinstance(loss_fn, SurvRankingLoss):
                surv_loss_dict = loss_fn(z=logits, times=label, censorships=censorship)
                results_dict['risk'] = logits

        loss = surv_loss_dict['loss']
        log_dict['surv_loss'] = surv_loss_dict['loss'].item()
        log_dict.update(
            {k: v.item() for k, v in surv_loss_dict.items() if isinstance(v, torch.Tensor)})
        results_dict.update({'loss': loss})

    return results_dict, log_dict


#
# Attention networks
#
class Attn_Net(nn.Module):
    """
    Attention Network without Gating (2 fc layers)
    args:
        L: input feature dimension
        D: hidden layer dimension
        dropout: dropout
        n_classes: number of classes
    """

    def __init__(self, L=1024, D=256, dropout=0., n_classes=1):
        super(Attn_Net, self).__init__()
        self.module = [
            nn.Linear(L, D),
            nn.Tanh(),
            nn.Dropout(dropout),
            nn.Linear(D, n_classes)]

        self.module = nn.Sequential(*self.module)

    def forward(self, x):
        return self.module(x), x  # N x n_classes


class Attn_Net_Gated(nn.Module):
    """
    Attention Network with Sigmoid Gating (3 fc layers)
    args:
        L: input feature dimension
        D: hidden layer dimension
        dropout: dropout
        n_classes: number of classes
    """

    def __init__(self, L=1024, D=256, dropout=0., n_classes=1):
        super(Attn_Net_Gated, self).__init__()
        self.attention_a = [
            nn.Linear(L, D),
            nn.Tanh(),
            nn.Dropout(dropout)]

        self.attention_b = [nn.Linear(L, D),
                            nn.Sigmoid(),
                            nn.Dropout(dropout)]

        self.attention_a = nn.Sequential(*self.attention_a)
        self.attention_b = nn.Sequential(*self.attention_b)

        self.attention_c = nn.Linear(D, n_classes)

    def forward(self, x):
        a = self.attention_a(x)
        b = self.attention_b(x)
        A = a.mul(b)
        A = self.attention_c(A)  # N x n_classes
        return A


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/model_factory.py
import os
from mil_models import ABMIL, ABMIL_FUSION
from mil_models import ABMILConfig

import pdb
import torch
from utils.file_utils import save_pkl, load_pkl
from os.path import join as j_
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")




def create_downstream_model(args, mode='classification', config_dir='/data/temporary/nadieh/PANTHER/src/configs'):
    """
    Create downstream modles for classification or survival
    """
    config_path = os.path.join(config_dir, args.model_config, 'config.json')
    assert os.path.exists(config_path), f"Config path {config_path} doesn't exist!"
    
    model_config = args.model_config
    model_type = args.model_type

    if 'IndivMLPEmb' in model_config:
        print(args)
        update_dict = {'in_dim': args.in_dim,
                       'p': 16,
                       'out_type': args.out_type,
                       }

    elif model_type == 'DeepAttnMIL':
        update_dict = {'in_dim': args.in_dim,
                       'out_size': args.out_size,
                       'load_proto': args.load_proto,
                       'fix_proto': args.fix_proto,
                       'proto_path': args.proto_path}
    else:
        update_dict = {'in_dim': args.in_dim}

    # Always add fusion_dim for ABMIL models to support fusion capabilities
    if model_type == 'ABMIL':
        update_dict.update({
            'fusion_dim': 320,  # MRI feature dimension
            'clinical_dim': 64,  # Default clinical feature dimension
            'clinical_hidden_dim': 64,  # Default hidden dimension for clinical MLP
            'clinical_layers': 2  # Default number of layers in clinical MLP
        })

    # Set clinical dimension based on the clinical processor if available
    if model_type == 'ABMIL' and hasattr(args, 'clinical_processor') and args.clinical_processor is not None:
        clinical_dim = args.clinical_processor.output_dim
        update_dict.update({'clinical_dim': clinical_dim})
        print(f"Setting clinical_dim to {clinical_dim} based on clinical processor")
    
    if mode == 'classification':
        update_dict.update({'n_classes': args.n_classes})
    elif mode == 'survival':
        if args.loss_fn == 'nll':
            update_dict.update({'n_classes': args.n_label_bins})
        elif args.loss_fn == 'cox':
            update_dict.update({'n_classes': 1})
        elif args.loss_fn == 'rank':
            update_dict.update({'n_classes': 1})
    else:
        raise NotImplementedError(f"Not implemented for {mode}...")
    
    if model_type == 'ABMIL':
        config = ABMILConfig.from_pretrained(config_path, update_dict=update_dict)
        print(f"Model factory: Config loaded with fusion_dim = {getattr(config, 'fusion_dim', 'NOT FOUND')}")
        print(f"Model factory: Update dict = {update_dict}")
        model = ABMIL_FUSION(config=config, mode=mode)  # Use fusion model by default
    # Prototype-based models will choose from the following
    elif model_type == 'LinearEmb':
        config = LinearEmbConfig.from_pretrained(config_path, update_dict=update_dict)
        model = LinearEmb(config=config, mode=mode)
    elif 'IndivMLPEmb' in model_type:            
        if 'IndivMLPEmb_Shared' == model_type:
            config = IndivMLPEmbConfig_Shared.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_Indiv' == model_type:
            config = IndivMLPEmbConfig_Indiv.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_SharedPost' == model_type:
            config = IndivMLPEmbConfig_SharedPost.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_IndivPost' == model_type:
            config = IndivMLPEmbConfig_IndivPost.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_SharedIndiv' == model_type:
            config = IndivMLPEmbConfig_SharedIndiv.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_SharedIndivPost' == model_type:
            config = IndivMLPEmbConfig_SharedIndivPost.from_pretrained(config_path, update_dict=update_dict)
        print(mode)
        model = IndivMLPEmb(config=config, mode=mode)
    else:
        raise NotImplementedError

    return model


def prepare_emb(datasets, args, mode='classification'):
    """
    Slide representation construction with patch feature aggregation trained in unsupervised manner
    """
   
    ### Preparing file path for saving embeddings
    print('\nConstructing unsupervised slide embedding...', end=' ')
    embeddings_kwargs = {
        'feats': args.data_source[0].split('/')[-2],
        'model_type': args.model_type,
        'out_size': args.n_proto
    }

    # Create embedding path
    fpath = "{feats}_{model_type}_embeddings_proto_{out_size}".format(**embeddings_kwargs)
    if args.model_type == 'PANTHER':
        DIEM_kwargs = {'tau': args.tau, 'out_type': args.out_type, 'eps': args.ot_eps, 'em_step': args.em_iter}
        name = '_{out_type}_em_{em_step}_eps_{eps}_tau_{tau}'.format(**DIEM_kwargs)
        fpath += name
    elif args.model_type == 'OT':
        OTK_kwargs = {'out_type': args.out_type, 'eps': args.ot_eps}
        name = '_{out_type}_eps_{eps}'.format(**OTK_kwargs)
        fpath += name
    embeddings_fpath = j_(args.split_dir, 'embeddings', fpath+'.pkl')
    
    ### Load existing embeddings if already created
    if os.path.isfile(embeddings_fpath):
        embeddings = load_pkl(embeddings_fpath)
        for k, loader in datasets.items():
            print(f'\n\tEmbedding already exists! Loading {k}', end=' ')
            loader.dataset.X, loader.dataset.y = embeddings[k]['X'], embeddings[k]['y']
    else:
        os.makedirs(j_(args.split_dir, 'embeddings'), exist_ok=True)
        
        model = create_embedding_model(args, mode=mode).to(device)

        ### Extracts prototypical features per split
        embeddings = {}
        for split, loader in datasets.items():
            print(f"\nAggregating {split} set features...")
            X, y = model.predict(loader,
                                 use_cuda=torch.cuda.is_available()
                                 )
            loader.dataset.X, loader.dataset.y = X, y
            embeddings[split] = {'X': X, 'y': y}
        save_pkl(embeddings_fpath, embeddings)

    return datasets, embeddings_fpath

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/model_abmil.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pdb

from .components import Attn_Net, Attn_Net_Gated, create_mlp, process_surv, process_clf
from .model_configs import ABMILConfig


class ABMIL(nn.Module):
    def __init__(self, config, mode):
        super().__init__()
        self.config = config
        self.mlp = create_mlp(in_dim=config.in_dim,
                              hid_dims=[config.embed_dim] *
                              (config.n_fc_layers - 1),
                              dropout=config.dropout,
                              out_dim=config.embed_dim,
                              end_with_fc=False)

        if config.gate:
            self.attention_net = Attn_Net_Gated(L=self.config.embed_dim,
                                                D=config.attn_dim,
                                                dropout=config.dropout,
                                                n_classes=1)
        else:
            self.attention_net = Attn_Net(L=config.embed_dim,
                                          D=config.attn_dim,
                                          dropout=config.dropout,
                                          n_classes=1)

        self.classifier = nn.Linear(config.embed_dim, config.n_classes)
        self.n_classes = config.n_classes

        self.mode = mode

    def forward_attention(self, h, attn_only=False):
        # B: batch size
        # N: number of instances per WSI
        # L: input dimension
        # K: number of attention heads (K = 1 for ABMIL)
        # h is B x N x L
        h = self.mlp(h)
        # h is B x N x D
        A = self.attention_net(h)  # B x N x K
        A = torch.transpose(A, -2, -1)  # B x K x N 
        if attn_only:
            return A
        else:
            return h, A

    def forward_no_loss(self, h, attn_mask=None):
        h, A = self.forward_attention(h)
        A_raw = A
        # A is B x K x N 
        if attn_mask is not None:
            A = A + (1 - attn_mask).unsqueeze(dim=1) * torch.finfo(A.dtype).min
        A = F.softmax(A, dim=-1)  # softmax over N
        M = torch.bmm(A, h).squeeze(dim=1) # B x K x C --> B x C
        logits = self.classifier(M)

        out = {'logits': logits, 'attn': A, 'feats': h, 'feats_agg': M}

        return out
    
    def forward(self, h, model_kwargs={}):

        if self.mode == 'classification':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_clf(logits, label, loss_fn)
        elif self.mode == 'survival':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            censorship = model_kwargs['censorship']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)
        else:
            raise NotImplementedError("Not Implemented!")

        return results_dict, log_dict


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/components-checkpoint.py
import torch.nn as nn
import torch
from tqdm import tqdm

from utils.losses import NLLSurvLoss, CoxLoss, SurvRankingLoss
from sksurv.util import Surv

def create_mlp(in_dim=None, hid_dims=[], act=nn.ReLU(), dropout=0.,
               out_dim=None, end_with_fc=True, bias=True):

    layers = []
    if len(hid_dims) < 0:
        mlp = nn.Identity()
    elif len(hid_dims) >= 0:
        if len(hid_dims) > 0:
            for hid_dim in hid_dims:
                layers.append(nn.Linear(in_dim, hid_dim, bias=bias))
                layers.append(act)
                layers.append(nn.Dropout(dropout))
                in_dim = hid_dim
        layers.append(nn.Linear(in_dim, out_dim))
        if not end_with_fc:
            layers.append(act)
        mlp = nn.Sequential(*layers)
    return mlp

def create_mlp_with_dropout(in_dim=None, hid_dims=[], act=nn.ReLU(), dropout=0.,
               out_dim=None, end_with_fc=True, bias=True):

    layers = []
    if len(hid_dims) < 0:
        mlp = nn.Identity()
    elif len(hid_dims) >= 0:
        if len(hid_dims) > 0:
            for hid_dim in hid_dims:
                layers.append(nn.Linear(in_dim, hid_dim, bias=bias))
                layers.append(act)
                layers.append(nn.Dropout(dropout))
                in_dim = hid_dim
        layers.append(nn.Linear(in_dim, out_dim))
        if not end_with_fc:
            layers.append(act)
            layers.append(nn.Dropout(dropout))
        mlp = nn.Sequential(*layers)
    return mlp

#
# Model processing
#
def predict_emb(self, dataset, use_cuda=True, permute=False):
    """
    Create prototype-based slide representation

    Returns
    - X (torch.Tensor): (n_data x output_set_dim)
    - y (torch.Tensor): (n_data)
    """

    X = []

    for i in tqdm(range(len(dataset))):
        batch = dataset.__getitem__(i)
        data = batch['img'].unsqueeze(dim=0)
        if use_cuda:
            data = data.cuda()
        
        with torch.no_grad():
            out = self.representation(data)
            out = out['repr'].data.detach().cpu()

        X.append(out)

    X = torch.cat(X)

    return X

def predict_clf(self, dataset, use_cuda=True, permute=False):
    """
    Create prototype-based slide representation

    Returns
    - X (torch.Tensor): (n_data x output_set_dim)
    - y (torch.Tensor): (n_data)
    """

    X, y = [], []

    for i in tqdm(range(len(dataset))):
        batch = dataset.__getitem__(i)
        data = batch['img'].unsqueeze(dim=0)
        label = batch['label']
        if use_cuda:
            data = data.cuda()
        
        with torch.no_grad():
            out = self.representation(data)
            out = out['repr'].data.detach().cpu()

        X.append(out)
        y.append(label)

    X = torch.cat(X)
    y = torch.Tensor(y).to(torch.long)

    return X, y

def process_clf(logits, label, loss_fn):
    results_dict = {'logits': logits}
    log_dict = {}

    if loss_fn is not None and label is not None:
        cls_loss = loss_fn(logits, label)
        loss = cls_loss
        log_dict.update({
            'cls_loss': cls_loss.item(),
            'loss': loss.item()})
        results_dict.update({'loss': loss})
    
    return results_dict, log_dict

def predict_surv(self, dataset,  use_cuda=True, permute=False):
    """
    Create prototype-based slide representation
    """

    output = []
    label_output = []
    censor_output = []
    time_output = []

    for i in tqdm(range(len(dataset))):
        batch = dataset.__getitem__(i)
        data, label, censorship, time = batch['img'].unsqueeze(dim=0), batch['label'].unsqueeze(dim=0), batch['censorship'].unsqueeze(dim=0), batch['survival_time'].unsqueeze(dim=0)
        batch_size = data.shape[0]

        if use_cuda:
            data = data.cuda()

        with torch.no_grad():
            batch_out = self.representation(data)
            batch_out = batch_out['repr'].data.cpu()

        output.append(batch_out)
        label_output.append(label)
        censor_output.append(censorship)
        time_output.append(time)

    output = torch.cat(output)
    label_output = torch.cat(label_output)
    censor_output = torch.cat(censor_output)
    time_output = torch.cat(time_output)

    y = Surv.from_arrays(~censor_output.numpy().astype('bool').squeeze(),
                            time_output.numpy().squeeze()
                            )
    
    return output, y


def process_surv(logits, label, censorship, loss_fn=None):
    results_dict = {'logits': logits}
    log_dict = {}

    if loss_fn is not None and label is not None:
        if isinstance(loss_fn, NLLSurvLoss):
            surv_loss_dict = loss_fn(logits=logits, times=label, censorships=censorship)
            hazards = torch.sigmoid(logits)
            survival = torch.cumprod(1 - hazards, dim=1)
            risk = -torch.sum(survival, dim=1).unsqueeze(dim=1)
            results_dict.update({'hazards': hazards,
                                    'survival': survival,
                                    'risk': risk})
        elif isinstance(loss_fn, CoxLoss):
            # logits is log risk
            surv_loss_dict = loss_fn(logits=logits, times=label, censorships=censorship)
            risk = torch.exp(logits)
            results_dict['risk'] = risk

        elif isinstance(loss_fn, SurvRankingLoss):
                surv_loss_dict = loss_fn(z=logits, times=label, censorships=censorship)
                results_dict['risk'] = logits

        loss = surv_loss_dict['loss']
        log_dict['surv_loss'] = surv_loss_dict['loss'].item()
        log_dict.update(
            {k: v.item() for k, v in surv_loss_dict.items() if isinstance(v, torch.Tensor)})
        results_dict.update({'loss': loss})

    return results_dict, log_dict


#
# Attention networks
#
class Attn_Net(nn.Module):
    """
    Attention Network without Gating (2 fc layers)
    args:
        L: input feature dimension
        D: hidden layer dimension
        dropout: dropout
        n_classes: number of classes
    """

    def __init__(self, L=1024, D=256, dropout=0., n_classes=1):
        super(Attn_Net, self).__init__()
        self.module = [
            nn.Linear(L, D),
            nn.Tanh(),
            nn.Dropout(dropout),
            nn.Linear(D, n_classes)]

        self.module = nn.Sequential(*self.module)

    def forward(self, x):
        return self.module(x), x  # N x n_classes


class Attn_Net_Gated(nn.Module):
    """
    Attention Network with Sigmoid Gating (3 fc layers)
    args:
        L: input feature dimension
        D: hidden layer dimension
        dropout: dropout
        n_classes: number of classes
    """

    def __init__(self, L=1024, D=256, dropout=0., n_classes=1):
        super(Attn_Net_Gated, self).__init__()
        self.attention_a = [
            nn.Linear(L, D),
            nn.Tanh(),
            nn.Dropout(dropout)]

        self.attention_b = [nn.Linear(L, D),
                            nn.Sigmoid(),
                            nn.Dropout(dropout)]

        self.attention_a = nn.Sequential(*self.attention_a)
        self.attention_b = nn.Sequential(*self.attention_b)

        self.attention_c = nn.Linear(D, n_classes)

    def forward(self, x):
        a = self.attention_a(x)
        b = self.attention_b(x)
        A = a.mul(b)
        A = self.attention_c(A)  # N x n_classes
        return A


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/tokenizer-checkpoint.py
import torch
import torch.nn as nn

class PrototypeTokenizer(nn.Module):
    """
    Tokenize the prototype features, so that we have access to mixture probabilities/mean/covariance
    """
    def __init__(self, proto_model_type='PANTHER', out_type='param_cat', p=8):
        super().__init__()
        self.model_type = proto_model_type
        self.p = p
        self.out_type = out_type

    def get_eff_dim(self):
        return 2 * self.p

    def forward(self, X):
        n_samples = X.shape[0]
        if self.model_type == 'OT':
            if self.out_type == 'allcat':
                prob = 1 / self.p * torch.ones((n_samples, self.p))
                mean = X.reshape(n_samples, self.p, -1)
                cov = None
            else:
                raise NotImplementedError(f"Not implemented for {self.out_type}")

        elif self.model_type == 'PANTHER':
            if self.out_type == 'allcat':
                d = (X.shape[1] - self.p) // (2 * self.p)
                prob = X[:, : self.p]
                mean = X[:, self.p: self.p * (1 + d)].reshape(-1, self.p, d)
                cov = X[:, self.p * (1 + d):].reshape(-1, self.p, d)
            else:
                raise NotImplementedError(f"Not implemented for {self.out_type}")
        else:
            raise NotImplementedError(f"Not implemented for {self.model_type}")


        return prob, mean, cov


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_OT-checkpoint.py
"""
Optimal transport (OT)-based aggregation

Ref:
    Mialon, GrÃ©goire, et al. "A trainable optimal transport embedding for feature aggregation and its relationship to attention." arXiv preprint arXiv:2006.12065 (2020).
    https://github.com/claying/OTK
"""

from .components import create_mlp, predict_surv, predict_clf, predict_emb
from .OT.otk.layers import OTKernel
from utils.proto_utils import check_prototypes
from utils.file_utils import save_pkl, load_pkl

import torch
from torch import nn
import numpy as np
import pdb

class OT(nn.Module):
    """
    OTK method without bells and whistles (no convolutional kernel & no bioembedding)
    """

    def __init__(self, config, mode):
        super().__init__()
        self.config = config

        self.attention = OTKernel(in_dim=config.in_dim, 
                                  out_size=config.out_size, 
                                  distance=config.distance,
                                  heads=config.heads, 
                                  max_iter=config.max_iter, 
                                  eps=config.ot_eps)

        self.out_type = config.out_type
        self.mode = mode

        if self.out_type == 'allcat':
            self.out_features = config.in_dim * config.out_size * config.heads
        elif self.out_type == 'weight_avg_mean':
            self.out_features = config.in_dim * config.heads
        else:
            raise NotImplementedError(f"OT Not implemented for {self.out_type}!")

        self.nclass = config.n_classes

        check_prototypes(config.out_size, config.in_dim, config.load_proto, config.proto_path)

        if config.load_proto:
            if config.proto_path.endswith('pkl'):
                weights = load_pkl(config.proto_path)['prototypes'].squeeze()
            elif config.proto_path.endswith('npy'):
                weights = np.load(config.proto_path)
            weights = torch.from_numpy(weights)
            self.attention.weight.data.copy_(weights)

    def representation(self, x):
        """
        Construct unsupervised slide representation
        """
        B = x.shape[0]
        # out = self.attention(x.permute(0, 2, 1))    # (batch_size, out_size, in_dim)
        out = self.attention(x)    # (batch_size, out_size, in_dim)

        if self.out_type == 'allcat':
            out = out.reshape(B, -1)
        elif self.out_type == 'weight_avg_mean':
            out = torch.mean(out, dim=1)
        else:
            raise NotImplementedError(f"OTK Not implemented for {self.out_type}!")
        
        return {'repr': out}

    def forward(self, x):
        out = self.representation(x)
        return out['repr']
    
    def predict(self, data_loader, use_cuda=True):
        if self.mode == 'classification':
            output, y = predict_clf(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'survival':
            output, y = predict_surv(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'emb':
            output = predict_emb(self, data_loader.dataset, use_cuda=use_cuda)
            y = None
        else:
            raise NotImplementedError(f"Not implemented for {self.mode}!")
        
        return output, y

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_linear-checkpoint.py
import torch
import torch.nn as nn
import pdb

from .components import create_mlp, create_mlp_with_dropout, process_surv, process_clf

class LinearEmb(nn.Module):
    """
    Linear fully-connected layer from slide representation to output
    """
    def __init__(self, config, mode):
        super().__init__()
        self.config = config
        self.classifier = nn.Linear(config.in_dim, config.n_classes, bias=False)
        self.n_classes = config.n_classes
        self.mode = mode

    def forward_no_loss(self, h, attn_mask=None):
        logits = self.classifier(h)
        out = {'logits': logits}
        return out
    
    def forward(self, h, model_kwargs={}):
        if self.mode == 'classification':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_clf(logits, label, loss_fn)
        elif self.mode == 'survival':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            censorship = model_kwargs['censorship']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)
        else:
            raise NotImplementedError("Not Implemented!")
        
        return results_dict, log_dict


#
# MLP per prototype
#
class IndivMLPEmb(nn.Module):
    """
    Comprised of three MLP (in sequence), each of which can be enabled/disabled and configured accordingly
    - Shared: Shared MLP across prototypes for feature dimension reduction
    - Indiv: Individual MLP per prototype
    - Post: Shared MLP across prototypes for final feature dimension reduction
    """
    def __init__(self, config, mode):
        super().__init__()
        self.config = config
        self.n_classes = config.n_classes
        self.p = config.p
        self.mode = mode

        mlp_func = create_mlp_with_dropout

        if config.shared_mlp:
            self.shared_mlp = mlp_func(in_dim=config.in_dim,
                                    hid_dims=[config.shared_embed_dim] *
                                            (config.n_fc_layers - 1),
                                    dropout=config.shared_dropout,
                                    out_dim=config.shared_embed_dim,
                                    end_with_fc=False)
            next_in_dim = config.shared_embed_dim
        else:
            self.shared_mlp = nn.Identity()
            next_in_dim = config.in_dim
            
        if config.indiv_mlps:
            self.indiv_mlps = nn.ModuleList([mlp_func(in_dim=next_in_dim,
                                hid_dims=[config.indiv_embed_dim] *
                                        (config.n_fc_layers - 1),
                                dropout=config.indiv_dropout,
                                out_dim=config.indiv_embed_dim,
                                end_with_fc=False) for i in range(config.p)])
            next_in_dim = config.p * config.indiv_embed_dim
        else:
            self.indiv_mlps = nn.ModuleList([nn.Identity() for i in range (config.p)])
            next_in_dim = config.p * next_in_dim

        if config.postcat_mlp:
            self.postcat_mlp = mlp_func(in_dim=next_in_dim,
                                    hid_dims=[config.postcat_embed_dim] *
                                            (config.n_fc_layers - 1),
                                    dropout=config.postcat_dropout,
                                    out_dim=config.postcat_embed_dim,
                                    end_with_fc=False)
            next_in_dim = config.postcat_embed_dim
        else:
            self.postcat_mlp = nn.Identity()
        
        self.classifier = nn.Linear(next_in_dim,
                                    config.n_classes,
                                    bias=False)

    def forward_no_loss(self, h, attn_mask=None):
        h = self.shared_mlp(h)
        h = torch.stack([self.indiv_mlps[idx](h[:, idx, :]) for idx in range(self.p)], dim=1)
        h = h.reshape(h.shape[0], -1)   # (n_samples, n_proto * config.indiv_embed_dim)
        h = self.postcat_mlp(h)
        logits = self.classifier(h)
        out = {'logits': logits}
        return out
        
    def forward(self, h, model_kwargs={}):
        if self.mode == 'classification':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_clf(logits, label, loss_fn)
        elif self.mode == 'survival':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            censorship = model_kwargs['censorship']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)
        else:
            raise NotImplementedError("Not Implemented!")
        
        return results_dict, log_dict

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/__init__-checkpoint.py
from .model_abmil import ABMIL
from .model_h2t import H2T
from .model_OT import OT
from .model_PANTHER import PANTHER
from .model_linear import LinearEmb, IndivMLPEmb
from .tokenizer import PrototypeTokenizer
from .model_protocount import ProtoCount
from .model_configs import PretrainedConfig, ABMILConfig, \
    OTConfig, PANTHERConfig, H2TConfig, ProtoCountConfig, LinearEmbConfig

from .model_configs import IndivMLPEmbConfig_Indiv, IndivMLPEmbConfig_Shared, IndivMLPEmbConfig_IndivPost, \
        IndivMLPEmbConfig_SharedPost, IndivMLPEmbConfig_SharedIndiv, IndivMLPEmbConfig_SharedIndivPost

from .model_factory import create_downstream_model, create_embedding_model, prepare_emb


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_h2t-checkpoint.py
"""
Hard-clustering-based aggregation

Ref:
    Vu, Quoc Dang, et al. "Handcrafted Histological Transformer (H2T): Unsupervised representation of whole slide images." Medical image analysis 85 (2023): 102743.
"""

import torch
import torch.nn as nn
import os
import numpy as np
import pdb

from tqdm import tqdm
from .components import predict_clf, predict_surv, predict_emb
from utils.file_utils import save_pkl, load_pkl

class H2T(nn.Module):
    """
    WSI is represented as a prototype-count vector
    """
    def __init__(self, config, mode):
        super().__init__()

        assert config.load_proto, "Prototypes must be loaded!"
        assert os.path.exists(config.proto_path), "Path {} doesn't exist!".format(config.proto_path)

        self.config = config
        self.mode = mode
        proto_path = config.proto_path

        if proto_path.endswith('pkl'):
            weights = load_pkl(proto_path)['prototypes'].squeeze()
        elif proto_path.endswith('npy'):
            weights = np.load(proto_path)

        self.n_proto = config.out_size
        self.prototypes = torch.from_numpy(weights).float()
        self.prototypes = self.prototypes / torch.norm(self.prototypes, dim=1).unsqueeze(1)

        emb_dim = config.in_dim

    def representation(self, x):
        """
        Construct unsupervised slide representation
        """
        self.prototypes = self.prototypes.to(x.device)

        x = x / torch.norm(x, dim=-1).unsqueeze(2)
        dist = torch.cdist(self.prototypes, x, p=2) # (1 x n_proto x n_instances)
        c_identity = torch.argmin(dist.squeeze(), dim=0) # (n_instances)

        feats = []
        for idx in range(self.n_proto):
            indices = torch.nonzero(c_identity == idx)
            if len(indices) != 0:
                feat = torch.mean(x[:, indices, :], dim=1)
            else:
                feat = torch.zeros((1,1,x.shape[-1])).to(x.device)
            feats.append(feat)
        out = torch.cat(feats, dim=1)
        out = out.reshape(x.shape[0], -1)
        return {'repr': out}

    def forward(self, x):
        out = self.representation(x)
        return out['repr']

    def predict(self, data_loader, use_cuda=True):
        if self.mode == 'classification':
            output, y = predict_clf(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'survival':
            output, y = predict_surv(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'emb':
            output = predict_emb(self, data_loader.dataset, use_cuda=use_cuda)
            y = None
        else:
            raise NotImplementedError(f"Not implemented for {self.mode}!")
        
        return output, y

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_abmil-checkpoint.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pdb

from .components import Attn_Net, Attn_Net_Gated, create_mlp, process_surv, process_clf
from .model_configs import ABMILConfig


class ABMIL(nn.Module):
    def __init__(self, config, mode):
        super().__init__()
        self.config = config
        self.mlp = create_mlp(in_dim=config.in_dim,
                              hid_dims=[config.embed_dim] *
                              (config.n_fc_layers - 1),
                              dropout=config.dropout,
                              out_dim=config.embed_dim,
                              end_with_fc=False)

        if config.gate:
            self.attention_net = Attn_Net_Gated(L=self.config.embed_dim,
                                                D=config.attn_dim,
                                                dropout=config.dropout,
                                                n_classes=1)
        else:
            self.attention_net = Attn_Net(L=config.embed_dim,
                                          D=config.attn_dim,
                                          dropout=config.dropout,
                                          n_classes=1)

        self.classifier = nn.Linear(config.embed_dim, config.n_classes)
        self.n_classes = config.n_classes

        self.mode = mode

    def forward_attention(self, h, attn_only=False):
        # B: batch size
        # N: number of instances per WSI
        # L: input dimension
        # K: number of attention heads (K = 1 for ABMIL)
        # h is B x N x L
        h = self.mlp(h)
        # h is B x N x D
        A = self.attention_net(h)  # B x N x K
        A = torch.transpose(A, -2, -1)  # B x K x N 
        if attn_only:
            return A
        else:
            return h, A

    def forward_no_loss(self, h, attn_mask=None):
        h, A = self.forward_attention(h)
        A_raw = A
        # A is B x K x N 
        if attn_mask is not None:
            A = A + (1 - attn_mask).unsqueeze(dim=1) * torch.finfo(A.dtype).min
        A = F.softmax(A, dim=-1)  # softmax over N
        M = torch.bmm(A, h).squeeze(dim=1) # B x K x C --> B x C
        logits = self.classifier(M)

        out = {'logits': logits, 'attn': A, 'feats': h, 'feats_agg': M}

        return out
    
    def forward(self, h, model_kwargs={}):

        if self.mode == 'classification':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_clf(logits, label, loss_fn)
        elif self.mode == 'survival':
            attn_mask = model_kwargs['attn_mask']
            label = model_kwargs['label']
            censorship = model_kwargs['censorship']
            loss_fn = model_kwargs['loss_fn']

            out = self.forward_no_loss(h, attn_mask=attn_mask)
            logits = out['logits']

            results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)
        else:
            raise NotImplementedError("Not Implemented!")

        return results_dict, log_dict


# class ABMILSurv(ABMIL):

#     def __init__(self, config: ABMILConfig):
#         super().__init__(config)

#     def forward(self, h, attn_mask=None, label=None, censorship=None, loss_fn=None):
#         out = self.forward_no_loss(h, attn_mask=attn_mask)
#         logits = out['logits']

#         results_dict, log_dict = process_surv(logits, label, censorship, loss_fn)

#         return results_dict, log_dict

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_factory-checkpoint.py
import os
from mil_models import (ABMIL, PANTHER, OT, H2T, ProtoCount, LinearEmb, IndivMLPEmb)

from mil_models import (ABMILConfig, LinearEmbConfig, PANTHERConfig, OTConfig, ProtoCountConfig, H2TConfig)
from mil_models import (IndivMLPEmbConfig_Shared, IndivMLPEmbConfig_Indiv, 
                        IndivMLPEmbConfig_SharedPost, IndivMLPEmbConfig_IndivPost, 
                        IndivMLPEmbConfig_SharedIndiv, IndivMLPEmbConfig_SharedIndivPost)

import pdb
import torch
from utils.file_utils import save_pkl, load_pkl
from os.path import join as j_
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def create_embedding_model(args, mode='classification', config_dir='./configs'):
    """
    Create classification or survival models
    """
    config_path = os.path.join(config_dir, args.model_config, 'config.json')
    assert os.path.exists(config_path), f"Config path {config_path} doesn't exist!"

    model_type = args.model_type
    update_dict = {'in_dim': args.in_dim,
                   'out_size': args.n_proto,
                   'load_proto': args.load_proto,
                   'fix_proto': args.fix_proto,
                   'proto_path': args.proto_path}
    
    if mode == 'classification':
        update_dict.update({'n_classes': args.n_classes})
    elif mode == 'survival':
        if args.loss_fn == 'nll':
            update_dict.update({'n_classes': args.n_label_bins})
        elif args.loss_fn == 'cox':
            update_dict.update({'n_classes': 1})
        elif args.loss_fn == 'rank':
            update_dict.update({'n_classes': 1})
    elif mode == 'emb': # Create just slide-representation model
        pass
    else:
        raise NotImplementedError(f"Not implemented for {mode}...")

    if model_type == 'PANTHER':
        update_dict.update({'out_type': args.out_type})
        config = PANTHERConfig.from_pretrained(config_path, update_dict=update_dict)
        model = PANTHER(config=config, mode=mode)
    elif model_type == 'OT':
        update_dict.update({'out_type': args.out_type})
        config = OTConfig.from_pretrained(config_path, update_dict=update_dict)
        model = OT(config=config, mode=mode)
    elif model_type == 'H2T':
        config = H2TConfig.from_pretrained(config_path, update_dict=update_dict)
        model = H2T(config=config, mode=mode)
    elif model_type == 'ProtoCount':
        config = ProtoCountConfig.from_pretrained(config_path, update_dict=update_dict)
        model = ProtoCount(config=config, mode=mode)
    else:
        raise NotImplementedError(f"Not implemented for {model_type}!")

    return model


def create_downstream_model(args, mode='classification', config_dir='/data/temporary/nadieh/PANTHER/src/configs'):
    """
    Create downstream modles for classification or survival
    """
    config_path = os.path.join(config_dir, args.model_config, 'config.json')
    assert os.path.exists(config_path), f"Config path {config_path} doesn't exist!"
    
    model_config = args.model_config
    model_type = args.model_type

    if 'IndivMLPEmb' in model_config:
        print(args)
        update_dict = {'in_dim': args.in_dim,
                       'p': 16,
                       'out_type': args.out_type,
                       }

    elif model_type == 'DeepAttnMIL':
        update_dict = {'in_dim': args.in_dim,
                       'out_size': args.out_size,
                       'load_proto': args.load_proto,
                       'fix_proto': args.fix_proto,
                       'proto_path': args.proto_path}
    else:
        update_dict = {'in_dim': args.in_dim}

    if mode == 'classification':
        update_dict.update({'n_classes': args.n_classes})
    elif mode == 'survival':
        if args.loss_fn == 'nll':
            update_dict.update({'n_classes': args.n_label_bins})
        elif args.loss_fn == 'cox':
            update_dict.update({'n_classes': 1})
        elif args.loss_fn == 'rank':
            update_dict.update({'n_classes': 1})
    else:
        raise NotImplementedError(f"Not implemented for {mode}...")
    
    if model_type == 'ABMIL':
        config = ABMILConfig.from_pretrained(config_path, update_dict=update_dict)
        model = ABMIL(config=config, mode=mode)
    # Prototype-based models will choose from the following
    elif model_type == 'LinearEmb':
        config = LinearEmbConfig.from_pretrained(config_path, update_dict=update_dict)
        model = LinearEmb(config=config, mode=mode)
    elif 'IndivMLPEmb' in model_type:            
        if 'IndivMLPEmb_Shared' == model_type:
            config = IndivMLPEmbConfig_Shared.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_Indiv' == model_type:
            config = IndivMLPEmbConfig_Indiv.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_SharedPost' == model_type:
            config = IndivMLPEmbConfig_SharedPost.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_IndivPost' == model_type:
            config = IndivMLPEmbConfig_IndivPost.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_SharedIndiv' == model_type:
            config = IndivMLPEmbConfig_SharedIndiv.from_pretrained(config_path, update_dict=update_dict)
        elif 'IndivMLPEmb_SharedIndivPost' == model_type:
            config = IndivMLPEmbConfig_SharedIndivPost.from_pretrained(config_path, update_dict=update_dict)
        print(mode)
        model = IndivMLPEmb(config=config, mode=mode)
    else:
        raise NotImplementedError

    return model


def prepare_emb(datasets, args, mode='classification'):
    """
    Slide representation construction with patch feature aggregation trained in unsupervised manner
    """
   
    ### Preparing file path for saving embeddings
    print('\nConstructing unsupervised slide embedding...', end=' ')
    embeddings_kwargs = {
        'feats': args.data_source[0].split('/')[-2],
        'model_type': args.model_type,
        'out_size': args.n_proto
    }

    # Create embedding path
    fpath = "{feats}_{model_type}_embeddings_proto_{out_size}".format(**embeddings_kwargs)
    if args.model_type == 'PANTHER':
        DIEM_kwargs = {'tau': args.tau, 'out_type': args.out_type, 'eps': args.ot_eps, 'em_step': args.em_iter}
        name = '_{out_type}_em_{em_step}_eps_{eps}_tau_{tau}'.format(**DIEM_kwargs)
        fpath += name
    elif args.model_type == 'OT':
        OTK_kwargs = {'out_type': args.out_type, 'eps': args.ot_eps}
        name = '_{out_type}_eps_{eps}'.format(**OTK_kwargs)
        fpath += name
    embeddings_fpath = j_(args.split_dir, 'embeddings', fpath+'.pkl')
    
    ### Load existing embeddings if already created
    if os.path.isfile(embeddings_fpath):
        embeddings = load_pkl(embeddings_fpath)
        for k, loader in datasets.items():
            print(f'\n\tEmbedding already exists! Loading {k}', end=' ')
            loader.dataset.X, loader.dataset.y = embeddings[k]['X'], embeddings[k]['y']
    else:
        os.makedirs(j_(args.split_dir, 'embeddings'), exist_ok=True)
        
        model = create_embedding_model(args, mode=mode).to(device)

        ### Extracts prototypical features per split
        embeddings = {}
        for split, loader in datasets.items():
            print(f"\nAggregating {split} set features...")
            X, y = model.predict(loader,
                                 use_cuda=torch.cuda.is_available()
                                 )
            loader.dataset.X, loader.dataset.y = X, y
            embeddings[split] = {'X': X, 'y': y}
        save_pkl(embeddings_fpath, embeddings)

    return datasets, embeddings_fpath

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_PANTHER-checkpoint.py
# Model initiation for PANTHER

from torch import nn
import numpy as np

from .components import predict_surv, predict_clf, predict_emb
from .PANTHER.layers import PANTHERBase
from utils.proto_utils import check_prototypes


class PANTHER(nn.Module):
    """
    Wrapper for PANTHER model
    """
    def __init__(self, config, mode):
        super(PANTHER, self).__init__()

        self.config = config
        emb_dim = config.in_dim

        self.emb_dim = emb_dim
        self.heads = config.heads
        self.outsize = config.out_size
        self.load_proto = config.load_proto
        self.mode = mode

        check_prototypes(config.out_size, self.emb_dim, self.load_proto, config.proto_path)
        # This module contains the EM step
        self.panther = PANTHERBase(self.emb_dim, p=config.out_size, L=config.em_iter,
                         tau=config.tau, out=config.out_type, ot_eps=config.ot_eps,
                         load_proto=config.load_proto, proto_path=config.proto_path,
                         fix_proto=config.fix_proto)

    def representation(self, x):
        """
        Construct unsupervised slide representation
        """
        out, qqs = self.panther(x)
        return {'repr': out, 'qq': qqs}

    def forward(self, x):
        out = self.representation(x)
        return out['repr']
    
    def predict(self, data_loader, use_cuda=True):
        if self.mode == 'classification':
            output, y = predict_clf(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'survival':
            output, y = predict_surv(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'emb':
            output = predict_emb(self, data_loader.dataset, use_cuda=use_cuda)
            y = None
        else:
            raise NotImplementedError(f"Not implemented for {self.mode}!")
        
        return output, y

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_protocount-checkpoint.py
import torch
import torch.nn as nn
import os
import numpy as np
import pdb

from .components import predict_surv, predict_clf, predict_emb
from utils.file_utils import save_pkl, load_pkl

class ProtoCount(nn.Module):
    """
    WSI is represented as a prototype-count vector
    """
    def __init__(self, config, mode):
        super().__init__()

        assert config.load_proto, "Prototypes must be loaded!"
        assert os.path.exists(config.proto_path), "Path {} doesn't exist!".format(config.proto_path)

        self.config = config
        proto_path = config.proto_path

        if proto_path.endswith('pkl'):
            weights = load_pkl(proto_path)['prototypes'].squeeze()
        elif proto_path.endswith('npy'):
            weights = np.load(proto_path)

        self.n_proto = config.out_size
        self.prototypes = torch.from_numpy(weights).float()
        self.mode = mode

        emb_dim = config.in_dim

    def representation(self, x):
        """
        Compute the distance Eulcidean between prototypes and the patch features

        Args:
            x:

        Returns:

        """
        self.prototypes = self.prototypes.to(x.device)
        dist = torch.cdist(self.prototypes, x, p=2) # (1 x n_proto x n_instances)
        c_identity = torch.argmin(dist.squeeze(), dim=0) # (n_instances)
        counts = torch.bincount(c_identity, minlength=self.n_proto).unsqueeze(dim=0).float() # (1, n_proto)
        counts = counts / torch.norm(counts, dim=1)

        return {'repr': counts}

    def forward(self, x):
        out = self.representation(x)
        return out['repr']
    
    def predict(self, data_loader, use_cuda=True):
        if self.mode == 'classification':
            output, y = predict_clf(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'survival':
            output, y = predict_surv(self, data_loader.dataset, use_cuda=use_cuda)
        elif self.mode == 'emb':
            output = predict_emb(self, data_loader.dataset, use_cuda=use_cuda)
            y = None
        else:
            raise NotImplementedError(f"Not implemented for {self.mode}!")
        return output, y

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/mil_models/.ipynb_checkpoints/model_configs-checkpoint.py
from dataclasses import dataclass, asdict
from typing import Optional, Union, Callable
import logging
import json
import os
logger = logging.getLogger(__name__)


@dataclass
class PretrainedConfig:
    def to_json_file(self, json_file_path: Union[str, os.PathLike]):
        """
        Save this instance to a JSON file.
        Args:
            json_file_path: Path to the JSON file in which this configuration instance's parameters will be saved.
        """
        config_dict = {k: v for k, v in asdict(self).items()}
        with open(json_file_path, "w", encoding="utf-8") as writer:
            writer.write(json.dumps(
                config_dict, indent=2, sort_keys=False) + "\n")

    @classmethod
    def from_pretrained(cls, config_path: Union[str, os.PathLike], update_dict={}):
        config_dict = json.load(open(config_path))
        for key in update_dict:
            if key in config_dict:
                config_dict[key] = update_dict[key]
        config = cls(**config_dict)
        return config

    def save_pretrained(self, save_directory: Union[str, os.PathLike]):
        """
        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the
        [`~PretrainedConfig.from_pretrained`] class method.
        Args:
            save_directory (`str` or `os.PathLike`):
                Directory where the configuration JSON file will be saved (will be created if it does not exist).
        """
        if os.path.isfile(save_directory):
            raise AssertionError(
                f"Provided path ({save_directory}) should be a directory, not a file")

        os.makedirs(save_directory, exist_ok=True)

        # If we save using the predefined names, we can load using `from_pretrained`
        output_config_file = os.path.join(save_directory, "config.json")

        self.to_json_file(output_config_file)
        logger.info(f"Configuration saved in {output_config_file}")

@dataclass
class ABMILConfig(PretrainedConfig):
    gate: bool = True
    in_dim: int = 768
    n_classes: int = 2
    embed_dim: int = 512
    attn_dim: int = 384
    n_fc_layers: int = 1
    dropout: float = 0.25

@dataclass
class OTConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    n_filters: int = 2048
    len_motifs: int = 1
    subsamplings: int = 1
    kernel_args: int = 0.4
    weight_decay: float = 0.0001
    ot_eps: float = 3.0
    heads: int = 1
    out_size: int = 3
    out_type: str = 'param_cat'
    max_iter: int = 100
    distance: str = 'euclidean'
    fit_bias: bool = False
    alternating: bool = False
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True


@dataclass
class PANTHERConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    heads: int = 1
    em_iter: int = 3
    tau: float = 0.001
    embed_dim: int = 512
    ot_eps: int = 0.1
    n_fc_layers: int = 1
    dropout: float = 0.
    out_type: str = 'param_cat'
    out_size: int = 3
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True


@dataclass
class ProtoCountConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    out_size: int = 3
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True

@dataclass
class H2TConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    out_size: int = 3
    load_proto: bool = True
    proto_path: str = '.'
    fix_proto: bool = True

@dataclass
class LinearEmbConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2


@dataclass
class IndivMLPEmbConfig(PretrainedConfig):
    in_dim: int = 768
    n_classes: int = 2
    embed_dim: int = 128
    n_fc_layers: int = 2
    dropout: float = 0.25
    proto_model_type: str = 'DIEM'
    p: int = 32
    out_type: str = 'param_cat'

@dataclass
class IndivMLPEmbConfig_Shared(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = True
    indiv_mlps: bool = False
    postcat_mlp: bool = False
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32

@dataclass
class IndivMLPEmbConfig_Indiv(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = False
    indiv_mlps: bool = True
    postcat_mlp: bool = False
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32

@dataclass
class IndivMLPEmbConfig_SharedPost(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = True
    indiv_mlps: bool = False
    postcat_mlp: bool = True
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32

@dataclass
class IndivMLPEmbConfig_IndivPost(PretrainedConfig):
    in_dim: int = 2049
    n_classes: int = 4
    shared_embed_dim: int = 256
    indiv_embed_dim: int = 128
    postcat_embed_dim: int = 1024
    shared_mlp: bool = False
    indiv_mlps: bool = True
    postcat_mlp: bool = True
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 16
    use_snn: bool = False

@dataclass
class IndivMLPEmbConfig_SharedIndiv(PretrainedConfig):
    in_dim: int = 2049
    n_classes: int = 4
    shared_embed_dim: int = 256
    indiv_embed_dim: int = 128
    postcat_embed_dim: int = 1024
    shared_mlp: bool = True
    indiv_mlps: bool = True
    postcat_mlp: bool = False
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 16
    use_snn: bool = False

@dataclass
class IndivMLPEmbConfig_SharedIndivPost(PretrainedConfig):
    in_dim: int = 129
    n_classes: int = 4
    shared_embed_dim: int = 64
    indiv_embed_dim: int = 32
    postcat_embed_dim: int = 512
    shared_mlp: bool = True
    indiv_mlps: bool = True
    postcat_mlp: bool = True
    n_fc_layers: int = 1
    shared_dropout: float = 0.25
    indiv_dropout: float = 0.25
    postcat_dropout: float = 0.25
    p: int = 32


# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/data_factory/__init__.py
from .cls_default import tasks, label_dicts

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/data_factory/cls_default.py
tasks = ['mutation', 'BRS']
label_dicts = {
        'ebrains_subtyping_fine': {'Glioblastoma, IDH-wildtype': 0, 'Transitional meningioma': 1, 'Anaplastic meningioma': 2, 'Pituitary adenoma': 3, 'Oligodendroglioma, IDH-mutant and 1p/19q codeleted': 4, 'Haemangioma': 5, 'Ganglioglioma': 6, 'Schwannoma': 7, 'Anaplastic oligodendroglioma, IDH-mutant and 1p/19q codeleted': 8, 'Anaplastic astrocytoma, IDH-wildtype': 9, 'Pilocytic astrocytoma': 10, 'Angiomatous meningioma': 11, 'Haemangioblastoma': 12, 'Gliosarcoma': 13, 'Adamantinomatous craniopharyngioma': 14, 'Anaplastic astrocytoma, IDH-mutant': 15, 'Ependymoma': 16, 'Anaplastic ependymoma': 17, 'Glioblastoma, IDH-mutant': 18, 'Atypical meningioma': 19, 'Metastatic tumours': 20, 'Meningothelial meningioma': 21, 'Langerhans cell histiocytosis': 22, 'Diffuse large B-cell lymphoma of the CNS': 23, 'Diffuse astrocytoma, IDH-mutant': 24, 'Secretory meningioma': 25, 'Haemangiopericytoma': 26, 'Fibrous meningioma': 27, 'Lipoma': 28, 'Medulloblastoma, non-WNT/non-SHH': 29},
        'ebrains_subtyping_coarse': {'Adult-type diffuse gliomas': 0, 'Meningiomas': 1, 'Tumours of the sellar region': 2, 'Mesenchymal, non-meningothelial tumours involving the CNS': 3, 'Glioneuronal and neuronal tumours': 4, 'Cranial and paraspinal nerve tumours': 5, 'Circumscribed astrocytic gliomas ': 6, 'Ependymal Tumours': 7, 'Metastatic tumours': 8, 'Haematolymphoid tumours involving the CNS': 9, 'Paediatric-type diffuse low-grade gliomas ': 10, 'Embryonal Tumors': 11},        
        'panda': {0:0,1:1,2:2,3:3,4:4,5:5}, 'mutation': {0:0,1:1},

}

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/data_factory/.ipynb_checkpoints/__init__-checkpoint.py
from .cls_default import tasks, label_dicts

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/Aggregators/data_factory/.ipynb_checkpoints/cls_default-checkpoint.py
tasks = ['mutation', 'BRS']
label_dicts = {
        'ebrains_subtyping_fine': {'Glioblastoma, IDH-wildtype': 0, 'Transitional meningioma': 1, 'Anaplastic meningioma': 2, 'Pituitary adenoma': 3, 'Oligodendroglioma, IDH-mutant and 1p/19q codeleted': 4, 'Haemangioma': 5, 'Ganglioglioma': 6, 'Schwannoma': 7, 'Anaplastic oligodendroglioma, IDH-mutant and 1p/19q codeleted': 8, 'Anaplastic astrocytoma, IDH-wildtype': 9, 'Pilocytic astrocytoma': 10, 'Angiomatous meningioma': 11, 'Haemangioblastoma': 12, 'Gliosarcoma': 13, 'Adamantinomatous craniopharyngioma': 14, 'Anaplastic astrocytoma, IDH-mutant': 15, 'Ependymoma': 16, 'Anaplastic ependymoma': 17, 'Glioblastoma, IDH-mutant': 18, 'Atypical meningioma': 19, 'Metastatic tumours': 20, 'Meningothelial meningioma': 21, 'Langerhans cell histiocytosis': 22, 'Diffuse large B-cell lymphoma of the CNS': 23, 'Diffuse astrocytoma, IDH-mutant': 24, 'Secretory meningioma': 25, 'Haemangiopericytoma': 26, 'Fibrous meningioma': 27, 'Lipoma': 28, 'Medulloblastoma, non-WNT/non-SHH': 29},
        'ebrains_subtyping_coarse': {'Adult-type diffuse gliomas': 0, 'Meningiomas': 1, 'Tumours of the sellar region': 2, 'Mesenchymal, non-meningothelial tumours involving the CNS': 3, 'Glioneuronal and neuronal tumours': 4, 'Cranial and paraspinal nerve tumours': 5, 'Circumscribed astrocytic gliomas ': 6, 'Ependymal Tumours': 7, 'Metastatic tumours': 8, 'Haematolymphoid tumours involving the CNS': 9, 'Paediatric-type diffuse low-grade gliomas ': 10, 'Embryonal Tumors': 11},        
        'panda': {0:0,1:1,2:2,3:3,4:4,5:5}, 'mutation': {0:0,1:1},

}

# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/evaluation/f1score.py

"""
The following is a simple example evaluation method.

It is meant to run within a container.

To run it locally, you can call the following bash script:

  ./test_run.sh

This will start the evaluation, reads from ./test/input and outputs to ./test/output

To save the container and prep it for upload to Grand-Challenge.org you can call:

  ./save.sh

Any container that shows the same behavior will do, this is purely an example of how one COULD do it.

Happy programming!
"""
import json
from glob import glob
from multiprocessing import Pool
from pathlib import Path
from pprint import pformat, pprint

import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, roc_auc_score, roc_curve

INPUT_DIRECTORY = Path("/input")
OUTPUT_DIRECTORY = Path("/output")
GROUND_TRUTH_DIRECTORY = Path("ground_truth")

def main():
    print_inputs()

    metrics = {}

    result_df = pd.DataFrame(columns=["case_id", "case_id_gt", "case_id_pred"])
    predictions = read_predictions()

    with Pool(processes=4) as pool:
        metrics["results"] = pool.map(process, predictions)
    
    result_df = pd.DataFrame(metrics["results"])
    
    y_true = result_df["case_id_gt"].astype(int).values
    y_prob = result_df["case_id_pred"].astype(float).values
    y_pred = (y_prob > 0.5).astype(int)

    f1 = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_prob)

    metrics["aggregates"] = {
        "f1_score": f1,
        "auc": auc
    }

    write_metrics(metrics=metrics)

    return 0

def process(job):
    report = "Processing:\n"
    report += pformat(job) + "\n"

    location_prediction = get_file_location(
        job_pk=job["pk"],
        values=job["outputs"],
        slug="brs3-probability",
    )

    pred_result = load_json_file(location=location_prediction)

    image_name = get_image_name(
        values=job["inputs"],
        slug="prostatectomy-tissue-whole-slide-image",
    )

    ground_truth_df = pd.read_csv(GROUND_TRUTH_DIRECTORY / "ground_truth.csv")
    gt_row = ground_truth_df[ground_truth_df["case_id"] == image_name]

    y_true = gt_row["brs3"].item()
    y_pred = pred_result

    return {
        "case_id": image_name,
        "case_id_gt": y_true,
        "case_id_pred": y_pred
    }

def print_inputs():
    input_files = [str(x) for x in Path(INPUT_DIRECTORY).rglob("*") if x.is_file()]
    print("Input Files:")
    pprint(input_files)
    print("")

def read_predictions():
    with open(INPUT_DIRECTORY / "predictions.json") as f:
        return json.loads(f.read())

def get_image_name(*, values, slug):
    for value in values:
        if value["interface"]["slug"] == slug:
            return value["image"]["name"]
    raise RuntimeError(f"Image with interface {slug} not found!")

def get_interface_relative_path(*, values, slug):
    for value in values:
        if value["interface"]["slug"] == slug:
            return value["interface"]["relative_path"]
    raise RuntimeError(f"Value with interface {slug} not found!")

def get_file_location(*, job_pk, values, slug):
    relative_path = get_interface_relative_path(values=values, slug=slug)
    return INPUT_DIRECTORY / job_pk / "output" / relative_path

def load_json_file(*, location):
    with open(location) as f:
        return json.loads(f.read())

def write_metrics(*, metrics):
    with open(OUTPUT_DIRECTORY / "metrics.json", "w") as f:
        f.write(json.dumps(metrics, indent=4))

if __name__ == "__main__":
    raise SystemExit(main())



# /Users/robertspaans/Documents/Projects/phd_projects/multimodal_working_group/MICCAI2025/MICCAI2025_models/CHIMERA/task1_baseline/evaluation/c-index.py


"""
The following is a simple example evaluation method.

It is meant to run within a container.

To run it locally, you can call the following bash script:

  ./test_run.sh

This will start the evaluation, reads from ./test/input and outputs to ./test/output

To save the container and prep it for upload to Grand-Challenge.org you can call:

  ./save.sh

Any container that shows the same behavior will do, this is purely an example of how one COULD do it.

Happy programming!
"""
import json
from glob import glob
import random
from multiprocessing import Pool
from statistics import mean
from pathlib import Path
from pprint import pformat, pprint

import pandas as pd
import numpy as np
from sksurv.metrics import concordance_index_censored


INPUT_DIRECTORY = Path("/input")
OUTPUT_DIRECTORY = Path("/output")
GROUND_TRUTH_DIRECTORY = Path("ground_truth")

#INPUT_DIRECTORY = Path("test/input")
#OUTPUT_DIRECTORY = Path("test/output")
#GROUND_TRUTH_DIRECTORY = Path("ground_truth")


def main():
    print_inputs()

    metrics = {}


    result_df = pd.DataFrame(columns=["case_id", "case_id_gt_time", "case_id_gt_event","case_id_prediction_years_to_recurrence"])
    predictions = read_predictions()



    # We now process each algorithm job for this submission
    # Note that the jobs are not in any order!
    # We work that out from predictions.json

    # Start a number of process workers, using multiprocessing
    # The optimal number of workers ultimately depends on how many
    # resources each process() would call upon
    with Pool(processes=4) as pool:
        metrics["results"] = pool.map(process, predictions)
    
    result_df = pd.DataFrame(metrics["results"])
    
    survival_times = np.array(result_df["case_id_gt_time"])
    events = np.array(result_df["case_id_gt_event"], dtype=bool)
    predicted_times = np.array(result_df["case_id_prediction_years_to_recurrence"])
    #print("events, survival_times, predicted_times",events, survival_times, predicted_times)
  
    # Negating the predicted times to convert them into risk scores before calculating the concordance index. 
    # This ensures the concordance index correctly interprets higher scores as lower risk.
    c_index = concordance_index_censored(events, survival_times, -predicted_times)

    print("c_index",c_index)
    



    # Now generate an overall score(s) for this submission

    # here use c-indexx function
    metrics["aggregates"] = {
        "c_index": c_index[0]
    }

    # Make sure to save the metrics
    write_metrics(metrics=metrics)

    return 0


def process(job):
    # Processes a single algorithm job, looking at the outputs
    report = "Processing:\n"
    report += pformat(job)
    report += "\n"


    # Firstly, find the location of the results
    location_overall_survival_years = get_file_location(
            job_pk=job["pk"],
            values=job["outputs"],
            slug="overall-survival-years",
        )
    

    # Secondly, read the results
    result_overall_survival_years = load_json_file(
        location=location_overall_survival_years,
    )
    
    #print('result_overall_survival_years',result_overall_survival_years)

    # Thirdly, retrieve the input image name to match it with an image in your ground truth
    image_name_prostatectomy_tissue_whole_slide_image = get_image_name(
            values=job["inputs"],
            slug="prostatectomy-tissue-whole-slide-image",
    )

    #print('image_name_prostatectomy_tissue_whole_slide_image',image_name_prostatectomy_tissue_whole_slide_image)

    # Fourthly, your load your ground truth
    # Include it in your evaluation container by placing it in ground_truth/
    ground_truth_df =  pd.read_csv(GROUND_TRUTH_DIRECTORY / "ground_truth.csv")

    gt_image_name_df = ground_truth_df[ground_truth_df["case_id"]==image_name_prostatectomy_tissue_whole_slide_image]

    

    gt_image_name_time = gt_image_name_df["follow_up_years"].item()

    print('gt_image_name_time',gt_image_name_time)

    gt_image_name_event = gt_image_name_df["event"].item()

    print('gt_image_name_event',gt_image_name_event)


    

    


    # Finally, calculate by comparing the ground truth to the actual results
    return {
        "case_id": image_name_prostatectomy_tissue_whole_slide_image,
        "case_id_gt_time": gt_image_name_time, 
        "case_id_gt_event": gt_image_name_event,
        "case_id_prediction_years_to_recurrence": result_overall_survival_years
    }


def print_inputs():
    # Just for convenience, in the logs you can then see what files you have to work with
    input_files = [str(x) for x in Path(INPUT_DIRECTORY).rglob("*") if x.is_file()]

    print("Input Files:")
    pprint(input_files)
    print("")


def read_predictions():
    # The prediction file tells us the location of the users' predictions
    with open(INPUT_DIRECTORY / "predictions.json") as f:
        return json.loads(f.read())


def get_image_name(*, values, slug):
    # This tells us the user-provided name of the input or output image
    for value in values:
        if value["interface"]["slug"] == slug:
            return value["image"]["name"]

    raise RuntimeError(f"Image with interface {slug} not found!")


def get_interface_relative_path(*, values, slug):
    # Gets the location of the interface relative to the input or output
    for value in values:
        if value["interface"]["slug"] == slug:
            return value["interface"]["relative_path"]

    raise RuntimeError(f"Value with interface {slug} not found!")


def get_file_location(*, job_pk, values, slug):
    # Where a job's output file will be located in the evaluation container
    relative_path = get_interface_relative_path(values=values, slug=slug)
    return INPUT_DIRECTORY / job_pk / "output" / relative_path


def load_json_file(*, location):
    # Reads a json file
    with open(location) as f:
        return json.loads(f.read())


def write_metrics(*, metrics):
    # Write a json document used for ranking results on the leaderboard
    with open(OUTPUT_DIRECTORY / "metrics.json", "w") as f:
        f.write(json.dumps(metrics, indent=4))


if __name__ == "__main__":
    raise SystemExit(main())


