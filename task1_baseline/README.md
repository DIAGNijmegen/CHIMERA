# CHIMERA Task 1 Baseline: Prostate Cancer Biochemical Recurrence Prediction

This package provides the baseline model for **Task 1** of the [CHIMERA Challenge](https://chimera.grand-challenge.org/). The goal of this task is to **predict biochemical recurrence (BCR) in prostate cancer** using a multimodal approach that integrates Whole Slide Images (WSI), Magnetic Resonance Imaging (MRI), and clinical data.

This implementation serves as a robust starting point for participants, providing a complete, runnable pipeline from data processing to prediction.

---

## üî¨ Pipeline Architecture

The baseline model employs a two-stage feature extraction process followed by a multimodal aggregation model:

1.  **Pathology Feature Extraction**:
    -   For training, features are pre-extracted from WSIs using the `UNI` model via the `slide2vec` library.
    -   For inference, the logic in `common/src/features/pathology/` is used to process slides on-the-fly.

2.  **Radiology Feature Extraction**:
    -   Employs a pre-trained `nnU-Net v1` model ensemble to extract deep features from multimodal MRI scans (T2w, ADC, HBV).
    -   The core logic for both training and inference is located in `common/src/features/radiology/`.

3.  **Multimodal Prediction**:
    -   The extracted pathology and radiology features are fed into an **Attention-Based Multiple Instance Learning (ABMIL)** model.
    -   This aggregator model, located in `prediction_model/Aggregators/`, combines the features to produce the final BCR prediction.

---

## üöÄ Getting Started

**System requirements:**
-   Linux-based OS (e.g., Ubuntu 22.04)
-   Python 3.10+
-   Docker installed

### 1. Clone the Repository

If you haven't already, clone the challenge repository:
```bash
git clone https://github.com/your-org/CHIMERA.git
cd CHIMERA/task1_baseline
```

### 2. Download Feature Extractor Weights

The pre-trained weights for the feature extractors are not included in this repository. You must download them separately from the links provided in the main `README.md` of this repository.

-   **Pathology (UNI) Weights**: Weights for the WSI feature extractor.
-   **Radiology (nnU-Net) Weights**: Weights for the MRI feature extractor.

The weights for the final prediction model (ABMIL) are not downloaded but are **generated by you** by following the training workflow outlined below.

### 3. Set Up the Model Directory

The inference pipeline expects the pre-trained model weights to be organized in a specific structure within the `common/model/` directory. This directory is included in the `.gitignore`, so you will need to create it yourself.

After downloading the feature extractor weights and training your own prediction model, arrange the files as follows:

```
CHIMERA/
‚îî‚îÄ‚îÄ common/
    ‚îî‚îÄ‚îÄ model/
        ‚îú‚îÄ‚îÄ pathology/
        ‚îÇ   ‚îú‚îÄ‚îÄ model.bin
        ‚îÇ   ‚îî‚îÄ‚îÄ config.json
        ‚îÇ
        ‚îú‚îÄ‚îÄ radiology/
        ‚îÇ   ‚îú‚îÄ‚îÄ fold_0/
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_best.model
        ‚îÇ   ‚îú‚îÄ‚îÄ fold_1/
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_best.model
        ‚îÇ   ‚îú‚îÄ‚îÄ ... (and so on for all folds)
        ‚îÇ   ‚îî‚îÄ‚îÄ plans.pkl
        ‚îÇ
        ‚îî‚îÄ‚îÄ ABMIL_task1/
            ‚îú‚îÄ‚îÄ s_checkpoint.pth
            ‚îú‚îÄ‚îÄ clinical_processor.pkl
            ‚îú‚îÄ‚îÄ calibration_data.pkl
            ‚îî‚îÄ‚îÄ config.json
```

-   **`common/model/pathology/`**: Contains the weights for the UNI pathology feature extractor. You need to download the `model.bin` and `config.json` files from the [UNI Hugging Face repository](https://huggingface.co/MahmoodLab/UNI/tree/main) and place them here.
-   **`common/model/radiology/`**: Contains the nnU-Net weights. Download the `plans.pkl` file and the `model_best.model` file from each of the `fold_*` directories from the [PI-CAI nnU-Net GitHub repository](https://github.com/DIAGNijmegen/picai_nnunet_semi_supervised_gc_algorithm/tree/master/results/nnUNet/3d_fullres/Task2203_picai_baseline/nnUNetTrainerV2_Loss_FL_and_CE_checkpoints__nnUNetPlansv2.1) and place them in the corresponding folders.
-   **`common/model/ABMIL_task1/`**: Contains the assets for the final prediction model, which are the **output of the training process**. This includes the model checkpoint (`s_checkpoint.pth`), the fitted clinical data processor (`clinical_processor.pkl`), and the model configuration (`config.json`). It also requires a calibration file named `calibration_data.pkl`. The `create_calibration_data.py` script generates two options: `linear_event_calibration.pkl` and `exponential_event_calibration.pkl`. You must choose one of these files and rename it to `calibration_data.pkl`.

The `do_test_run.sh` script automatically mounts this `common/model` directory into the Docker container at `/opt/ml/model/`, which is where the inference script looks for the weights.

---

## ‚öôÔ∏è Running the Baseline

To run the baseline model locally:

1.  **Build the Docker Container**:
    This command builds the Docker image with all necessary dependencies specified in `requirements.txt`.
    ```bash
    ./do_build.sh
    ```

2.  **Run the Test Script**:
    This script executes the full inference pipeline on sample data. It mounts the necessary input, output, and model directories and runs the main inference logic.
    ```bash
    ./do_test_run.sh
    ```

---

## üõ†Ô∏è Model Training and Inference Workflow

This repository is structured to support both training new models and running inference with existing ones. The core logic is split between two directories:
-   `prediction_model/Aggregators/training/`: Contains scripts for training the survival model.
-   `prediction_model/Aggregators/inference/`: Contains scripts for running prediction on new data.

### Prerequisite: Feature Extraction

Before training the prediction model, you must first extract features from the raw WSI and MRI data.

#### Pathology Feature Extraction (WSI)

Pathology features are extracted from Whole Slide Images using the `slide2vec` library, which leverages the `UNI` model.

1.  **Clone the `slide2vec` repository:**
    ```bash
    git clone https://github.com/clemsgrs/slide2vec.git
    cd slide2vec
    pip install -r requirements.txt
    ```

2.  **Run feature extraction:**
    You will need to run the feature extraction script from the `slide2vec` repository on your WSI dataset. The output should be a directory containing one `.pt` file per slide.

#### Radiology Feature Extraction (MRI)

Radiology features are extracted using the `stand_alone_feature_extractor.py` script provided in this repository. This script is designed to work with the downloaded training data.

```bash
python common/src/features/radiology/stand_alone_feature_extractor.py \
  --input_dir /path/to/your/mri_scans/ \
  --output_dir /path/to/your/radiology_features/ \
  --model_dir common/model/radiology/
```


### Training a New Model

You can train the ABMIL model from scratch using your own data. The process involves several steps, from data preparation to model calibration. For local training, use `main_survival.py`. For hyperparameter optimization, use `main_survival_sweep.py` with Weights & Biases.

Here is a typical workflow:

1.  **Create Data Splits (5-fold Cross-Validation)**
    ```bash
    python3 prediction_model/Aggregators/training/bin_kfold_CSV.py \
      --clin_dat_path /path/to/clinical_data \
      --feat_path /path/to/pathology/features \
      --output_dir /path/to/output/folds
    ```

2.  **Train with 5-Fold Cross-Validation**
    Run this command for each fold (e.g., `fold_0` to `fold_4`):
    ```bash
    python3 prediction_model/Aggregators/training/main_survival.py \
      --split_dir /path/to/output/folds/fold_0 \
      --data_source /path/to/pathology/features \
      --mri_feature_path /path/to/radiology/features \
      --clinical_data_path /path/to/clinical_data \
      --results_dir /path/to/results/fold_0 \
      --max_epochs 20
    ```

3.  **(Optional) Summarize Fold Statistics**
    ```bash
    python3 prediction_model/Aggregators/training/summarize_folds.py /path/to/results
    ```

4.  **(Optional) Hyperparameter Sweep with W&B**
    ```bash
    python3 prediction_model/Aggregators/training/main_survival_sweep.py \
      --split_dir /path/to/output/folds/fold_0 \
      --data_source /path/to/pathology/features \
      --mri_feature_path /path/to/radiology/features \
      --clinical_data_path /path/to/clinical_data \
      --results_dir /path/to/sweep_results \
      --project_name "CHIMERA_Task1_Sweep" \
      --run_count 20
    ```

5.  **Train Final Model on Full Dataset**
    After identifying the best hyperparameters, train a final model on all your data.

6.  **Create Clinical Data Processor**
    This saves the scaling and encoding rules from your training data.
    ```bash
    python3 prediction_model/Aggregators/training/create_clinical_processor.py \
      --clinical_data_dir /path/to/clinical_data \
      --output_dir /path/to/final_model_assets
    ```

7.  **Calibrate the Final Model**
    This step creates a calibration file to convert model risk scores into time-to-event predictions.
    ```bash
    python3 prediction_model/Aggregators/training/create_calibration_data.py \
      --pathology_features_dir /path/to/pathology/features \
      --radiology_features_dir /path/to/radiology/features \
      --clinical_data_dir /path/to/clinical_data \
      --model_dir /path/to/your/final_model_directory \
      --output_dir /path/to/final_model_assets
    ```

### Running Inference

Once you have a trained model and its associated assets (`s_checkpoint.pth`, `clinical_processor.pkl`, `calibration_data.pkl`), you can run inference.

```bash
python3 prediction_model/Aggregators/inference/inference.py \
  --input_dir /path/to/test/input \
  --output_dir /path/to/inference/output \
  --model_dir /path/to/packaged_model_weights
```

## üìÑ License

This project is licensed under the Apache License 2.0. See the `LICENSE` file for details.