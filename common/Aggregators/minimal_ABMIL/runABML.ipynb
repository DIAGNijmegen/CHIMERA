{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f060d071-7627-414e-b07a-5584ba0e22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting POT==0.4.0\n",
      "  Downloading POT-0.4.0.tar.gz (315 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m925.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from POT==0.4.0) (1.23.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from POT==0.4.0) (1.9.1)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.9/site-packages (from POT==0.4.0) (0.29.34)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/site-packages (from POT==0.4.0) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/site-packages (from matplotlib->POT==0.4.0) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->POT==0.4.0) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->POT==0.4.0) (1.16.0)\n",
      "Building wheels for collected packages: POT\n",
      "  Building wheel for POT (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for POT: filename=POT-0.4.0-cp39-cp39-linux_x86_64.whl size=294761 sha256=1fbfbfab9bbe92c4760e922e78ce59ddbf1899b27aa0aa9d4bfc9830049515a5\n",
      "  Stored in directory: /home/nadiehkhalili/.cache/pip/wheels/7b/2c/3b/c3b1cb10fd1c98bda6416a1c5cb8c7e5cb5f331bce94f70667\n",
      "Successfully built POT\n",
      "Installing collected packages: POT\n",
      "Successfully installed POT-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install POT==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079aced6-1f80-4850-bc32-4550a7d9d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the static variables\n",
    "data_source = r\"/data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\"\n",
    "task = \"mutation\"\n",
    "batch_size = 4\n",
    "results_dir = \"/data/temporary/nadieh/mutation_prediction/results\"\n",
    "split_names = \"train,tune,test\"\n",
    "in_dim = 1024\n",
    "epoch = 13\n",
    "model_configs = [ \"ABMIL_default\"]\n",
    "label = 'TP53'\n",
    "# Loop over k from 1 to 5\n",
    "for model_config in model_configs:\n",
    "    model_type = \"ABMIL\"\n",
    "    for k in range(0, 3):\n",
    "        split_dir = f\"/data/temporary/nadieh/mutation_prediction/3-folds-correct-only-prostate/fold-{k}\"\n",
    "        # Build the command\n",
    "        cmd = [\n",
    "            'python3', 'training/main_classification.py',\n",
    "            '--data_source', data_source,\n",
    "            '--split_dir', split_dir,\n",
    "            '--task', task,\n",
    "            '--batch_size', str(batch_size),\n",
    "            '--results_dir', results_dir,\n",
    "            '--split_names', split_names,\n",
    "            '--in_dim', str(in_dim),\n",
    "            '--model_config', model_config,\n",
    "            '--target_col', label,\n",
    "            '--model_type', model_type,\n",
    "            '--max_epochs', str(epoch),\n",
    "\n",
    "        ]\n",
    "        # Execute the command\n",
    "        subprocess.run(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103151d-6366-45c6-90a0-176763d9fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map:  {0: 0, 1: 1}\n",
      "task:  mutation\n",
      "split_dir:  /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-2\n",
      ".\n",
      "data source:  /data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\n",
      "feats_pt\n",
      "  feats_name  mag  patch_size\n",
      "0   feats_pt   20         256\n",
      "All data sources have the same feature extraction parameters.\n",
      "\n",
      "################### Settings ###################\n",
      "max_epochs:  15\n",
      "lr:  0.0001\n",
      "wd:  1e-05\n",
      "accum_steps:  1\n",
      "opt:  adamW\n",
      "lr_scheduler:  constant\n",
      "warmup_steps:  -1\n",
      "warmup_epochs:  -1\n",
      "batch_size:  4\n",
      "print_every:  100\n",
      "seed:  1\n",
      "num_workers:  2\n",
      "early_stopping:  False\n",
      "es_min_epochs:  15\n",
      "es_patience:  10\n",
      "es_metric:  loss\n",
      "model_type:  ABMIL\n",
      "emb_model_type:  LinEmb_LR\n",
      "ot_eps:  0.1\n",
      "model_config:  ABMIL_default\n",
      "in_dim:  1024\n",
      "in_dropout:  0.0\n",
      "bag_size:  -1\n",
      "train_bag_size:  -1\n",
      "val_bag_size:  -1\n",
      "train_sampler:  random\n",
      "n_fc_layers:  None\n",
      "em_iter:  None\n",
      "tau:  None\n",
      "out_type:  param_cat\n",
      "load_proto:  False\n",
      "proto_path:  .\n",
      "fix_proto:  False\n",
      "n_proto:  None\n",
      "exp_code:  None\n",
      "task:  mutation\n",
      "loss_fn:  None\n",
      "target_col:  TP53\n",
      "data_source:  ['/data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt']\n",
      "split_dir:  /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-2\n",
      "split_names:  train,tune,test\n",
      "overwrite:  False\n",
      "results_dir:  /data/temporary/nadieh/mutation_prediction/results/mutation/k=0/temporary::ABMIL_default::feats_pt/temporary::ABMIL_default::feats_pt::25-05-02-13-28-41\n",
      "tags:  None\n",
      "label_map:  {0: 0, 1: 1}\n",
      "n_classes:  2\n",
      "split_name_clean:  temporary\n",
      "split_k:  0\n",
      "pretrain_enc:  feats_pt\n",
      "pretrain_algo:  \n",
      "pretrain_exp:  \n",
      "pretrain_ckpt:  -1\n",
      "inference_prec:  fp32\n",
      "patch_mag:  20\n",
      "patch_size:  256\n",
      "feat_names:  ['feats_pt']\n",
      "Using the following split names: ['train', 'tune', 'test']\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-2/train.csv\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-2/tune.csv\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-2/test.csv\n",
      "{'train':            case_id  TP53                                           slide_id\n",
      "0     TCGA-KK-A8IA     0  TCGA-KK-A8IA-01Z-00-DX1.4396AFCE-0AE2-4066-BB7...\n",
      "1     TCGA-Y6-A9XI     0  TCGA-Y6-A9XI-01Z-00-DX6.1AA896B6-F5CA-4394-BDC...\n",
      "2     TCGA-Y6-A9XI     0  TCGA-Y6-A9XI-01Z-00-DX2.43D8478F-EE66-414A-81B...\n",
      "3     TCGA-Y6-A9XI     0  TCGA-Y6-A9XI-01Z-00-DX3.167452BB-33BA-4C13-979...\n",
      "4     TCGA-Y6-A9XI     0  TCGA-Y6-A9XI-01Z-00-DX1.BCEC7405-546D-4CF3-90F...\n",
      "...            ...   ...                                                ...\n",
      "1584  TCGA-ZF-A9R1     0  TCGA-ZF-A9R1-01Z-00-DX1.74520759-D448-45B5-B8A...\n",
      "1585  TCGA-CF-A3MI     0  TCGA-CF-A3MI-01Z-00-DX1.5619CA1A-F772-44CC-BBE...\n",
      "1586  TCGA-DK-A1AF     0  TCGA-DK-A1AF-01Z-00-DX1.E18DE029-27A6-43A6-9E9...\n",
      "1587  TCGA-BT-A20P     0  TCGA-BT-A20P-01Z-00-DX1.721A7D6B-C9F3-4A70-8E8...\n",
      "1588  TCGA-GV-A3QG     0  TCGA-GV-A3QG-01Z-00-DX1.4A550B20-2A4C-48E5-BFE...\n",
      "\n",
      "[1589 rows x 3 columns], 'tune':          case_id  TP53                                           slide_id\n",
      "0   TCGA-XJ-A9DI     0  TCGA-XJ-A9DI-01Z-00-DX1.0A4BD633-9872-45EC-981...\n",
      "1   TCGA-KK-A59V     0  TCGA-KK-A59V-01Z-00-DX1.181D78A7-2B0C-46BC-AF1...\n",
      "2   TCGA-CH-5769     0  TCGA-CH-5769-01Z-00-DX1.cd4fbd94-7698-43e6-92e...\n",
      "3   TCGA-HC-A6AO     0  TCGA-HC-A6AO-01Z-00-DX1.826FFDEE-62D9-4946-906...\n",
      "4   TCGA-G9-6333     0  TCGA-G9-6333-01Z-00-DX1.930417fd-52d1-4993-938...\n",
      "5   TCGA-KK-A6E0     0  TCGA-KK-A6E0-01Z-00-DX1.F69CA949-C533-4D94-AB1...\n",
      "6   TCGA-VP-A87B     0  TCGA-VP-A87B-01Z-00-DX1.E11AE7F4-676D-4A0E-A18...\n",
      "7   TCGA-J4-A6G1     0  TCGA-J4-A6G1-01Z-00-DX1.CB849D80-60EA-407B-87A...\n",
      "8   TCGA-YL-A8SQ     0  TCGA-YL-A8SQ-01Z-00-DX1.0DFEF9A0-9C25-46F9-8EB...\n",
      "9   TCGA-EJ-A46G     0  TCGA-EJ-A46G-01Z-00-DX1.F6443830-54BB-44D8-91F...\n",
      "10  TCGA-HC-7737     0  TCGA-HC-7737-01Z-00-DX1.44947b83-a5b8-43f7-b0a...\n",
      "11  TCGA-G9-6385     0  TCGA-G9-6385-01Z-00-DX1.e75b4a90-f29f-4e80-a0a...\n",
      "12  TCGA-G9-6379     0  TCGA-G9-6379-01Z-00-DX1.7EE265D9-72A6-4019-924...\n",
      "13  TCGA-2A-A8W1     0  TCGA-2A-A8W1-01Z-00-DX1.0CA35481-A243-4722-B3E...\n",
      "14  TCGA-HC-7209     0  TCGA-HC-7209-01Z-00-DX1.ccf1d3c8-0351-4b7e-853...\n",
      "15  TCGA-HC-7081     0  TCGA-HC-7081-01Z-00-DX1.d4118273-e95d-41ed-915...\n",
      "16  TCGA-2A-AAYO     0  TCGA-2A-AAYO-01Z-00-DX1.A5BB3437-956A-4A42-AC2...\n",
      "17  TCGA-2A-A8VO     0  TCGA-2A-A8VO-01Z-00-DX1.3A69CC37-B066-4529-B1B...\n",
      "18  TCGA-EJ-5522     0  TCGA-EJ-5522-01Z-00-DX1.0358ea1e-8123-4c9a-885...\n",
      "19  TCGA-J4-A6G3     0  TCGA-J4-A6G3-01Z-00-DX1.BDCBE64A-B59F-4735-B9B...\n",
      "20  TCGA-EJ-5495     0  TCGA-EJ-5495-01Z-00-DX1.0db6207b-4cc6-463c-8b9...\n",
      "21  TCGA-V1-A9OQ     0  TCGA-V1-A9OQ-01Z-00-DX1.6B57FA2E-B1D8-4CA4-B0E...\n",
      "22  TCGA-SU-A7E7     0  TCGA-SU-A7E7-01Z-00-DX1.1C4CB02E-100C-4A40-8C9...\n",
      "23  TCGA-VP-A878     0  TCGA-VP-A878-01Z-00-DX1.9FED3449-AC9E-4C21-BCA...\n",
      "24  TCGA-HC-7233     0  TCGA-HC-7233-01Z-00-DX1.f27b1b5d-4a74-4fa9-95b...\n",
      "25  TCGA-YL-A9WH     1  TCGA-YL-A9WH-01Z-00-DX1.7240AB29-F661-4456-A68...\n",
      "26  TCGA-G9-6363     0  TCGA-G9-6363-01Z-00-DX1.390dc5bd-9d79-459c-825...\n",
      "27  TCGA-2A-A8VV     0  TCGA-2A-A8VV-01Z-00-DX1.DD094519-863E-42EF-8B3...\n",
      "28  TCGA-KK-A6E8     0  TCGA-KK-A6E8-01Z-00-DX1.5A8673FF-44A5-4383-86E...\n",
      "29  TCGA-EJ-5525     1  TCGA-EJ-5525-01Z-00-DX1.7baf2b11-607e-4001-b5a...\n",
      "30  TCGA-TP-A8TV     0  TCGA-TP-A8TV-01Z-00-DX1.16CEB4D6-1E14-4211-A9E...\n",
      "31  TCGA-G9-6494     0  TCGA-G9-6494-01Z-00-DX1.023d173a-85dc-4814-9c1...\n",
      "32  TCGA-KK-A8IJ     0  TCGA-KK-A8IJ-01Z-00-DX1.84EA7BE5-2707-4DDC-965...\n",
      "33  TCGA-G9-6384     0  TCGA-G9-6384-01Z-00-DX1.5f44fa1f-bedb-45b7-829...\n",
      "34  TCGA-V1-A8MJ     1  TCGA-V1-A8MJ-01Z-00-DX1.D93AA527-A186-4A95-AE0...\n",
      "35  TCGA-EJ-5510     0  TCGA-EJ-5510-01Z-00-DX1.ab391b79-7b8f-4ad8-b5b...\n",
      "36  TCGA-HC-8260     0  TCGA-HC-8260-01Z-00-DX1.b12ffab8-be66-4fb2-98f...\n",
      "37  TCGA-CH-5740     0  TCGA-CH-5740-01Z-00-DX1.cf299b59-b69b-4ab3-b7a...\n",
      "38  TCGA-J4-A67M     0  TCGA-J4-A67M-01Z-00-DX1.6C01EE22-ECE8-4FF3-803...\n",
      "39  TCGA-V1-A9OL     0  TCGA-V1-A9OL-01Z-00-DX1.1E81143A-3C49-4A79-880...\n",
      "40  TCGA-CH-5746     0  TCGA-CH-5746-01Z-00-DX1.5d6bab11-b652-4730-80e...\n",
      "41  TCGA-G9-6351     0  TCGA-G9-6351-01Z-00-DX1.53bfb95a-6a60-49dc-80e...\n",
      "42  TCGA-EJ-7782     1  TCGA-EJ-7782-01Z-00-DX1.ff736ebc-94c8-4ffd-b41...\n",
      "43  TCGA-EJ-7115     0  TCGA-EJ-7115-01Z-00-DX1.c603df66-3411-44b4-9e5...\n",
      "44  TCGA-M7-A724     0  TCGA-M7-A724-01Z-00-DX1.9A09FA7A-21B8-4F04-BA5...\n",
      "45  TCGA-CH-5748     0  TCGA-CH-5748-01Z-00-DX1.032202f2-70c0-4115-99f...\n",
      "46  TCGA-YL-A8SC     0  TCGA-YL-A8SC-01Z-00-DX1.B8CD6404-5586-44B1-A7A...\n",
      "47  TCGA-HC-A631     1  TCGA-HC-A631-01Z-00-DX1.9771F112-D955-4F5A-819...\n",
      "48  TCGA-HC-8256     0  TCGA-HC-8256-01Z-00-DX1.24CBDB5B-F10C-4C5D-9AC...\n",
      "49  TCGA-CH-5761     1  TCGA-CH-5761-01Z-00-DX1.8a1146c7-cd4b-40fd-b60...\n",
      "50  TCGA-HC-7211     0  TCGA-HC-7211-01Z-00-DX1.59ee78f9-0af3-4b3b-be4...\n",
      "51  TCGA-EJ-5511     0  TCGA-EJ-5511-01Z-00-DX1.45a97ef9-90fb-400d-9bd..., 'test':           case_id  TP53                                           slide_id\n",
      "0    TCGA-HC-7230     1  TCGA-HC-7230-01Z-00-DX1.da088457-3abd-4943-92e...\n",
      "1    TCGA-HC-7742     1  TCGA-HC-7742-01Z-00-DX1.59cf7b97-16a4-4c83-b4e...\n",
      "2    TCGA-XK-AAIW     1  TCGA-XK-AAIW-01Z-00-DX1.31B98849-4AA9-4AC1-A69...\n",
      "3    TCGA-V1-A9ZI     1  TCGA-V1-A9ZI-01Z-00-DX1.66AC3D31-41CB-4CF8-907...\n",
      "4    TCGA-V1-A9Z7     1  TCGA-V1-A9Z7-01Z-00-DX1.25193852-0015-4D15-A0E...\n",
      "..            ...   ...                                                ...\n",
      "152  TCGA-XQ-A8TA     0  TCGA-XQ-A8TA-01Z-00-DX1.D1346DCD-E0CC-4914-967...\n",
      "153  TCGA-HC-8265     0  TCGA-HC-8265-01Z-00-DX1.f0d35c7c-697c-4589-878...\n",
      "154  TCGA-J4-A83N     0  TCGA-J4-A83N-01Z-00-DX1.65D44347-8852-4394-A3B...\n",
      "155  TCGA-J4-A83K     0  TCGA-J4-A83K-01Z-00-DX1.7A8B686F-E4E3-48DF-876...\n",
      "156  TCGA-YL-A8SF     0  TCGA-YL-A8SF-01Z-00-DX1.CD189160-0B4D-4450-87D...\n",
      "\n",
      "[157 rows x 3 columns]}\n",
      "successfully read splits for:  ['train', 'tune', 'test']\n",
      "\n",
      "SPLIT:  train\n",
      "TP53\n",
      "0    1032\n",
      "1     557\n",
      "split: train, n: 1589\n",
      "\n",
      "SPLIT:  tune\n",
      "TP53\n",
      "0    46\n",
      "1     6\n",
      "split: tune, n: 52\n",
      "\n",
      "SPLIT:  test\n",
      "TP53\n",
      "0    139\n",
      "1     18\n",
      "split: test, n: 157\n",
      "\n",
      "Init Model... ABMIL doesn't construct unsupervised slide-level embeddings!\n",
      "Done!\n",
      "\n",
      "Init optimizer ... \n",
      "No EarlyStopping... ########## TRAIN Epoch: 0 ##########\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the static variables\n",
    "data_source = r\"/data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\"\n",
    "task = \"mutation\"\n",
    "batch_size = 4\n",
    "results_dir = \"/data/temporary/nadieh/mutation_prediction/results\"\n",
    "split_names = \"train,tune,test\"\n",
    "in_dim = 1024\n",
    "epoch = 15\n",
    "model_configs = [ \"ABMIL_default\"]\n",
    "label = 'TP53'\n",
    "# Loop over k from 1 to 5\n",
    "for model_config in model_configs:\n",
    "    model_type = \"ABMIL\"\n",
    "    for k in [2]:\n",
    "        split_dir = f\"/data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-{k}\"\n",
    "        # Build the command\n",
    "        cmd = [\n",
    "            'python3', 'training/main_classification.py',\n",
    "            '--data_source', data_source,\n",
    "            '--split_dir', split_dir,\n",
    "            '--task', task,\n",
    "            '--batch_size', str(batch_size),\n",
    "            '--results_dir', results_dir,\n",
    "            '--split_names', split_names,\n",
    "            '--in_dim', str(in_dim),\n",
    "            '--model_config', model_config,\n",
    "            '--target_col', label,\n",
    "            '--model_type', model_type,\n",
    "            '--max_epochs', str(epoch)\n",
    "        ]\n",
    "        # Execute the command\n",
    "        subprocess.run(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8520ba-6065-4ef3-a026-b38d8e0484dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map:  {0: 0, 1: 1}\n",
      "task:  mutation\n",
      "split_dir:  /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-0\n",
      ".\n",
      "data source:  /data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\n",
      "feats_pt\n",
      "  feats_name  mag  patch_size\n",
      "0   feats_pt   20         256\n",
      "All data sources have the same feature extraction parameters.\n",
      "\n",
      "################### Settings ###################\n",
      "max_epochs:  20\n",
      "lr:  0.0001\n",
      "wd:  1e-05\n",
      "accum_steps:  1\n",
      "opt:  adamW\n",
      "lr_scheduler:  constant\n",
      "warmup_steps:  -1\n",
      "warmup_epochs:  -1\n",
      "batch_size:  4\n",
      "print_every:  100\n",
      "seed:  1\n",
      "num_workers:  2\n",
      "early_stopping:  False\n",
      "es_min_epochs:  15\n",
      "es_patience:  10\n",
      "es_metric:  loss\n",
      "model_type:  ABMIL\n",
      "emb_model_type:  LinEmb_LR\n",
      "ot_eps:  0.1\n",
      "model_config:  ABMIL_default\n",
      "in_dim:  1024\n",
      "in_dropout:  0.0\n",
      "bag_size:  -1\n",
      "train_bag_size:  -1\n",
      "val_bag_size:  -1\n",
      "train_sampler:  random\n",
      "n_fc_layers:  None\n",
      "em_iter:  None\n",
      "tau:  None\n",
      "out_type:  param_cat\n",
      "load_proto:  False\n",
      "proto_path:  .\n",
      "fix_proto:  False\n",
      "n_proto:  None\n",
      "exp_code:  None\n",
      "task:  mutation\n",
      "loss_fn:  None\n",
      "target_col:  TP53\n",
      "data_source:  ['/data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt']\n",
      "split_dir:  /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-0\n",
      "split_names:  train,tune,test\n",
      "overwrite:  False\n",
      "results_dir:  /data/temporary/nadieh/mutation_prediction/results/mutation/k=0/temporary::ABMIL_default::feats_pt/temporary::ABMIL_default::feats_pt::25-02-27-02-29-27\n",
      "tags:  None\n",
      "label_map:  {0: 0, 1: 1}\n",
      "n_classes:  2\n",
      "split_name_clean:  temporary\n",
      "split_k:  0\n",
      "pretrain_enc:  feats_pt\n",
      "pretrain_algo:  \n",
      "pretrain_exp:  \n",
      "pretrain_ckpt:  -1\n",
      "inference_prec:  fp32\n",
      "patch_mag:  20\n",
      "patch_size:  256\n",
      "feat_names:  ['feats_pt']\n",
      "Using the following split names: ['train', 'tune', 'test']\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-0/train.csv\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-0/tune.csv\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-0/test.csv\n",
      "{'train':            case_id  TP53                                           slide_id\n",
      "0     TCGA-KK-A59Z     0  TCGA-KK-A59Z-01Z-00-DX1.83C16C66-CA61-4E86-8CF...\n",
      "1     TCGA-CH-5763     0  TCGA-CH-5763-01Z-00-DX1.7d4eff47-8d99-41d4-87f...\n",
      "2     TCGA-EJ-7793     0  TCGA-EJ-7793-01Z-00-DX1.9b4a589b-59d9-4abf-bfe...\n",
      "3     TCGA-G9-6361     0  TCGA-G9-6361-01Z-00-DX1.41814e1f-71af-4e91-b5b...\n",
      "4     TCGA-XK-AAIW     1  TCGA-XK-AAIW-01Z-00-DX1.31B98849-4AA9-4AC1-A69...\n",
      "...            ...   ...                                                ...\n",
      "1584  TCGA-ZF-A9R1     0  TCGA-ZF-A9R1-01Z-00-DX1.74520759-D448-45B5-B8A...\n",
      "1585  TCGA-CF-A3MI     0  TCGA-CF-A3MI-01Z-00-DX1.5619CA1A-F772-44CC-BBE...\n",
      "1586  TCGA-DK-A1AF     0  TCGA-DK-A1AF-01Z-00-DX1.E18DE029-27A6-43A6-9E9...\n",
      "1587  TCGA-BT-A20P     0  TCGA-BT-A20P-01Z-00-DX1.721A7D6B-C9F3-4A70-8E8...\n",
      "1588  TCGA-GV-A3QG     0  TCGA-GV-A3QG-01Z-00-DX1.4A550B20-2A4C-48E5-BFE...\n",
      "\n",
      "[1589 rows x 3 columns], 'tune':          case_id  TP53                                           slide_id\n",
      "0   TCGA-J4-A67O     0  TCGA-J4-A67O-01Z-00-DX1.C1E6799B-4351-485E-A7D...\n",
      "1   TCGA-HC-7078     0  TCGA-HC-7078-01Z-00-DX1.12d9739d-337d-4e6f-b97...\n",
      "2   TCGA-2A-AAYU     0  TCGA-2A-AAYU-01Z-00-DX1.F3FFE8F2-86B0-4552-A36...\n",
      "3   TCGA-M7-A720     0  TCGA-M7-A720-01Z-00-DX1.5680EDA8-FA91-4077-AB6...\n",
      "4   TCGA-CH-5746     0  TCGA-CH-5746-01Z-00-DX1.5d6bab11-b652-4730-80e...\n",
      "..           ...   ...                                                ...\n",
      "56  TCGA-EJ-7314     0  TCGA-EJ-7314-01Z-00-DX1.d0eb7da6-3259-475f-b81...\n",
      "57  TCGA-EJ-5507     1  TCGA-EJ-5507-01Z-00-DX1.6f3f7e05-a6f7-4450-a4b...\n",
      "58  TCGA-V1-A9OA     0  TCGA-V1-A9OA-01Z-00-DX1.72156A18-EA88-4004-964...\n",
      "59  TCGA-HC-8261     0  TCGA-HC-8261-01Z-00-DX1.231568ea-f873-4f62-85d...\n",
      "60  TCGA-FC-A5OB     0  TCGA-FC-A5OB-01Z-00-DX1.0055A98C-140F-48D0-B4E...\n",
      "\n",
      "[61 rows x 3 columns], 'test':           case_id  TP53                                           slide_id\n",
      "0    TCGA-EJ-7782     1  TCGA-EJ-7782-01Z-00-DX1.ff736ebc-94c8-4ffd-b41...\n",
      "1    TCGA-KK-A8IG     1  TCGA-KK-A8IG-01Z-00-DX1.57499D3B-8589-4A1A-A29...\n",
      "2    TCGA-J4-A67L     1  TCGA-J4-A67L-01Z-00-DX1.4B2B89CD-B390-488F-AE3...\n",
      "3    TCGA-YL-A8HK     1  TCGA-YL-A8HK-01Z-00-DX1.9A095D1F-A8CA-4BE4-9B4...\n",
      "4    TCGA-KK-A7AU     1  TCGA-KK-A7AU-01Z-00-DX1.5DA62B04-B06C-4EBB-951...\n",
      "..            ...   ...                                                ...\n",
      "143  TCGA-EJ-5501     0  TCGA-EJ-5501-01Z-00-DX1.b948a611-e351-417f-8f3...\n",
      "144  TCGA-CH-5762     0  TCGA-CH-5762-01Z-00-DX1.13c73407-5fdb-4109-91d...\n",
      "145  TCGA-EJ-7321     0  TCGA-EJ-7321-01Z-00-DX1.3c4d5cb6-647d-4f31-a46...\n",
      "146  TCGA-SU-A7E7     0  TCGA-SU-A7E7-01Z-00-DX1.1C4CB02E-100C-4A40-8C9...\n",
      "147  TCGA-2A-A8VO     0  TCGA-2A-A8VO-01Z-00-DX1.3A69CC37-B066-4529-B1B...\n",
      "\n",
      "[148 rows x 3 columns]}\n",
      "successfully read splits for:  ['train', 'tune', 'test']\n",
      "\n",
      "SPLIT:  train\n",
      "TP53\n",
      "0    1034\n",
      "1     555\n",
      "split: train, n: 1589\n",
      "\n",
      "SPLIT:  tune\n",
      "TP53\n",
      "0    51\n",
      "1    10\n",
      "split: tune, n: 61\n",
      "\n",
      "SPLIT:  test\n",
      "TP53\n",
      "0    132\n",
      "1     16\n",
      "split: test, n: 148\n",
      "\n",
      "Init Model... ABMIL doesn't construct unsupervised slide-level embeddings!\n",
      "Done!\n",
      "\n",
      "Init optimizer ... \n",
      "No EarlyStopping... ########## TRAIN Epoch: 0 ##########\n",
      "batch 99\tavg_bag_size: 53955.3900\tavg_cls_acc: 0.6600\tavg_cls_loss: 0.6532\tavg_loss: 0.6532\n",
      "batch 199\tavg_bag_size: 49241.7350\tavg_cls_acc: 0.7000\tavg_cls_loss: 0.5982\tavg_loss: 0.5982\n",
      "batch 299\tavg_bag_size: 49457.9167\tavg_cls_acc: 0.7000\tavg_cls_loss: 0.5919\tavg_loss: 0.5919\n",
      "batch 399\tavg_bag_size: 48999.3350\tavg_cls_acc: 0.6975\tavg_cls_loss: 0.5864\tavg_loss: 0.5864\n",
      "batch 499\tavg_bag_size: 49607.2440\tavg_cls_acc: 0.7040\tavg_cls_loss: 0.5796\tavg_loss: 0.5796\n",
      "batch 599\tavg_bag_size: 49897.1067\tavg_cls_acc: 0.7083\tavg_cls_loss: 0.5690\tavg_loss: 0.5690\n",
      "batch 699\tavg_bag_size: 49629.9386\tavg_cls_acc: 0.7129\tavg_cls_loss: 0.5703\tavg_loss: 0.5703\n",
      "batch 799\tavg_bag_size: 49741.6137\tavg_cls_acc: 0.7137\tavg_cls_loss: 0.5610\tavg_loss: 0.5610\n",
      "batch 899\tavg_bag_size: 50674.9433\tavg_cls_acc: 0.7222\tavg_cls_loss: 0.5544\tavg_loss: 0.5544\n",
      "batch 999\tavg_bag_size: 50145.0090\tavg_cls_acc: 0.7210\tavg_cls_loss: 0.5583\tavg_loss: 0.5583\n",
      "batch 1099\tavg_bag_size: 49929.0691\tavg_cls_acc: 0.7227\tavg_cls_loss: 0.5534\tavg_loss: 0.5534\n",
      "batch 1199\tavg_bag_size: 49829.3000\tavg_cls_acc: 0.7242\tavg_cls_loss: 0.5498\tavg_loss: 0.5498\n",
      "batch 1299\tavg_bag_size: 49605.7400\tavg_cls_acc: 0.7262\tavg_cls_loss: 0.5465\tavg_loss: 0.5465\n",
      "batch 1399\tavg_bag_size: 49591.5950\tavg_cls_acc: 0.7343\tavg_cls_loss: 0.5360\tavg_loss: 0.5360\n",
      "batch 1499\tavg_bag_size: 49567.7500\tavg_cls_acc: 0.7347\tavg_cls_loss: 0.5304\tavg_loss: 0.5304\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.7388\tavg_cls_loss: 0.5262\tavg_loss: 0.5262\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 1 ##########\n",
      "batch 99\tavg_bag_size: 50240.9800\tavg_cls_acc: 0.8100\tavg_cls_loss: 0.4503\tavg_loss: 0.4503\n",
      "batch 199\tavg_bag_size: 49254.4350\tavg_cls_acc: 0.7950\tavg_cls_loss: 0.4505\tavg_loss: 0.4505\n",
      "batch 299\tavg_bag_size: 49635.1333\tavg_cls_acc: 0.7767\tavg_cls_loss: 0.4533\tavg_loss: 0.4533\n",
      "batch 399\tavg_bag_size: 49534.2050\tavg_cls_acc: 0.7775\tavg_cls_loss: 0.4537\tavg_loss: 0.4537\n",
      "batch 499\tavg_bag_size: 49607.4460\tavg_cls_acc: 0.7900\tavg_cls_loss: 0.4375\tavg_loss: 0.4375\n",
      "batch 599\tavg_bag_size: 48970.0817\tavg_cls_acc: 0.8017\tavg_cls_loss: 0.4298\tavg_loss: 0.4298\n",
      "batch 699\tavg_bag_size: 48571.8386\tavg_cls_acc: 0.8000\tavg_cls_loss: 0.4351\tavg_loss: 0.4351\n",
      "batch 799\tavg_bag_size: 48731.4238\tavg_cls_acc: 0.7987\tavg_cls_loss: 0.4373\tavg_loss: 0.4373\n",
      "batch 899\tavg_bag_size: 48502.3433\tavg_cls_acc: 0.8022\tavg_cls_loss: 0.4373\tavg_loss: 0.4373\n",
      "batch 999\tavg_bag_size: 48639.7830\tavg_cls_acc: 0.8030\tavg_cls_loss: 0.4394\tavg_loss: 0.4394\n",
      "batch 1099\tavg_bag_size: 48469.4782\tavg_cls_acc: 0.8064\tavg_cls_loss: 0.4381\tavg_loss: 0.4381\n",
      "batch 1199\tavg_bag_size: 48850.0500\tavg_cls_acc: 0.8108\tavg_cls_loss: 0.4284\tavg_loss: 0.4284\n",
      "batch 1299\tavg_bag_size: 48850.5592\tavg_cls_acc: 0.8069\tavg_cls_loss: 0.4316\tavg_loss: 0.4316\n",
      "batch 1399\tavg_bag_size: 49281.1500\tavg_cls_acc: 0.8093\tavg_cls_loss: 0.4287\tavg_loss: 0.4287\n",
      "batch 1499\tavg_bag_size: 49576.7820\tavg_cls_acc: 0.8040\tavg_cls_loss: 0.4373\tavg_loss: 0.4373\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.8049\tavg_cls_loss: 0.4347\tavg_loss: 0.4347\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 2 ##########\n",
      "batch 99\tavg_bag_size: 47100.2000\tavg_cls_acc: 0.8400\tavg_cls_loss: 0.3368\tavg_loss: 0.3368\n",
      "batch 199\tavg_bag_size: 50110.7700\tavg_cls_acc: 0.8450\tavg_cls_loss: 0.3594\tavg_loss: 0.3594\n",
      "batch 299\tavg_bag_size: 51523.3167\tavg_cls_acc: 0.8533\tavg_cls_loss: 0.3336\tavg_loss: 0.3336\n",
      "batch 399\tavg_bag_size: 51416.0650\tavg_cls_acc: 0.8625\tavg_cls_loss: 0.3316\tavg_loss: 0.3316\n",
      "batch 499\tavg_bag_size: 50943.1600\tavg_cls_acc: 0.8480\tavg_cls_loss: 0.3667\tavg_loss: 0.3667\n",
      "batch 599\tavg_bag_size: 50154.8400\tavg_cls_acc: 0.8433\tavg_cls_loss: 0.3710\tavg_loss: 0.3710\n",
      "batch 699\tavg_bag_size: 49906.4543\tavg_cls_acc: 0.8371\tavg_cls_loss: 0.3769\tavg_loss: 0.3769\n",
      "batch 799\tavg_bag_size: 49898.7750\tavg_cls_acc: 0.8350\tavg_cls_loss: 0.3766\tavg_loss: 0.3766\n",
      "batch 899\tavg_bag_size: 49499.6578\tavg_cls_acc: 0.8322\tavg_cls_loss: 0.3759\tavg_loss: 0.3759\n",
      "batch 999\tavg_bag_size: 49146.1120\tavg_cls_acc: 0.8350\tavg_cls_loss: 0.3721\tavg_loss: 0.3721\n",
      "batch 1099\tavg_bag_size: 49026.2945\tavg_cls_acc: 0.8318\tavg_cls_loss: 0.3871\tavg_loss: 0.3871\n",
      "batch 1199\tavg_bag_size: 49225.9208\tavg_cls_acc: 0.8342\tavg_cls_loss: 0.3851\tavg_loss: 0.3851\n",
      "batch 1299\tavg_bag_size: 49888.0408\tavg_cls_acc: 0.8369\tavg_cls_loss: 0.3826\tavg_loss: 0.3826\n",
      "batch 1399\tavg_bag_size: 50019.8436\tavg_cls_acc: 0.8364\tavg_cls_loss: 0.3823\tavg_loss: 0.3823\n",
      "batch 1499\tavg_bag_size: 49804.1620\tavg_cls_acc: 0.8367\tavg_cls_loss: 0.3883\tavg_loss: 0.3883\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.8364\tavg_cls_loss: 0.3880\tavg_loss: 0.3880\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 3 ##########\n",
      "batch 99\tavg_bag_size: 50923.3300\tavg_cls_acc: 0.9000\tavg_cls_loss: 0.2886\tavg_loss: 0.2886\n",
      "batch 199\tavg_bag_size: 49566.6850\tavg_cls_acc: 0.8700\tavg_cls_loss: 0.3230\tavg_loss: 0.3230\n",
      "batch 299\tavg_bag_size: 49398.0267\tavg_cls_acc: 0.8667\tavg_cls_loss: 0.3196\tavg_loss: 0.3196\n",
      "batch 399\tavg_bag_size: 50707.7900\tavg_cls_acc: 0.8625\tavg_cls_loss: 0.3181\tavg_loss: 0.3181\n",
      "batch 499\tavg_bag_size: 50574.8320\tavg_cls_acc: 0.8660\tavg_cls_loss: 0.3194\tavg_loss: 0.3194\n",
      "batch 599\tavg_bag_size: 50393.5983\tavg_cls_acc: 0.8667\tavg_cls_loss: 0.3245\tavg_loss: 0.3245\n",
      "batch 699\tavg_bag_size: 50256.5529\tavg_cls_acc: 0.8629\tavg_cls_loss: 0.3233\tavg_loss: 0.3233\n",
      "batch 799\tavg_bag_size: 50297.2688\tavg_cls_acc: 0.8612\tavg_cls_loss: 0.3275\tavg_loss: 0.3275\n",
      "batch 899\tavg_bag_size: 50097.2967\tavg_cls_acc: 0.8567\tavg_cls_loss: 0.3421\tavg_loss: 0.3421\n",
      "batch 999\tavg_bag_size: 49922.1730\tavg_cls_acc: 0.8590\tavg_cls_loss: 0.3386\tavg_loss: 0.3386\n",
      "batch 1099\tavg_bag_size: 50021.1836\tavg_cls_acc: 0.8582\tavg_cls_loss: 0.3452\tavg_loss: 0.3452\n",
      "batch 1199\tavg_bag_size: 50214.8283\tavg_cls_acc: 0.8567\tavg_cls_loss: 0.3438\tavg_loss: 0.3438\n",
      "batch 1299\tavg_bag_size: 49662.4592\tavg_cls_acc: 0.8569\tavg_cls_loss: 0.3430\tavg_loss: 0.3430\n",
      "batch 1399\tavg_bag_size: 49528.2036\tavg_cls_acc: 0.8586\tavg_cls_loss: 0.3419\tavg_loss: 0.3419\n",
      "batch 1499\tavg_bag_size: 49552.0473\tavg_cls_acc: 0.8573\tavg_cls_loss: 0.3406\tavg_loss: 0.3406\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.8553\tavg_cls_loss: 0.3452\tavg_loss: 0.3452\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 4 ##########\n",
      "batch 99\tavg_bag_size: 53663.7700\tavg_cls_acc: 0.9300\tavg_cls_loss: 0.1839\tavg_loss: 0.1839\n",
      "batch 199\tavg_bag_size: 52109.3500\tavg_cls_acc: 0.9150\tavg_cls_loss: 0.2103\tavg_loss: 0.2103\n",
      "batch 299\tavg_bag_size: 50167.9600\tavg_cls_acc: 0.9167\tavg_cls_loss: 0.2091\tavg_loss: 0.2091\n",
      "batch 399\tavg_bag_size: 49286.9900\tavg_cls_acc: 0.9025\tavg_cls_loss: 0.2309\tavg_loss: 0.2309\n",
      "batch 499\tavg_bag_size: 50026.7720\tavg_cls_acc: 0.8960\tavg_cls_loss: 0.2485\tavg_loss: 0.2485\n",
      "batch 599\tavg_bag_size: 49642.7650\tavg_cls_acc: 0.8883\tavg_cls_loss: 0.2659\tavg_loss: 0.2659\n",
      "batch 699\tavg_bag_size: 49853.7829\tavg_cls_acc: 0.8829\tavg_cls_loss: 0.2735\tavg_loss: 0.2735\n",
      "batch 799\tavg_bag_size: 49865.9313\tavg_cls_acc: 0.8900\tavg_cls_loss: 0.2650\tavg_loss: 0.2650\n",
      "batch 899\tavg_bag_size: 49515.2767\tavg_cls_acc: 0.8878\tavg_cls_loss: 0.2662\tavg_loss: 0.2662\n",
      "batch 999\tavg_bag_size: 48893.2750\tavg_cls_acc: 0.8870\tavg_cls_loss: 0.2747\tavg_loss: 0.2747\n",
      "batch 1099\tavg_bag_size: 49191.0800\tavg_cls_acc: 0.8882\tavg_cls_loss: 0.2752\tavg_loss: 0.2752\n",
      "batch 1199\tavg_bag_size: 49270.9317\tavg_cls_acc: 0.8833\tavg_cls_loss: 0.2834\tavg_loss: 0.2834\n",
      "batch 1299\tavg_bag_size: 49569.7331\tavg_cls_acc: 0.8846\tavg_cls_loss: 0.2845\tavg_loss: 0.2845\n",
      "batch 1399\tavg_bag_size: 49529.6243\tavg_cls_acc: 0.8843\tavg_cls_loss: 0.2851\tavg_loss: 0.2851\n",
      "batch 1499\tavg_bag_size: 49614.3740\tavg_cls_acc: 0.8853\tavg_cls_loss: 0.2858\tavg_loss: 0.2858\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.8848\tavg_cls_loss: 0.2859\tavg_loss: 0.2859\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 5 ##########\n",
      "batch 99\tavg_bag_size: 50483.3100\tavg_cls_acc: 0.8900\tavg_cls_loss: 0.2148\tavg_loss: 0.2148\n",
      "batch 199\tavg_bag_size: 48516.8600\tavg_cls_acc: 0.9050\tavg_cls_loss: 0.2140\tavg_loss: 0.2140\n",
      "batch 299\tavg_bag_size: 51112.1067\tavg_cls_acc: 0.9133\tavg_cls_loss: 0.2046\tavg_loss: 0.2046\n",
      "batch 399\tavg_bag_size: 51124.8550\tavg_cls_acc: 0.9100\tavg_cls_loss: 0.2069\tavg_loss: 0.2069\n",
      "batch 499\tavg_bag_size: 50823.1700\tavg_cls_acc: 0.9100\tavg_cls_loss: 0.2081\tavg_loss: 0.2081\n",
      "batch 599\tavg_bag_size: 50179.6067\tavg_cls_acc: 0.9117\tavg_cls_loss: 0.2046\tavg_loss: 0.2046\n",
      "batch 699\tavg_bag_size: 50159.8971\tavg_cls_acc: 0.9157\tavg_cls_loss: 0.2037\tavg_loss: 0.2037\n",
      "batch 799\tavg_bag_size: 50235.8763\tavg_cls_acc: 0.9163\tavg_cls_loss: 0.2139\tavg_loss: 0.2139\n",
      "batch 899\tavg_bag_size: 49713.5256\tavg_cls_acc: 0.9156\tavg_cls_loss: 0.2191\tavg_loss: 0.2191\n",
      "batch 999\tavg_bag_size: 49369.4840\tavg_cls_acc: 0.9100\tavg_cls_loss: 0.2309\tavg_loss: 0.2309\n",
      "batch 1099\tavg_bag_size: 49181.0255\tavg_cls_acc: 0.9073\tavg_cls_loss: 0.2381\tavg_loss: 0.2381\n",
      "batch 1199\tavg_bag_size: 49065.8533\tavg_cls_acc: 0.9083\tavg_cls_loss: 0.2347\tavg_loss: 0.2347\n",
      "batch 1299\tavg_bag_size: 49217.4492\tavg_cls_acc: 0.9046\tavg_cls_loss: 0.2352\tavg_loss: 0.2352\n",
      "batch 1399\tavg_bag_size: 49369.3593\tavg_cls_acc: 0.9057\tavg_cls_loss: 0.2323\tavg_loss: 0.2323\n",
      "batch 1499\tavg_bag_size: 49554.6793\tavg_cls_acc: 0.9047\tavg_cls_loss: 0.2323\tavg_loss: 0.2323\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9018\tavg_cls_loss: 0.2351\tavg_loss: 0.2351\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 6 ##########\n",
      "batch 99\tavg_bag_size: 54554.4300\tavg_cls_acc: 0.9300\tavg_cls_loss: 0.1637\tavg_loss: 0.1637\n",
      "batch 199\tavg_bag_size: 52724.3400\tavg_cls_acc: 0.9400\tavg_cls_loss: 0.1370\tavg_loss: 0.1370\n",
      "batch 299\tavg_bag_size: 51704.4833\tavg_cls_acc: 0.9400\tavg_cls_loss: 0.1375\tavg_loss: 0.1375\n",
      "batch 399\tavg_bag_size: 50267.1750\tavg_cls_acc: 0.9400\tavg_cls_loss: 0.1360\tavg_loss: 0.1360\n",
      "batch 499\tavg_bag_size: 50055.3960\tavg_cls_acc: 0.9280\tavg_cls_loss: 0.1538\tavg_loss: 0.1538\n",
      "batch 599\tavg_bag_size: 50154.2900\tavg_cls_acc: 0.9233\tavg_cls_loss: 0.1660\tavg_loss: 0.1660\n",
      "batch 699\tavg_bag_size: 49782.2043\tavg_cls_acc: 0.9271\tavg_cls_loss: 0.1649\tavg_loss: 0.1649\n",
      "batch 799\tavg_bag_size: 49403.5150\tavg_cls_acc: 0.9250\tavg_cls_loss: 0.1644\tavg_loss: 0.1644\n",
      "batch 899\tavg_bag_size: 49103.0144\tavg_cls_acc: 0.9267\tavg_cls_loss: 0.1721\tavg_loss: 0.1721\n",
      "batch 999\tavg_bag_size: 49003.0900\tavg_cls_acc: 0.9210\tavg_cls_loss: 0.1825\tavg_loss: 0.1825\n",
      "batch 1099\tavg_bag_size: 49073.3600\tavg_cls_acc: 0.9236\tavg_cls_loss: 0.1814\tavg_loss: 0.1814\n",
      "batch 1199\tavg_bag_size: 49490.0500\tavg_cls_acc: 0.9233\tavg_cls_loss: 0.1827\tavg_loss: 0.1827\n",
      "batch 1299\tavg_bag_size: 49530.0000\tavg_cls_acc: 0.9246\tavg_cls_loss: 0.1818\tavg_loss: 0.1818\n",
      "batch 1399\tavg_bag_size: 49636.4493\tavg_cls_acc: 0.9264\tavg_cls_loss: 0.1804\tavg_loss: 0.1804\n",
      "batch 1499\tavg_bag_size: 49517.8833\tavg_cls_acc: 0.9227\tavg_cls_loss: 0.1879\tavg_loss: 0.1879\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9220\tavg_cls_loss: 0.1897\tavg_loss: 0.1897\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 7 ##########\n",
      "batch 99\tavg_bag_size: 47030.9700\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.1273\tavg_loss: 0.1273\n",
      "batch 199\tavg_bag_size: 49904.8950\tavg_cls_acc: 0.9550\tavg_cls_loss: 0.1258\tavg_loss: 0.1258\n",
      "batch 299\tavg_bag_size: 51339.5867\tavg_cls_acc: 0.9567\tavg_cls_loss: 0.1162\tavg_loss: 0.1162\n",
      "batch 399\tavg_bag_size: 49786.6150\tavg_cls_acc: 0.9525\tavg_cls_loss: 0.1243\tavg_loss: 0.1243\n",
      "batch 499\tavg_bag_size: 49839.3020\tavg_cls_acc: 0.9480\tavg_cls_loss: 0.1413\tavg_loss: 0.1413\n",
      "batch 599\tavg_bag_size: 49646.2517\tavg_cls_acc: 0.9467\tavg_cls_loss: 0.1447\tavg_loss: 0.1447\n",
      "batch 699\tavg_bag_size: 49823.8357\tavg_cls_acc: 0.9500\tavg_cls_loss: 0.1389\tavg_loss: 0.1389\n",
      "batch 799\tavg_bag_size: 49633.1875\tavg_cls_acc: 0.9525\tavg_cls_loss: 0.1337\tavg_loss: 0.1337\n",
      "batch 899\tavg_bag_size: 49436.0633\tavg_cls_acc: 0.9511\tavg_cls_loss: 0.1326\tavg_loss: 0.1326\n",
      "batch 999\tavg_bag_size: 49897.8120\tavg_cls_acc: 0.9490\tavg_cls_loss: 0.1340\tavg_loss: 0.1340\n",
      "batch 1099\tavg_bag_size: 49907.6309\tavg_cls_acc: 0.9491\tavg_cls_loss: 0.1415\tavg_loss: 0.1415\n",
      "batch 1199\tavg_bag_size: 49711.4542\tavg_cls_acc: 0.9500\tavg_cls_loss: 0.1403\tavg_loss: 0.1403\n",
      "batch 1299\tavg_bag_size: 49843.7908\tavg_cls_acc: 0.9469\tavg_cls_loss: 0.1427\tavg_loss: 0.1427\n",
      "batch 1399\tavg_bag_size: 49702.5186\tavg_cls_acc: 0.9443\tavg_cls_loss: 0.1493\tavg_loss: 0.1493\n",
      "batch 1499\tavg_bag_size: 49654.1560\tavg_cls_acc: 0.9447\tavg_cls_loss: 0.1517\tavg_loss: 0.1517\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9452\tavg_cls_loss: 0.1490\tavg_loss: 0.1490\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 8 ##########\n",
      "batch 99\tavg_bag_size: 47016.0500\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0965\tavg_loss: 0.0965\n",
      "batch 199\tavg_bag_size: 44261.9750\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0693\tavg_loss: 0.0693\n",
      "batch 299\tavg_bag_size: 47965.1767\tavg_cls_acc: 0.9767\tavg_cls_loss: 0.0788\tavg_loss: 0.0788\n",
      "batch 399\tavg_bag_size: 49465.1025\tavg_cls_acc: 0.9750\tavg_cls_loss: 0.0799\tavg_loss: 0.0799\n",
      "batch 499\tavg_bag_size: 49737.3520\tavg_cls_acc: 0.9720\tavg_cls_loss: 0.0825\tavg_loss: 0.0825\n",
      "batch 599\tavg_bag_size: 49503.7567\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0858\tavg_loss: 0.0858\n",
      "batch 699\tavg_bag_size: 48944.6800\tavg_cls_acc: 0.9686\tavg_cls_loss: 0.0918\tavg_loss: 0.0918\n",
      "batch 799\tavg_bag_size: 48974.2513\tavg_cls_acc: 0.9688\tavg_cls_loss: 0.0933\tavg_loss: 0.0933\n",
      "batch 899\tavg_bag_size: 49579.0533\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0912\tavg_loss: 0.0912\n",
      "batch 999\tavg_bag_size: 49960.3270\tavg_cls_acc: 0.9630\tavg_cls_loss: 0.1017\tavg_loss: 0.1017\n",
      "batch 1099\tavg_bag_size: 49879.8982\tavg_cls_acc: 0.9636\tavg_cls_loss: 0.1038\tavg_loss: 0.1038\n",
      "batch 1199\tavg_bag_size: 49987.0100\tavg_cls_acc: 0.9617\tavg_cls_loss: 0.1078\tavg_loss: 0.1078\n",
      "batch 1299\tavg_bag_size: 49571.1031\tavg_cls_acc: 0.9623\tavg_cls_loss: 0.1065\tavg_loss: 0.1065\n",
      "batch 1399\tavg_bag_size: 49682.4179\tavg_cls_acc: 0.9629\tavg_cls_loss: 0.1075\tavg_loss: 0.1075\n",
      "batch 1499\tavg_bag_size: 49763.9420\tavg_cls_acc: 0.9633\tavg_cls_loss: 0.1046\tavg_loss: 0.1046\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9610\tavg_cls_loss: 0.1073\tavg_loss: 0.1073\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 9 ##########\n",
      "batch 99\tavg_bag_size: 45732.8300\tavg_cls_acc: 0.9600\tavg_cls_loss: 0.0679\tavg_loss: 0.0679\n",
      "batch 199\tavg_bag_size: 47107.4600\tavg_cls_acc: 0.9650\tavg_cls_loss: 0.0774\tavg_loss: 0.0774\n",
      "batch 299\tavg_bag_size: 48101.4633\tavg_cls_acc: 0.9567\tavg_cls_loss: 0.0990\tavg_loss: 0.0990\n",
      "batch 399\tavg_bag_size: 49887.9025\tavg_cls_acc: 0.9600\tavg_cls_loss: 0.0994\tavg_loss: 0.0994\n",
      "batch 499\tavg_bag_size: 50078.7800\tavg_cls_acc: 0.9580\tavg_cls_loss: 0.0993\tavg_loss: 0.0993\n",
      "batch 599\tavg_bag_size: 49308.3950\tavg_cls_acc: 0.9617\tavg_cls_loss: 0.0936\tavg_loss: 0.0936\n",
      "batch 699\tavg_bag_size: 49034.8943\tavg_cls_acc: 0.9657\tavg_cls_loss: 0.0878\tavg_loss: 0.0878\n",
      "batch 799\tavg_bag_size: 49120.7087\tavg_cls_acc: 0.9688\tavg_cls_loss: 0.0830\tavg_loss: 0.0830\n",
      "batch 899\tavg_bag_size: 49232.1667\tavg_cls_acc: 0.9667\tavg_cls_loss: 0.0853\tavg_loss: 0.0853\n",
      "batch 999\tavg_bag_size: 49183.6920\tavg_cls_acc: 0.9660\tavg_cls_loss: 0.0880\tavg_loss: 0.0880\n",
      "batch 1099\tavg_bag_size: 49240.0873\tavg_cls_acc: 0.9655\tavg_cls_loss: 0.0874\tavg_loss: 0.0874\n",
      "batch 1199\tavg_bag_size: 49346.1733\tavg_cls_acc: 0.9667\tavg_cls_loss: 0.0854\tavg_loss: 0.0854\n",
      "batch 1299\tavg_bag_size: 49529.2092\tavg_cls_acc: 0.9692\tavg_cls_loss: 0.0826\tavg_loss: 0.0826\n",
      "batch 1399\tavg_bag_size: 49404.4636\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0826\tavg_loss: 0.0826\n",
      "batch 1499\tavg_bag_size: 49432.3920\tavg_cls_acc: 0.9707\tavg_cls_loss: 0.0804\tavg_loss: 0.0804\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9685\tavg_cls_loss: 0.0820\tavg_loss: 0.0820\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 10 ##########\n",
      "batch 99\tavg_bag_size: 47760.0300\tavg_cls_acc: 0.9500\tavg_cls_loss: 0.1925\tavg_loss: 0.1925\n",
      "batch 199\tavg_bag_size: 47494.2250\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.1243\tavg_loss: 0.1243\n",
      "batch 299\tavg_bag_size: 48343.1967\tavg_cls_acc: 0.9767\tavg_cls_loss: 0.0992\tavg_loss: 0.0992\n",
      "batch 399\tavg_bag_size: 47511.0125\tavg_cls_acc: 0.9825\tavg_cls_loss: 0.0818\tavg_loss: 0.0818\n",
      "batch 499\tavg_bag_size: 47654.0680\tavg_cls_acc: 0.9840\tavg_cls_loss: 0.0725\tavg_loss: 0.0725\n",
      "batch 599\tavg_bag_size: 48149.7633\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0660\tavg_loss: 0.0660\n",
      "batch 699\tavg_bag_size: 48310.4100\tavg_cls_acc: 0.9857\tavg_cls_loss: 0.0618\tavg_loss: 0.0618\n",
      "batch 799\tavg_bag_size: 49162.6338\tavg_cls_acc: 0.9812\tavg_cls_loss: 0.0695\tavg_loss: 0.0695\n",
      "batch 899\tavg_bag_size: 49255.9644\tavg_cls_acc: 0.9778\tavg_cls_loss: 0.0754\tavg_loss: 0.0754\n",
      "batch 999\tavg_bag_size: 49706.4690\tavg_cls_acc: 0.9780\tavg_cls_loss: 0.0739\tavg_loss: 0.0739\n",
      "batch 1099\tavg_bag_size: 49786.4827\tavg_cls_acc: 0.9791\tavg_cls_loss: 0.0706\tavg_loss: 0.0706\n",
      "batch 1199\tavg_bag_size: 49841.8967\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0701\tavg_loss: 0.0701\n",
      "batch 1299\tavg_bag_size: 49750.1723\tavg_cls_acc: 0.9777\tavg_cls_loss: 0.0735\tavg_loss: 0.0735\n",
      "batch 1399\tavg_bag_size: 49859.8907\tavg_cls_acc: 0.9793\tavg_cls_loss: 0.0706\tavg_loss: 0.0706\n",
      "batch 1499\tavg_bag_size: 49918.0780\tavg_cls_acc: 0.9807\tavg_cls_loss: 0.0678\tavg_loss: 0.0678\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9805\tavg_cls_loss: 0.0687\tavg_loss: 0.0687\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 11 ##########\n",
      "batch 99\tavg_bag_size: 47992.3700\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0560\tavg_loss: 0.0560\n",
      "batch 199\tavg_bag_size: 48295.0200\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0435\tavg_loss: 0.0435\n",
      "batch 299\tavg_bag_size: 49940.4467\tavg_cls_acc: 0.9833\tavg_cls_loss: 0.0609\tavg_loss: 0.0609\n",
      "batch 399\tavg_bag_size: 49791.1550\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0600\tavg_loss: 0.0600\n",
      "batch 499\tavg_bag_size: 50011.4360\tavg_cls_acc: 0.9840\tavg_cls_loss: 0.0525\tavg_loss: 0.0525\n",
      "batch 599\tavg_bag_size: 49629.6067\tavg_cls_acc: 0.9833\tavg_cls_loss: 0.0519\tavg_loss: 0.0519\n",
      "batch 699\tavg_bag_size: 49882.9557\tavg_cls_acc: 0.9786\tavg_cls_loss: 0.0590\tavg_loss: 0.0590\n",
      "batch 799\tavg_bag_size: 49595.4212\tavg_cls_acc: 0.9788\tavg_cls_loss: 0.0601\tavg_loss: 0.0601\n",
      "batch 899\tavg_bag_size: 50021.8533\tavg_cls_acc: 0.9811\tavg_cls_loss: 0.0571\tavg_loss: 0.0571\n",
      "batch 999\tavg_bag_size: 49910.0260\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0577\tavg_loss: 0.0577\n",
      "batch 1099\tavg_bag_size: 50029.7900\tavg_cls_acc: 0.9782\tavg_cls_loss: 0.0594\tavg_loss: 0.0594\n",
      "batch 1199\tavg_bag_size: 50074.7458\tavg_cls_acc: 0.9783\tavg_cls_loss: 0.0587\tavg_loss: 0.0587\n",
      "batch 1299\tavg_bag_size: 50144.8331\tavg_cls_acc: 0.9792\tavg_cls_loss: 0.0588\tavg_loss: 0.0588\n",
      "batch 1399\tavg_bag_size: 49911.1579\tavg_cls_acc: 0.9793\tavg_cls_loss: 0.0582\tavg_loss: 0.0582\n",
      "batch 1499\tavg_bag_size: 49880.9487\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0558\tavg_loss: 0.0558\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9811\tavg_cls_loss: 0.0538\tavg_loss: 0.0538\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 12 ##########\n",
      "batch 99\tavg_bag_size: 48571.9900\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0136\tavg_loss: 0.0136\n",
      "batch 199\tavg_bag_size: 47380.5350\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0459\tavg_loss: 0.0459\n",
      "batch 299\tavg_bag_size: 47467.7367\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0373\tavg_loss: 0.0373\n",
      "batch 399\tavg_bag_size: 47947.2250\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0290\tavg_loss: 0.0290\n",
      "batch 499\tavg_bag_size: 47548.2340\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0283\tavg_loss: 0.0283\n",
      "batch 599\tavg_bag_size: 47490.6567\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0275\tavg_loss: 0.0275\n",
      "batch 699\tavg_bag_size: 48422.7486\tavg_cls_acc: 0.9886\tavg_cls_loss: 0.0293\tavg_loss: 0.0293\n",
      "batch 799\tavg_bag_size: 48779.2225\tavg_cls_acc: 0.9888\tavg_cls_loss: 0.0290\tavg_loss: 0.0290\n",
      "batch 899\tavg_bag_size: 49415.9422\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0364\tavg_loss: 0.0364\n",
      "batch 999\tavg_bag_size: 49250.3740\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0379\tavg_loss: 0.0379\n",
      "batch 1099\tavg_bag_size: 49667.3418\tavg_cls_acc: 0.9855\tavg_cls_loss: 0.0393\tavg_loss: 0.0393\n",
      "batch 1199\tavg_bag_size: 49677.6058\tavg_cls_acc: 0.9858\tavg_cls_loss: 0.0391\tavg_loss: 0.0391\n",
      "batch 1299\tavg_bag_size: 49676.5500\tavg_cls_acc: 0.9854\tavg_cls_loss: 0.0407\tavg_loss: 0.0407\n",
      "batch 1399\tavg_bag_size: 49945.6464\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0400\tavg_loss: 0.0400\n",
      "batch 1499\tavg_bag_size: 49913.5553\tavg_cls_acc: 0.9853\tavg_cls_loss: 0.0396\tavg_loss: 0.0396\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9849\tavg_cls_loss: 0.0393\tavg_loss: 0.0393\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 13 ##########\n",
      "batch 99\tavg_bag_size: 46254.6600\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0183\tavg_loss: 0.0183\n",
      "batch 199\tavg_bag_size: 46890.0250\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0330\tavg_loss: 0.0330\n",
      "batch 299\tavg_bag_size: 47202.1067\tavg_cls_acc: 0.9967\tavg_cls_loss: 0.0236\tavg_loss: 0.0236\n",
      "batch 399\tavg_bag_size: 48877.5750\tavg_cls_acc: 0.9925\tavg_cls_loss: 0.0306\tavg_loss: 0.0306\n",
      "batch 499\tavg_bag_size: 48968.5500\tavg_cls_acc: 0.9860\tavg_cls_loss: 0.0853\tavg_loss: 0.0853\n",
      "batch 599\tavg_bag_size: 49001.6533\tavg_cls_acc: 0.9817\tavg_cls_loss: 0.0924\tavg_loss: 0.0924\n",
      "batch 699\tavg_bag_size: 49422.5800\tavg_cls_acc: 0.9771\tavg_cls_loss: 0.0973\tavg_loss: 0.0973\n",
      "batch 799\tavg_bag_size: 49353.9437\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0873\tavg_loss: 0.0873\n",
      "batch 899\tavg_bag_size: 49155.1678\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0841\tavg_loss: 0.0841\n",
      "batch 999\tavg_bag_size: 49456.9330\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0808\tavg_loss: 0.0808\n",
      "batch 1099\tavg_bag_size: 49778.3955\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0778\tavg_loss: 0.0778\n",
      "batch 1199\tavg_bag_size: 49573.7942\tavg_cls_acc: 0.9817\tavg_cls_loss: 0.0730\tavg_loss: 0.0730\n",
      "batch 1299\tavg_bag_size: 49245.7115\tavg_cls_acc: 0.9831\tavg_cls_loss: 0.0691\tavg_loss: 0.0691\n",
      "batch 1399\tavg_bag_size: 49567.7500\tavg_cls_acc: 0.9843\tavg_cls_loss: 0.0648\tavg_loss: 0.0648\n",
      "batch 1499\tavg_bag_size: 49706.4107\tavg_cls_acc: 0.9853\tavg_cls_loss: 0.0616\tavg_loss: 0.0616\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9862\tavg_cls_loss: 0.0590\tavg_loss: 0.0590\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 14 ##########\n",
      "batch 99\tavg_bag_size: 46890.3000\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0040\tavg_loss: 0.0040\n",
      "batch 199\tavg_bag_size: 48918.0900\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0055\tavg_loss: 0.0055\n",
      "batch 299\tavg_bag_size: 47021.8533\tavg_cls_acc: 0.9967\tavg_cls_loss: 0.0127\tavg_loss: 0.0127\n",
      "batch 399\tavg_bag_size: 47744.7550\tavg_cls_acc: 0.9975\tavg_cls_loss: 0.0112\tavg_loss: 0.0112\n",
      "batch 499\tavg_bag_size: 48458.4780\tavg_cls_acc: 0.9980\tavg_cls_loss: 0.0107\tavg_loss: 0.0107\n",
      "batch 599\tavg_bag_size: 48443.5283\tavg_cls_acc: 0.9983\tavg_cls_loss: 0.0095\tavg_loss: 0.0095\n",
      "batch 699\tavg_bag_size: 48985.8143\tavg_cls_acc: 0.9986\tavg_cls_loss: 0.0090\tavg_loss: 0.0090\n",
      "batch 799\tavg_bag_size: 49768.5675\tavg_cls_acc: 0.9988\tavg_cls_loss: 0.0095\tavg_loss: 0.0095\n",
      "batch 899\tavg_bag_size: 49617.8122\tavg_cls_acc: 0.9922\tavg_cls_loss: 0.0225\tavg_loss: 0.0225\n",
      "batch 999\tavg_bag_size: 49472.1590\tavg_cls_acc: 0.9860\tavg_cls_loss: 0.0341\tavg_loss: 0.0341\n",
      "batch 1099\tavg_bag_size: 49723.6518\tavg_cls_acc: 0.9855\tavg_cls_loss: 0.0361\tavg_loss: 0.0361\n",
      "batch 1199\tavg_bag_size: 49901.9092\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0381\tavg_loss: 0.0381\n",
      "batch 1299\tavg_bag_size: 49839.8692\tavg_cls_acc: 0.9846\tavg_cls_loss: 0.0370\tavg_loss: 0.0370\n",
      "batch 1399\tavg_bag_size: 50152.3479\tavg_cls_acc: 0.9857\tavg_cls_loss: 0.0350\tavg_loss: 0.0350\n",
      "batch 1499\tavg_bag_size: 49802.5660\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0335\tavg_loss: 0.0335\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9874\tavg_cls_loss: 0.0320\tavg_loss: 0.0320\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 15 ##########\n",
      "batch 99\tavg_bag_size: 50674.0200\tavg_cls_acc: 0.9600\tavg_cls_loss: 0.1031\tavg_loss: 0.1031\n",
      "batch 199\tavg_bag_size: 49146.0000\tavg_cls_acc: 0.9750\tavg_cls_loss: 0.0640\tavg_loss: 0.0640\n",
      "batch 299\tavg_bag_size: 48444.4900\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0520\tavg_loss: 0.0520\n",
      "batch 399\tavg_bag_size: 47523.2225\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0404\tavg_loss: 0.0404\n",
      "batch 499\tavg_bag_size: 48623.3980\tavg_cls_acc: 0.9880\tavg_cls_loss: 0.0344\tavg_loss: 0.0344\n",
      "batch 599\tavg_bag_size: 48643.1533\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0294\tavg_loss: 0.0294\n",
      "batch 699\tavg_bag_size: 48596.6329\tavg_cls_acc: 0.9886\tavg_cls_loss: 0.0355\tavg_loss: 0.0355\n",
      "batch 799\tavg_bag_size: 48166.4825\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0357\tavg_loss: 0.0357\n",
      "batch 899\tavg_bag_size: 48763.4289\tavg_cls_acc: 0.9889\tavg_cls_loss: 0.0327\tavg_loss: 0.0327\n",
      "batch 999\tavg_bag_size: 49089.0240\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0304\tavg_loss: 0.0304\n",
      "batch 1099\tavg_bag_size: 49133.8027\tavg_cls_acc: 0.9891\tavg_cls_loss: 0.0389\tavg_loss: 0.0389\n",
      "batch 1199\tavg_bag_size: 49269.7183\tavg_cls_acc: 0.9892\tavg_cls_loss: 0.0395\tavg_loss: 0.0395\n",
      "batch 1299\tavg_bag_size: 49200.0292\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0381\tavg_loss: 0.0381\n",
      "batch 1399\tavg_bag_size: 49553.7150\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0374\tavg_loss: 0.0374\n",
      "batch 1499\tavg_bag_size: 49585.3740\tavg_cls_acc: 0.9907\tavg_cls_loss: 0.0351\tavg_loss: 0.0351\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9912\tavg_cls_loss: 0.0337\tavg_loss: 0.0337\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 16 ##########\n",
      "batch 99\tavg_bag_size: 47521.6600\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0295\tavg_loss: 0.0295\n",
      "batch 199\tavg_bag_size: 50232.5600\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0212\tavg_loss: 0.0212\n",
      "batch 299\tavg_bag_size: 49375.6467\tavg_cls_acc: 0.9933\tavg_cls_loss: 0.0170\tavg_loss: 0.0170\n",
      "batch 399\tavg_bag_size: 50096.5125\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0311\tavg_loss: 0.0311\n",
      "batch 499\tavg_bag_size: 50901.4760\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0281\tavg_loss: 0.0281\n",
      "batch 599\tavg_bag_size: 50547.5000\tavg_cls_acc: 0.9883\tavg_cls_loss: 0.0315\tavg_loss: 0.0315\n",
      "batch 699\tavg_bag_size: 50976.1043\tavg_cls_acc: 0.9829\tavg_cls_loss: 0.0402\tavg_loss: 0.0402\n",
      "batch 799\tavg_bag_size: 50691.8938\tavg_cls_acc: 0.9825\tavg_cls_loss: 0.0423\tavg_loss: 0.0423\n",
      "batch 899\tavg_bag_size: 50699.7833\tavg_cls_acc: 0.9822\tavg_cls_loss: 0.0471\tavg_loss: 0.0471\n",
      "batch 999\tavg_bag_size: 50223.7980\tavg_cls_acc: 0.9830\tavg_cls_loss: 0.0441\tavg_loss: 0.0441\n",
      "batch 1099\tavg_bag_size: 49893.5109\tavg_cls_acc: 0.9818\tavg_cls_loss: 0.0466\tavg_loss: 0.0466\n",
      "batch 1199\tavg_bag_size: 50119.8342\tavg_cls_acc: 0.9825\tavg_cls_loss: 0.0453\tavg_loss: 0.0453\n",
      "batch 1299\tavg_bag_size: 49926.3985\tavg_cls_acc: 0.9823\tavg_cls_loss: 0.0475\tavg_loss: 0.0475\n",
      "batch 1399\tavg_bag_size: 50088.2186\tavg_cls_acc: 0.9829\tavg_cls_loss: 0.0467\tavg_loss: 0.0467\n",
      "batch 1499\tavg_bag_size: 49585.8960\tavg_cls_acc: 0.9833\tavg_cls_loss: 0.0457\tavg_loss: 0.0457\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9843\tavg_cls_loss: 0.0440\tavg_loss: 0.0440\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 17 ##########\n",
      "batch 99\tavg_bag_size: 46797.4200\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0098\tavg_loss: 0.0098\n",
      "batch 199\tavg_bag_size: 49529.4100\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0067\tavg_loss: 0.0067\n",
      "batch 299\tavg_bag_size: 50681.4133\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0052\tavg_loss: 0.0052\n",
      "batch 399\tavg_bag_size: 50766.4950\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0072\tavg_loss: 0.0072\n",
      "batch 499\tavg_bag_size: 49397.1320\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0079\tavg_loss: 0.0079\n",
      "batch 599\tavg_bag_size: 49250.9967\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0079\tavg_loss: 0.0079\n",
      "batch 699\tavg_bag_size: 49782.4929\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0070\tavg_loss: 0.0070\n",
      "batch 799\tavg_bag_size: 49870.1963\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0063\tavg_loss: 0.0063\n",
      "batch 899\tavg_bag_size: 49510.0967\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0065\tavg_loss: 0.0065\n",
      "batch 999\tavg_bag_size: 49393.9210\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0066\tavg_loss: 0.0066\n",
      "batch 1099\tavg_bag_size: 49649.2827\tavg_cls_acc: 0.9991\tavg_cls_loss: 0.0079\tavg_loss: 0.0079\n",
      "batch 1199\tavg_bag_size: 49944.1450\tavg_cls_acc: 0.9983\tavg_cls_loss: 0.0102\tavg_loss: 0.0102\n",
      "batch 1299\tavg_bag_size: 49860.3900\tavg_cls_acc: 0.9977\tavg_cls_loss: 0.0106\tavg_loss: 0.0106\n",
      "batch 1399\tavg_bag_size: 49421.0907\tavg_cls_acc: 0.9979\tavg_cls_loss: 0.0107\tavg_loss: 0.0107\n",
      "batch 1499\tavg_bag_size: 49400.1460\tavg_cls_acc: 0.9960\tavg_cls_loss: 0.0156\tavg_loss: 0.0156\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9962\tavg_cls_loss: 0.0150\tavg_loss: 0.0150\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 18 ##########\n",
      "batch 99\tavg_bag_size: 51465.2400\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0039\tavg_loss: 0.0039\n",
      "batch 199\tavg_bag_size: 48282.3250\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0082\tavg_loss: 0.0082\n",
      "batch 299\tavg_bag_size: 47847.3933\tavg_cls_acc: 0.9933\tavg_cls_loss: 0.0195\tavg_loss: 0.0195\n",
      "batch 399\tavg_bag_size: 48327.1450\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0164\tavg_loss: 0.0164\n",
      "batch 499\tavg_bag_size: 49407.2060\tavg_cls_acc: 0.9960\tavg_cls_loss: 0.0154\tavg_loss: 0.0154\n",
      "batch 599\tavg_bag_size: 49863.3333\tavg_cls_acc: 0.9967\tavg_cls_loss: 0.0135\tavg_loss: 0.0135\n",
      "batch 699\tavg_bag_size: 49687.3300\tavg_cls_acc: 0.9914\tavg_cls_loss: 0.0240\tavg_loss: 0.0240\n",
      "batch 799\tavg_bag_size: 49345.9275\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0280\tavg_loss: 0.0280\n",
      "batch 899\tavg_bag_size: 49290.5689\tavg_cls_acc: 0.9844\tavg_cls_loss: 0.0372\tavg_loss: 0.0372\n",
      "batch 999\tavg_bag_size: 49212.6340\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0389\tavg_loss: 0.0389\n",
      "batch 1099\tavg_bag_size: 49662.5773\tavg_cls_acc: 0.9855\tavg_cls_loss: 0.0378\tavg_loss: 0.0378\n",
      "batch 1199\tavg_bag_size: 49427.7292\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0355\tavg_loss: 0.0355\n",
      "batch 1299\tavg_bag_size: 49405.1569\tavg_cls_acc: 0.9838\tavg_cls_loss: 0.0413\tavg_loss: 0.0413\n",
      "batch 1399\tavg_bag_size: 49751.8900\tavg_cls_acc: 0.9829\tavg_cls_loss: 0.0441\tavg_loss: 0.0441\n",
      "batch 1499\tavg_bag_size: 49945.8120\tavg_cls_acc: 0.9827\tavg_cls_loss: 0.0439\tavg_loss: 0.0439\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9824\tavg_cls_loss: 0.0462\tavg_loss: 0.0462\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 19 ##########\n",
      "batch 99\tavg_bag_size: 51904.7800\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0308\tavg_loss: 0.0308\n",
      "batch 199\tavg_bag_size: 51803.4500\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0232\tavg_loss: 0.0232\n",
      "batch 299\tavg_bag_size: 50833.8900\tavg_cls_acc: 0.9933\tavg_cls_loss: 0.0233\tavg_loss: 0.0233\n",
      "batch 399\tavg_bag_size: 50408.5325\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0192\tavg_loss: 0.0192\n",
      "batch 499\tavg_bag_size: 50241.5540\tavg_cls_acc: 0.9960\tavg_cls_loss: 0.0180\tavg_loss: 0.0180\n",
      "batch 599\tavg_bag_size: 50421.8817\tavg_cls_acc: 0.9967\tavg_cls_loss: 0.0154\tavg_loss: 0.0154\n",
      "batch 699\tavg_bag_size: 50172.8157\tavg_cls_acc: 0.9971\tavg_cls_loss: 0.0136\tavg_loss: 0.0136\n",
      "batch 799\tavg_bag_size: 49888.6562\tavg_cls_acc: 0.9975\tavg_cls_loss: 0.0129\tavg_loss: 0.0129\n",
      "batch 899\tavg_bag_size: 50100.4089\tavg_cls_acc: 0.9978\tavg_cls_loss: 0.0117\tavg_loss: 0.0117\n",
      "batch 999\tavg_bag_size: 49605.1500\tavg_cls_acc: 0.9980\tavg_cls_loss: 0.0110\tavg_loss: 0.0110\n",
      "batch 1099\tavg_bag_size: 49557.3591\tavg_cls_acc: 0.9982\tavg_cls_loss: 0.0103\tavg_loss: 0.0103\n",
      "batch 1199\tavg_bag_size: 49301.1583\tavg_cls_acc: 0.9967\tavg_cls_loss: 0.0131\tavg_loss: 0.0131\n",
      "batch 1299\tavg_bag_size: 49307.5123\tavg_cls_acc: 0.9969\tavg_cls_loss: 0.0133\tavg_loss: 0.0133\n",
      "batch 1399\tavg_bag_size: 49160.9621\tavg_cls_acc: 0.9971\tavg_cls_loss: 0.0128\tavg_loss: 0.0128\n",
      "batch 1499\tavg_bag_size: 49345.4513\tavg_cls_acc: 0.9960\tavg_cls_loss: 0.0139\tavg_loss: 0.0139\n",
      "batch 1588\tavg_bag_size: 49696.4059\tavg_cls_acc: 0.9962\tavg_cls_loss: 0.0135\tavg_loss: 0.0135\n",
      "##################################### \n",
      "\n",
      "End of training. Evaluating on Split TRAIN...:\n",
      "End of training. Evaluating on Split TUNE...:\n",
      "acc: 0.7869\n",
      "bacc: 0.5510\n",
      "kappa: 0.1159\n",
      "roc_auc: 0.7922\n",
      "weighted_f1: 0.7711\n",
      "bag_size: 52053.2459\n",
      "cls_acc: 0.7869\n",
      "cls_loss: 1.1286\n",
      "loss: 1.1286\n",
      "End of training. Evaluating on Split TEST...:\n",
      "acc: 0.8851\n",
      "bacc: 0.5237\n",
      "kappa: 0.0736\n",
      "roc_auc: 0.6738\n",
      "weighted_f1: 0.8485\n",
      "bag_size: 53733.6486\n",
      "cls_acc: 0.8851\n",
      "cls_loss: 0.9456\n",
      "loss: 0.9456\n",
      "FINISHED!\n",
      "\n",
      "\n",
      "\n",
      "label map:  {0: 0, 1: 1}\n",
      "task:  mutation\n",
      "split_dir:  /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-1\n",
      ".\n",
      "data source:  /data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\n",
      "feats_pt\n",
      "  feats_name  mag  patch_size\n",
      "0   feats_pt   20         256\n",
      "All data sources have the same feature extraction parameters.\n",
      "\n",
      "################### Settings ###################\n",
      "max_epochs:  20\n",
      "lr:  0.0001\n",
      "wd:  1e-05\n",
      "accum_steps:  1\n",
      "opt:  adamW\n",
      "lr_scheduler:  constant\n",
      "warmup_steps:  -1\n",
      "warmup_epochs:  -1\n",
      "batch_size:  4\n",
      "print_every:  100\n",
      "seed:  1\n",
      "num_workers:  2\n",
      "early_stopping:  False\n",
      "es_min_epochs:  15\n",
      "es_patience:  10\n",
      "es_metric:  loss\n",
      "model_type:  ABMIL\n",
      "emb_model_type:  LinEmb_LR\n",
      "ot_eps:  0.1\n",
      "model_config:  ABMIL_default\n",
      "in_dim:  1024\n",
      "in_dropout:  0.0\n",
      "bag_size:  -1\n",
      "train_bag_size:  -1\n",
      "val_bag_size:  -1\n",
      "train_sampler:  random\n",
      "n_fc_layers:  None\n",
      "em_iter:  None\n",
      "tau:  None\n",
      "out_type:  param_cat\n",
      "load_proto:  False\n",
      "proto_path:  .\n",
      "fix_proto:  False\n",
      "n_proto:  None\n",
      "exp_code:  None\n",
      "task:  mutation\n",
      "loss_fn:  None\n",
      "target_col:  TP53\n",
      "data_source:  ['/data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt']\n",
      "split_dir:  /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-1\n",
      "split_names:  train,tune,test\n",
      "overwrite:  False\n",
      "results_dir:  /data/temporary/nadieh/mutation_prediction/results/mutation/k=0/temporary::ABMIL_default::feats_pt/temporary::ABMIL_default::feats_pt::25-02-27-06-29-15\n",
      "tags:  None\n",
      "label_map:  {0: 0, 1: 1}\n",
      "n_classes:  2\n",
      "split_name_clean:  temporary\n",
      "split_k:  0\n",
      "pretrain_enc:  feats_pt\n",
      "pretrain_algo:  \n",
      "pretrain_exp:  \n",
      "pretrain_ckpt:  -1\n",
      "inference_prec:  fp32\n",
      "patch_mag:  20\n",
      "patch_size:  256\n",
      "feat_names:  ['feats_pt']\n",
      "Using the following split names: ['train', 'tune', 'test']\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-1/train.csv\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-1/tune.csv\n",
      "hhh /data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-1/test.csv\n",
      "{'train':            case_id  TP53                                           slide_id\n",
      "0     TCGA-2A-A8VO     0  TCGA-2A-A8VO-01Z-00-DX1.3A69CC37-B066-4529-B1B...\n",
      "1     TCGA-HC-7232     0  TCGA-HC-7232-01Z-00-DX1.a10b0b32-36bf-406c-bb8...\n",
      "2     TCGA-KK-A59V     0  TCGA-KK-A59V-01Z-00-DX1.181D78A7-2B0C-46BC-AF1...\n",
      "3     TCGA-HI-7169     0  TCGA-HI-7169-01Z-00-DX1.ac873262-34a1-4a47-817...\n",
      "4     TCGA-YL-A9WL     0  TCGA-YL-A9WL-01Z-00-DX1.4EE10C9A-18EA-4DB0-BE2...\n",
      "...            ...   ...                                                ...\n",
      "1589  TCGA-ZF-A9R1     0  TCGA-ZF-A9R1-01Z-00-DX1.74520759-D448-45B5-B8A...\n",
      "1590  TCGA-CF-A3MI     0  TCGA-CF-A3MI-01Z-00-DX1.5619CA1A-F772-44CC-BBE...\n",
      "1591  TCGA-DK-A1AF     0  TCGA-DK-A1AF-01Z-00-DX1.E18DE029-27A6-43A6-9E9...\n",
      "1592  TCGA-BT-A20P     0  TCGA-BT-A20P-01Z-00-DX1.721A7D6B-C9F3-4A70-8E8...\n",
      "1593  TCGA-GV-A3QG     0  TCGA-GV-A3QG-01Z-00-DX1.4A550B20-2A4C-48E5-BFE...\n",
      "\n",
      "[1594 rows x 3 columns], 'tune':          case_id  TP53                                           slide_id\n",
      "0   TCGA-KK-A5A1     0  TCGA-KK-A5A1-01Z-00-DX1.E2FB5324-5471-424D-95D...\n",
      "1   TCGA-CH-5766     0  TCGA-CH-5766-01Z-00-DX1.ea679de9-2957-4958-a11...\n",
      "2   TCGA-EJ-5503     0  TCGA-EJ-5503-01Z-00-DX1.ba2ee0d2-915a-45bd-b49...\n",
      "3   TCGA-YL-A8S8     0  TCGA-YL-A8S8-01Z-00-DX1.54845009-AA3A-4507-9FB...\n",
      "4   TCGA-G9-6354     0  TCGA-G9-6354-01Z-00-DX1.CE5E817C-B5AC-4D81-B61...\n",
      "..           ...   ...                                                ...\n",
      "60  TCGA-EJ-5495     0  TCGA-EJ-5495-01Z-00-DX1.0db6207b-4cc6-463c-8b9...\n",
      "61  TCGA-EJ-5507     1  TCGA-EJ-5507-01Z-00-DX1.6f3f7e05-a6f7-4450-a4b...\n",
      "62  TCGA-CH-5764     0  TCGA-CH-5764-01Z-00-DX1.153078f9-e6c0-48f4-af0...\n",
      "63  TCGA-HC-7211     0  TCGA-HC-7211-01Z-00-DX1.59ee78f9-0af3-4b3b-be4...\n",
      "64  TCGA-G9-6351     0  TCGA-G9-6351-01Z-00-DX1.53bfb95a-6a60-49dc-80e...\n",
      "\n",
      "[65 rows x 3 columns], 'test':           case_id  TP53                                           slide_id\n",
      "0    TCGA-G9-A9S0     1  TCGA-G9-A9S0-01Z-00-DX1.790729D2-1AF4-4ACF-A47...\n",
      "1    TCGA-YL-A9WH     1  TCGA-YL-A9WH-01Z-00-DX1.7240AB29-F661-4456-A68...\n",
      "2    TCGA-HC-7213     1  TCGA-HC-7213-01Z-00-DX1.5b03a3f2-ae77-4906-81b...\n",
      "3    TCGA-EJ-7315     1  TCGA-EJ-7315-01Z-00-DX1.6e08c383-ca90-45ad-8a2...\n",
      "4    TCGA-HC-A9TH     1  TCGA-HC-A9TH-01Z-00-DX1.13526861-B926-4CF5-9E9...\n",
      "..            ...   ...                                                ...\n",
      "134  TCGA-HC-7749     0  TCGA-HC-7749-01Z-00-DX1.2dbf5d61-954d-406e-9cc...\n",
      "135  TCGA-V1-A9OY     0  TCGA-V1-A9OY-01Z-00-DX1.95F6F791-F14D-4C88-834...\n",
      "136  TCGA-V1-A9O7     0  TCGA-V1-A9O7-01Z-00-DX1.FED3BDE1-B135-47B5-8B3...\n",
      "137  TCGA-KK-A8IA     0  TCGA-KK-A8IA-01Z-00-DX1.4396AFCE-0AE2-4066-BB7...\n",
      "138  TCGA-EJ-5499     0  TCGA-EJ-5499-01Z-00-DX1.bda3d0ed-5295-40c2-b23...\n",
      "\n",
      "[139 rows x 3 columns]}\n",
      "successfully read splits for:  ['train', 'tune', 'test']\n",
      "\n",
      "SPLIT:  train\n",
      "TP53\n",
      "0    1036\n",
      "1     558\n",
      "split: train, n: 1594\n",
      "\n",
      "SPLIT:  tune\n",
      "TP53\n",
      "0    58\n",
      "1     7\n",
      "split: tune, n: 65\n",
      "\n",
      "SPLIT:  test\n",
      "TP53\n",
      "0    123\n",
      "1     16\n",
      "split: test, n: 139\n",
      "\n",
      "Init Model... ABMIL doesn't construct unsupervised slide-level embeddings!\n",
      "Done!\n",
      "\n",
      "Init optimizer ... \n",
      "No EarlyStopping... ########## TRAIN Epoch: 0 ##########\n",
      "batch 99\tavg_bag_size: 50763.2600\tavg_cls_acc: 0.5700\tavg_cls_loss: 0.6950\tavg_loss: 0.6950\n",
      "batch 199\tavg_bag_size: 49649.0650\tavg_cls_acc: 0.6250\tavg_cls_loss: 0.6456\tavg_loss: 0.6456\n",
      "batch 299\tavg_bag_size: 50668.5133\tavg_cls_acc: 0.6733\tavg_cls_loss: 0.6016\tavg_loss: 0.6016\n",
      "batch 399\tavg_bag_size: 50109.2500\tavg_cls_acc: 0.7050\tavg_cls_loss: 0.5575\tavg_loss: 0.5575\n",
      "batch 499\tavg_bag_size: 50916.2220\tavg_cls_acc: 0.7020\tavg_cls_loss: 0.5566\tavg_loss: 0.5566\n",
      "batch 599\tavg_bag_size: 50195.8700\tavg_cls_acc: 0.7217\tavg_cls_loss: 0.5435\tavg_loss: 0.5435\n",
      "batch 699\tavg_bag_size: 50030.5457\tavg_cls_acc: 0.7129\tavg_cls_loss: 0.5556\tavg_loss: 0.5556\n",
      "batch 799\tavg_bag_size: 50006.8438\tavg_cls_acc: 0.7163\tavg_cls_loss: 0.5513\tavg_loss: 0.5513\n",
      "batch 899\tavg_bag_size: 49720.4356\tavg_cls_acc: 0.7222\tavg_cls_loss: 0.5499\tavg_loss: 0.5499\n",
      "batch 999\tavg_bag_size: 49794.2430\tavg_cls_acc: 0.7220\tavg_cls_loss: 0.5513\tavg_loss: 0.5513\n",
      "batch 1099\tavg_bag_size: 49808.9973\tavg_cls_acc: 0.7245\tavg_cls_loss: 0.5453\tavg_loss: 0.5453\n",
      "batch 1199\tavg_bag_size: 49582.1575\tavg_cls_acc: 0.7283\tavg_cls_loss: 0.5428\tavg_loss: 0.5428\n",
      "batch 1299\tavg_bag_size: 49626.0262\tavg_cls_acc: 0.7354\tavg_cls_loss: 0.5321\tavg_loss: 0.5321\n",
      "batch 1399\tavg_bag_size: 49587.6964\tavg_cls_acc: 0.7393\tavg_cls_loss: 0.5269\tavg_loss: 0.5269\n",
      "batch 1499\tavg_bag_size: 49571.7447\tavg_cls_acc: 0.7427\tavg_cls_loss: 0.5261\tavg_loss: 0.5261\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.7434\tavg_cls_loss: 0.5247\tavg_loss: 0.5247\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 1 ##########\n",
      "batch 99\tavg_bag_size: 53889.1200\tavg_cls_acc: 0.7900\tavg_cls_loss: 0.4312\tavg_loss: 0.4312\n",
      "batch 199\tavg_bag_size: 52524.8750\tavg_cls_acc: 0.8350\tavg_cls_loss: 0.3789\tavg_loss: 0.3789\n",
      "batch 299\tavg_bag_size: 52025.2467\tavg_cls_acc: 0.8033\tavg_cls_loss: 0.4355\tavg_loss: 0.4355\n",
      "batch 399\tavg_bag_size: 51110.5625\tavg_cls_acc: 0.8150\tavg_cls_loss: 0.4355\tavg_loss: 0.4355\n",
      "batch 499\tavg_bag_size: 50161.2840\tavg_cls_acc: 0.8200\tavg_cls_loss: 0.4285\tavg_loss: 0.4285\n",
      "batch 599\tavg_bag_size: 49487.9750\tavg_cls_acc: 0.8267\tavg_cls_loss: 0.4272\tavg_loss: 0.4272\n",
      "batch 699\tavg_bag_size: 49690.9157\tavg_cls_acc: 0.8357\tavg_cls_loss: 0.4200\tavg_loss: 0.4200\n",
      "batch 799\tavg_bag_size: 49856.0200\tavg_cls_acc: 0.8263\tavg_cls_loss: 0.4334\tavg_loss: 0.4334\n",
      "batch 899\tavg_bag_size: 49670.8567\tavg_cls_acc: 0.8200\tavg_cls_loss: 0.4459\tavg_loss: 0.4459\n",
      "batch 999\tavg_bag_size: 49562.6230\tavg_cls_acc: 0.8190\tavg_cls_loss: 0.4494\tavg_loss: 0.4494\n",
      "batch 1099\tavg_bag_size: 49373.5955\tavg_cls_acc: 0.8191\tavg_cls_loss: 0.4454\tavg_loss: 0.4454\n",
      "batch 1199\tavg_bag_size: 49157.0692\tavg_cls_acc: 0.8200\tavg_cls_loss: 0.4404\tavg_loss: 0.4404\n",
      "batch 1299\tavg_bag_size: 49194.4792\tavg_cls_acc: 0.8154\tavg_cls_loss: 0.4437\tavg_loss: 0.4437\n",
      "batch 1399\tavg_bag_size: 49175.0379\tavg_cls_acc: 0.8107\tavg_cls_loss: 0.4445\tavg_loss: 0.4445\n",
      "batch 1499\tavg_bag_size: 49304.2173\tavg_cls_acc: 0.8113\tavg_cls_loss: 0.4419\tavg_loss: 0.4419\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.8137\tavg_cls_loss: 0.4391\tavg_loss: 0.4391\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 2 ##########\n",
      "batch 99\tavg_bag_size: 48484.6200\tavg_cls_acc: 0.9200\tavg_cls_loss: 0.2997\tavg_loss: 0.2997\n",
      "batch 199\tavg_bag_size: 49341.9600\tavg_cls_acc: 0.8750\tavg_cls_loss: 0.3500\tavg_loss: 0.3500\n",
      "batch 299\tavg_bag_size: 49013.4133\tavg_cls_acc: 0.8433\tavg_cls_loss: 0.3731\tavg_loss: 0.3731\n",
      "batch 399\tavg_bag_size: 49036.2825\tavg_cls_acc: 0.8400\tavg_cls_loss: 0.3863\tavg_loss: 0.3863\n",
      "batch 499\tavg_bag_size: 48957.8000\tavg_cls_acc: 0.8420\tavg_cls_loss: 0.3753\tavg_loss: 0.3753\n",
      "batch 599\tavg_bag_size: 48750.4850\tavg_cls_acc: 0.8383\tavg_cls_loss: 0.3932\tavg_loss: 0.3932\n",
      "batch 699\tavg_bag_size: 48611.9957\tavg_cls_acc: 0.8400\tavg_cls_loss: 0.3841\tavg_loss: 0.3841\n",
      "batch 799\tavg_bag_size: 48215.2962\tavg_cls_acc: 0.8375\tavg_cls_loss: 0.3937\tavg_loss: 0.3937\n",
      "batch 899\tavg_bag_size: 48553.6856\tavg_cls_acc: 0.8356\tavg_cls_loss: 0.3981\tavg_loss: 0.3981\n",
      "batch 999\tavg_bag_size: 49284.9660\tavg_cls_acc: 0.8340\tavg_cls_loss: 0.3971\tavg_loss: 0.3971\n",
      "batch 1099\tavg_bag_size: 49529.4391\tavg_cls_acc: 0.8373\tavg_cls_loss: 0.3916\tavg_loss: 0.3916\n",
      "batch 1199\tavg_bag_size: 49439.1825\tavg_cls_acc: 0.8408\tavg_cls_loss: 0.3875\tavg_loss: 0.3875\n",
      "batch 1299\tavg_bag_size: 49722.3192\tavg_cls_acc: 0.8392\tavg_cls_loss: 0.3868\tavg_loss: 0.3868\n",
      "batch 1399\tavg_bag_size: 49849.9400\tavg_cls_acc: 0.8400\tavg_cls_loss: 0.3877\tavg_loss: 0.3877\n",
      "batch 1499\tavg_bag_size: 50005.5707\tavg_cls_acc: 0.8413\tavg_cls_loss: 0.3859\tavg_loss: 0.3859\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.8344\tavg_cls_loss: 0.3958\tavg_loss: 0.3958\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 3 ##########\n",
      "batch 99\tavg_bag_size: 48261.6900\tavg_cls_acc: 0.8700\tavg_cls_loss: 0.3529\tavg_loss: 0.3529\n",
      "batch 199\tavg_bag_size: 46771.6250\tavg_cls_acc: 0.8550\tavg_cls_loss: 0.3680\tavg_loss: 0.3680\n",
      "batch 299\tavg_bag_size: 48090.7267\tavg_cls_acc: 0.8567\tavg_cls_loss: 0.3458\tavg_loss: 0.3458\n",
      "batch 399\tavg_bag_size: 49388.2875\tavg_cls_acc: 0.8600\tavg_cls_loss: 0.3446\tavg_loss: 0.3446\n",
      "batch 499\tavg_bag_size: 48955.2220\tavg_cls_acc: 0.8580\tavg_cls_loss: 0.3367\tavg_loss: 0.3367\n",
      "batch 599\tavg_bag_size: 49640.1700\tavg_cls_acc: 0.8517\tavg_cls_loss: 0.3462\tavg_loss: 0.3462\n",
      "batch 699\tavg_bag_size: 50515.6614\tavg_cls_acc: 0.8543\tavg_cls_loss: 0.3432\tavg_loss: 0.3432\n",
      "batch 799\tavg_bag_size: 50073.4275\tavg_cls_acc: 0.8575\tavg_cls_loss: 0.3436\tavg_loss: 0.3436\n",
      "batch 899\tavg_bag_size: 49837.7244\tavg_cls_acc: 0.8644\tavg_cls_loss: 0.3362\tavg_loss: 0.3362\n",
      "batch 999\tavg_bag_size: 49930.4450\tavg_cls_acc: 0.8710\tavg_cls_loss: 0.3310\tavg_loss: 0.3310\n",
      "batch 1099\tavg_bag_size: 50074.0945\tavg_cls_acc: 0.8709\tavg_cls_loss: 0.3358\tavg_loss: 0.3358\n",
      "batch 1199\tavg_bag_size: 50069.5550\tavg_cls_acc: 0.8683\tavg_cls_loss: 0.3352\tavg_loss: 0.3352\n",
      "batch 1299\tavg_bag_size: 49644.0162\tavg_cls_acc: 0.8623\tavg_cls_loss: 0.3494\tavg_loss: 0.3494\n",
      "batch 1399\tavg_bag_size: 49501.5914\tavg_cls_acc: 0.8629\tavg_cls_loss: 0.3478\tavg_loss: 0.3478\n",
      "batch 1499\tavg_bag_size: 49220.3560\tavg_cls_acc: 0.8600\tavg_cls_loss: 0.3521\tavg_loss: 0.3521\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.8614\tavg_cls_loss: 0.3466\tavg_loss: 0.3466\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 4 ##########\n",
      "batch 99\tavg_bag_size: 51005.2900\tavg_cls_acc: 0.8100\tavg_cls_loss: 0.3792\tavg_loss: 0.3792\n",
      "batch 199\tavg_bag_size: 51553.9350\tavg_cls_acc: 0.8650\tavg_cls_loss: 0.3030\tavg_loss: 0.3030\n",
      "batch 299\tavg_bag_size: 51703.1567\tavg_cls_acc: 0.9000\tavg_cls_loss: 0.2446\tavg_loss: 0.2446\n",
      "batch 399\tavg_bag_size: 50133.8450\tavg_cls_acc: 0.9000\tavg_cls_loss: 0.2440\tavg_loss: 0.2440\n",
      "batch 499\tavg_bag_size: 51154.5220\tavg_cls_acc: 0.8940\tavg_cls_loss: 0.2597\tavg_loss: 0.2597\n",
      "batch 599\tavg_bag_size: 51014.0867\tavg_cls_acc: 0.8900\tavg_cls_loss: 0.2660\tavg_loss: 0.2660\n",
      "batch 699\tavg_bag_size: 50704.3957\tavg_cls_acc: 0.8886\tavg_cls_loss: 0.2767\tavg_loss: 0.2767\n",
      "batch 799\tavg_bag_size: 50508.1038\tavg_cls_acc: 0.8838\tavg_cls_loss: 0.2782\tavg_loss: 0.2782\n",
      "batch 899\tavg_bag_size: 50018.4744\tavg_cls_acc: 0.8867\tavg_cls_loss: 0.2716\tavg_loss: 0.2716\n",
      "batch 999\tavg_bag_size: 49998.7670\tavg_cls_acc: 0.8820\tavg_cls_loss: 0.2775\tavg_loss: 0.2775\n",
      "batch 1099\tavg_bag_size: 50111.2609\tavg_cls_acc: 0.8827\tavg_cls_loss: 0.2790\tavg_loss: 0.2790\n",
      "batch 1199\tavg_bag_size: 49886.1492\tavg_cls_acc: 0.8792\tavg_cls_loss: 0.2909\tavg_loss: 0.2909\n",
      "batch 1299\tavg_bag_size: 49638.2362\tavg_cls_acc: 0.8815\tavg_cls_loss: 0.2874\tavg_loss: 0.2874\n",
      "batch 1399\tavg_bag_size: 49838.6786\tavg_cls_acc: 0.8771\tavg_cls_loss: 0.2964\tavg_loss: 0.2964\n",
      "batch 1499\tavg_bag_size: 49526.3647\tavg_cls_acc: 0.8767\tavg_cls_loss: 0.2955\tavg_loss: 0.2955\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.8752\tavg_cls_loss: 0.2975\tavg_loss: 0.2975\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 5 ##########\n",
      "batch 99\tavg_bag_size: 49850.6800\tavg_cls_acc: 0.9000\tavg_cls_loss: 0.2375\tavg_loss: 0.2375\n",
      "batch 199\tavg_bag_size: 50934.2050\tavg_cls_acc: 0.9150\tavg_cls_loss: 0.2305\tavg_loss: 0.2305\n",
      "batch 299\tavg_bag_size: 49526.1300\tavg_cls_acc: 0.9200\tavg_cls_loss: 0.2317\tavg_loss: 0.2317\n",
      "batch 399\tavg_bag_size: 48552.6500\tavg_cls_acc: 0.9200\tavg_cls_loss: 0.2342\tavg_loss: 0.2342\n",
      "batch 499\tavg_bag_size: 49181.7200\tavg_cls_acc: 0.9080\tavg_cls_loss: 0.2478\tavg_loss: 0.2478\n",
      "batch 599\tavg_bag_size: 49581.0400\tavg_cls_acc: 0.9050\tavg_cls_loss: 0.2460\tavg_loss: 0.2460\n",
      "batch 699\tavg_bag_size: 49625.1543\tavg_cls_acc: 0.9143\tavg_cls_loss: 0.2353\tavg_loss: 0.2353\n",
      "batch 799\tavg_bag_size: 49285.9587\tavg_cls_acc: 0.9062\tavg_cls_loss: 0.2499\tavg_loss: 0.2499\n",
      "batch 899\tavg_bag_size: 49242.4844\tavg_cls_acc: 0.9033\tavg_cls_loss: 0.2471\tavg_loss: 0.2471\n",
      "batch 999\tavg_bag_size: 49533.1890\tavg_cls_acc: 0.9030\tavg_cls_loss: 0.2513\tavg_loss: 0.2513\n",
      "batch 1099\tavg_bag_size: 49646.9527\tavg_cls_acc: 0.9000\tavg_cls_loss: 0.2550\tavg_loss: 0.2550\n",
      "batch 1199\tavg_bag_size: 49770.4567\tavg_cls_acc: 0.9017\tavg_cls_loss: 0.2540\tavg_loss: 0.2540\n",
      "batch 1299\tavg_bag_size: 49737.9738\tavg_cls_acc: 0.9015\tavg_cls_loss: 0.2526\tavg_loss: 0.2526\n",
      "batch 1399\tavg_bag_size: 49786.4200\tavg_cls_acc: 0.9036\tavg_cls_loss: 0.2514\tavg_loss: 0.2514\n",
      "batch 1499\tavg_bag_size: 49716.1893\tavg_cls_acc: 0.9013\tavg_cls_loss: 0.2529\tavg_loss: 0.2529\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9040\tavg_cls_loss: 0.2461\tavg_loss: 0.2461\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 6 ##########\n",
      "batch 99\tavg_bag_size: 53940.3500\tavg_cls_acc: 0.9300\tavg_cls_loss: 0.2048\tavg_loss: 0.2048\n",
      "batch 199\tavg_bag_size: 48821.0350\tavg_cls_acc: 0.9350\tavg_cls_loss: 0.1895\tavg_loss: 0.1895\n",
      "batch 299\tavg_bag_size: 49636.1833\tavg_cls_acc: 0.9333\tavg_cls_loss: 0.1838\tavg_loss: 0.1838\n",
      "batch 399\tavg_bag_size: 48269.0475\tavg_cls_acc: 0.9275\tavg_cls_loss: 0.1914\tavg_loss: 0.1914\n",
      "batch 499\tavg_bag_size: 47357.5080\tavg_cls_acc: 0.9280\tavg_cls_loss: 0.1900\tavg_loss: 0.1900\n",
      "batch 599\tavg_bag_size: 47503.8317\tavg_cls_acc: 0.9333\tavg_cls_loss: 0.1880\tavg_loss: 0.1880\n",
      "batch 699\tavg_bag_size: 48166.6929\tavg_cls_acc: 0.9371\tavg_cls_loss: 0.1780\tavg_loss: 0.1780\n",
      "batch 799\tavg_bag_size: 48469.1675\tavg_cls_acc: 0.9350\tavg_cls_loss: 0.1760\tavg_loss: 0.1760\n",
      "batch 899\tavg_bag_size: 49204.6800\tavg_cls_acc: 0.9322\tavg_cls_loss: 0.1855\tavg_loss: 0.1855\n",
      "batch 999\tavg_bag_size: 49524.1430\tavg_cls_acc: 0.9340\tavg_cls_loss: 0.1808\tavg_loss: 0.1808\n",
      "batch 1099\tavg_bag_size: 49862.8018\tavg_cls_acc: 0.9345\tavg_cls_loss: 0.1798\tavg_loss: 0.1798\n",
      "batch 1199\tavg_bag_size: 50000.5517\tavg_cls_acc: 0.9308\tavg_cls_loss: 0.1859\tavg_loss: 0.1859\n",
      "batch 1299\tavg_bag_size: 49792.9246\tavg_cls_acc: 0.9292\tavg_cls_loss: 0.1872\tavg_loss: 0.1872\n",
      "batch 1399\tavg_bag_size: 49546.1943\tavg_cls_acc: 0.9307\tavg_cls_loss: 0.1844\tavg_loss: 0.1844\n",
      "batch 1499\tavg_bag_size: 49511.7413\tavg_cls_acc: 0.9300\tavg_cls_loss: 0.1884\tavg_loss: 0.1884\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9297\tavg_cls_loss: 0.1900\tavg_loss: 0.1900\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 7 ##########\n",
      "batch 99\tavg_bag_size: 50882.2500\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0981\tavg_loss: 0.0981\n",
      "batch 199\tavg_bag_size: 50998.8300\tavg_cls_acc: 0.9550\tavg_cls_loss: 0.1607\tavg_loss: 0.1607\n",
      "batch 299\tavg_bag_size: 51198.9133\tavg_cls_acc: 0.9467\tavg_cls_loss: 0.1510\tavg_loss: 0.1510\n",
      "batch 399\tavg_bag_size: 50681.1375\tavg_cls_acc: 0.9550\tavg_cls_loss: 0.1366\tavg_loss: 0.1366\n",
      "batch 499\tavg_bag_size: 49373.2240\tavg_cls_acc: 0.9580\tavg_cls_loss: 0.1372\tavg_loss: 0.1372\n",
      "batch 599\tavg_bag_size: 49694.6017\tavg_cls_acc: 0.9450\tavg_cls_loss: 0.1497\tavg_loss: 0.1497\n",
      "batch 699\tavg_bag_size: 50020.1343\tavg_cls_acc: 0.9486\tavg_cls_loss: 0.1449\tavg_loss: 0.1449\n",
      "batch 799\tavg_bag_size: 50500.6225\tavg_cls_acc: 0.9463\tavg_cls_loss: 0.1495\tavg_loss: 0.1495\n",
      "batch 899\tavg_bag_size: 50244.7144\tavg_cls_acc: 0.9467\tavg_cls_loss: 0.1413\tavg_loss: 0.1413\n",
      "batch 999\tavg_bag_size: 50102.3220\tavg_cls_acc: 0.9500\tavg_cls_loss: 0.1362\tavg_loss: 0.1362\n",
      "batch 1099\tavg_bag_size: 50499.8800\tavg_cls_acc: 0.9500\tavg_cls_loss: 0.1399\tavg_loss: 0.1399\n",
      "batch 1199\tavg_bag_size: 50420.3958\tavg_cls_acc: 0.9483\tavg_cls_loss: 0.1431\tavg_loss: 0.1431\n",
      "batch 1299\tavg_bag_size: 50218.3231\tavg_cls_acc: 0.9446\tavg_cls_loss: 0.1501\tavg_loss: 0.1501\n",
      "batch 1399\tavg_bag_size: 50136.9464\tavg_cls_acc: 0.9464\tavg_cls_loss: 0.1491\tavg_loss: 0.1491\n",
      "batch 1499\tavg_bag_size: 49987.8620\tavg_cls_acc: 0.9480\tavg_cls_loss: 0.1473\tavg_loss: 0.1473\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9473\tavg_cls_loss: 0.1467\tavg_loss: 0.1467\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 8 ##########\n",
      "batch 99\tavg_bag_size: 47510.7300\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0892\tavg_loss: 0.0892\n",
      "batch 199\tavg_bag_size: 48200.1400\tavg_cls_acc: 0.9750\tavg_cls_loss: 0.0814\tavg_loss: 0.0814\n",
      "batch 299\tavg_bag_size: 47353.4800\tavg_cls_acc: 0.9767\tavg_cls_loss: 0.0791\tavg_loss: 0.0791\n",
      "batch 399\tavg_bag_size: 48644.7000\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0822\tavg_loss: 0.0822\n",
      "batch 499\tavg_bag_size: 48665.9480\tavg_cls_acc: 0.9660\tavg_cls_loss: 0.0871\tavg_loss: 0.0871\n",
      "batch 599\tavg_bag_size: 49204.1267\tavg_cls_acc: 0.9667\tavg_cls_loss: 0.0881\tavg_loss: 0.0881\n",
      "batch 699\tavg_bag_size: 49594.3257\tavg_cls_acc: 0.9714\tavg_cls_loss: 0.0826\tavg_loss: 0.0826\n",
      "batch 799\tavg_bag_size: 49717.7550\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0858\tavg_loss: 0.0858\n",
      "batch 899\tavg_bag_size: 49689.0944\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0847\tavg_loss: 0.0847\n",
      "batch 999\tavg_bag_size: 49905.8300\tavg_cls_acc: 0.9680\tavg_cls_loss: 0.0883\tavg_loss: 0.0883\n",
      "batch 1099\tavg_bag_size: 49408.7364\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0860\tavg_loss: 0.0860\n",
      "batch 1199\tavg_bag_size: 49056.2633\tavg_cls_acc: 0.9708\tavg_cls_loss: 0.0835\tavg_loss: 0.0835\n",
      "batch 1299\tavg_bag_size: 48854.7692\tavg_cls_acc: 0.9708\tavg_cls_loss: 0.0864\tavg_loss: 0.0864\n",
      "batch 1399\tavg_bag_size: 48950.8893\tavg_cls_acc: 0.9707\tavg_cls_loss: 0.0865\tavg_loss: 0.0865\n",
      "batch 1499\tavg_bag_size: 49589.5013\tavg_cls_acc: 0.9680\tavg_cls_loss: 0.0941\tavg_loss: 0.0941\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9674\tavg_cls_loss: 0.0957\tavg_loss: 0.0957\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 9 ##########\n",
      "batch 99\tavg_bag_size: 48967.4800\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0701\tavg_loss: 0.0701\n",
      "batch 199\tavg_bag_size: 50358.9600\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0553\tavg_loss: 0.0553\n",
      "batch 299\tavg_bag_size: 52725.5367\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0481\tavg_loss: 0.0481\n",
      "batch 399\tavg_bag_size: 52420.2325\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0451\tavg_loss: 0.0451\n",
      "batch 499\tavg_bag_size: 51750.9420\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0614\tavg_loss: 0.0614\n",
      "batch 599\tavg_bag_size: 51767.6767\tavg_cls_acc: 0.9817\tavg_cls_loss: 0.0572\tavg_loss: 0.0572\n",
      "batch 699\tavg_bag_size: 51755.9586\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0577\tavg_loss: 0.0577\n",
      "batch 799\tavg_bag_size: 51525.3075\tavg_cls_acc: 0.9788\tavg_cls_loss: 0.0653\tavg_loss: 0.0653\n",
      "batch 899\tavg_bag_size: 50775.8900\tavg_cls_acc: 0.9733\tavg_cls_loss: 0.0709\tavg_loss: 0.0709\n",
      "batch 999\tavg_bag_size: 50822.5720\tavg_cls_acc: 0.9710\tavg_cls_loss: 0.0765\tavg_loss: 0.0765\n",
      "batch 1099\tavg_bag_size: 50625.7736\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0810\tavg_loss: 0.0810\n",
      "batch 1199\tavg_bag_size: 50198.3117\tavg_cls_acc: 0.9683\tavg_cls_loss: 0.0870\tavg_loss: 0.0870\n",
      "batch 1299\tavg_bag_size: 50290.6062\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0870\tavg_loss: 0.0870\n",
      "batch 1399\tavg_bag_size: 50222.3771\tavg_cls_acc: 0.9714\tavg_cls_loss: 0.0840\tavg_loss: 0.0840\n",
      "batch 1499\tavg_bag_size: 49922.5140\tavg_cls_acc: 0.9727\tavg_cls_loss: 0.0826\tavg_loss: 0.0826\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9718\tavg_cls_loss: 0.0862\tavg_loss: 0.0862\n",
      "#################################### \n",
      "\n",
      "########## TRAIN Epoch: 10 ##########\n",
      "batch 99\tavg_bag_size: 47761.0500\tavg_cls_acc: 0.9700\tavg_cls_loss: 0.0552\tavg_loss: 0.0552\n",
      "batch 199\tavg_bag_size: 49674.0950\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0385\tavg_loss: 0.0385\n",
      "batch 299\tavg_bag_size: 50563.7300\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0334\tavg_loss: 0.0334\n",
      "batch 399\tavg_bag_size: 50825.2225\tavg_cls_acc: 0.9925\tavg_cls_loss: 0.0305\tavg_loss: 0.0305\n",
      "batch 499\tavg_bag_size: 50911.0200\tavg_cls_acc: 0.9940\tavg_cls_loss: 0.0278\tavg_loss: 0.0278\n",
      "batch 599\tavg_bag_size: 50631.1617\tavg_cls_acc: 0.9917\tavg_cls_loss: 0.0311\tavg_loss: 0.0311\n",
      "batch 699\tavg_bag_size: 50794.8143\tavg_cls_acc: 0.9914\tavg_cls_loss: 0.0376\tavg_loss: 0.0376\n",
      "batch 799\tavg_bag_size: 50838.3938\tavg_cls_acc: 0.9862\tavg_cls_loss: 0.0450\tavg_loss: 0.0450\n",
      "batch 899\tavg_bag_size: 51252.3133\tavg_cls_acc: 0.9833\tavg_cls_loss: 0.0581\tavg_loss: 0.0581\n",
      "batch 999\tavg_bag_size: 50957.1220\tavg_cls_acc: 0.9820\tavg_cls_loss: 0.0597\tavg_loss: 0.0597\n",
      "batch 1099\tavg_bag_size: 50946.6545\tavg_cls_acc: 0.9827\tavg_cls_loss: 0.0575\tavg_loss: 0.0575\n",
      "batch 1199\tavg_bag_size: 50733.6167\tavg_cls_acc: 0.9825\tavg_cls_loss: 0.0577\tavg_loss: 0.0577\n",
      "batch 1299\tavg_bag_size: 50219.1854\tavg_cls_acc: 0.9838\tavg_cls_loss: 0.0545\tavg_loss: 0.0545\n",
      "batch 1399\tavg_bag_size: 49944.2243\tavg_cls_acc: 0.9836\tavg_cls_loss: 0.0555\tavg_loss: 0.0555\n",
      "batch 1499\tavg_bag_size: 49826.5100\tavg_cls_acc: 0.9840\tavg_cls_loss: 0.0541\tavg_loss: 0.0541\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9824\tavg_cls_loss: 0.0584\tavg_loss: 0.0584\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 11 ##########\n",
      "batch 99\tavg_bag_size: 49922.9100\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0624\tavg_loss: 0.0624\n",
      "batch 199\tavg_bag_size: 47420.0100\tavg_cls_acc: 0.9750\tavg_cls_loss: 0.0635\tavg_loss: 0.0635\n",
      "batch 299\tavg_bag_size: 49162.5667\tavg_cls_acc: 0.9767\tavg_cls_loss: 0.0617\tavg_loss: 0.0617\n",
      "batch 399\tavg_bag_size: 48802.9025\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0621\tavg_loss: 0.0621\n",
      "batch 499\tavg_bag_size: 49295.2400\tavg_cls_acc: 0.9760\tavg_cls_loss: 0.0615\tavg_loss: 0.0615\n",
      "batch 599\tavg_bag_size: 49209.3267\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0538\tavg_loss: 0.0538\n",
      "batch 699\tavg_bag_size: 49378.1971\tavg_cls_acc: 0.9814\tavg_cls_loss: 0.0524\tavg_loss: 0.0524\n",
      "batch 799\tavg_bag_size: 50043.2862\tavg_cls_acc: 0.9812\tavg_cls_loss: 0.0531\tavg_loss: 0.0531\n",
      "batch 899\tavg_bag_size: 49906.9522\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0539\tavg_loss: 0.0539\n",
      "batch 999\tavg_bag_size: 49536.0330\tavg_cls_acc: 0.9780\tavg_cls_loss: 0.0586\tavg_loss: 0.0586\n",
      "batch 1099\tavg_bag_size: 49636.4218\tavg_cls_acc: 0.9773\tavg_cls_loss: 0.0617\tavg_loss: 0.0617\n",
      "batch 1199\tavg_bag_size: 49905.6517\tavg_cls_acc: 0.9775\tavg_cls_loss: 0.0606\tavg_loss: 0.0606\n",
      "batch 1299\tavg_bag_size: 49850.0969\tavg_cls_acc: 0.9762\tavg_cls_loss: 0.0637\tavg_loss: 0.0637\n",
      "batch 1399\tavg_bag_size: 49974.1379\tavg_cls_acc: 0.9736\tavg_cls_loss: 0.0667\tavg_loss: 0.0667\n",
      "batch 1499\tavg_bag_size: 49931.1207\tavg_cls_acc: 0.9747\tavg_cls_loss: 0.0664\tavg_loss: 0.0664\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9730\tavg_cls_loss: 0.0679\tavg_loss: 0.0679\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 12 ##########\n",
      "batch 99\tavg_bag_size: 54835.5900\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0358\tavg_loss: 0.0358\n",
      "batch 199\tavg_bag_size: 51495.8700\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0356\tavg_loss: 0.0356\n",
      "batch 299\tavg_bag_size: 50353.5133\tavg_cls_acc: 0.9967\tavg_cls_loss: 0.0295\tavg_loss: 0.0295\n",
      "batch 399\tavg_bag_size: 49942.5975\tavg_cls_acc: 0.9975\tavg_cls_loss: 0.0249\tavg_loss: 0.0249\n",
      "batch 499\tavg_bag_size: 49894.5620\tavg_cls_acc: 0.9980\tavg_cls_loss: 0.0225\tavg_loss: 0.0225\n",
      "batch 599\tavg_bag_size: 49309.2983\tavg_cls_acc: 0.9983\tavg_cls_loss: 0.0215\tavg_loss: 0.0215\n",
      "batch 699\tavg_bag_size: 49138.7043\tavg_cls_acc: 0.9986\tavg_cls_loss: 0.0208\tavg_loss: 0.0208\n",
      "batch 799\tavg_bag_size: 49206.0500\tavg_cls_acc: 0.9988\tavg_cls_loss: 0.0201\tavg_loss: 0.0201\n",
      "batch 899\tavg_bag_size: 49294.0089\tavg_cls_acc: 0.9989\tavg_cls_loss: 0.0184\tavg_loss: 0.0184\n",
      "batch 999\tavg_bag_size: 49569.4040\tavg_cls_acc: 0.9990\tavg_cls_loss: 0.0183\tavg_loss: 0.0183\n",
      "batch 1099\tavg_bag_size: 49690.8018\tavg_cls_acc: 0.9991\tavg_cls_loss: 0.0170\tavg_loss: 0.0170\n",
      "batch 1199\tavg_bag_size: 49540.8900\tavg_cls_acc: 0.9983\tavg_cls_loss: 0.0179\tavg_loss: 0.0179\n",
      "batch 1299\tavg_bag_size: 49408.5631\tavg_cls_acc: 0.9931\tavg_cls_loss: 0.0322\tavg_loss: 0.0322\n",
      "batch 1399\tavg_bag_size: 49623.9479\tavg_cls_acc: 0.9929\tavg_cls_loss: 0.0325\tavg_loss: 0.0325\n",
      "batch 1499\tavg_bag_size: 49613.5060\tavg_cls_acc: 0.9927\tavg_cls_loss: 0.0334\tavg_loss: 0.0334\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9925\tavg_cls_loss: 0.0332\tavg_loss: 0.0332\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 13 ##########\n",
      "batch 99\tavg_bag_size: 50496.8600\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0141\tavg_loss: 0.0141\n",
      "batch 199\tavg_bag_size: 49674.3000\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0206\tavg_loss: 0.0206\n",
      "batch 299\tavg_bag_size: 49963.5267\tavg_cls_acc: 0.9933\tavg_cls_loss: 0.0178\tavg_loss: 0.0178\n",
      "batch 399\tavg_bag_size: 50112.9950\tavg_cls_acc: 0.9925\tavg_cls_loss: 0.0241\tavg_loss: 0.0241\n",
      "batch 499\tavg_bag_size: 50470.4800\tavg_cls_acc: 0.9940\tavg_cls_loss: 0.0239\tavg_loss: 0.0239\n",
      "batch 599\tavg_bag_size: 49958.9600\tavg_cls_acc: 0.9933\tavg_cls_loss: 0.0250\tavg_loss: 0.0250\n",
      "batch 699\tavg_bag_size: 49878.0071\tavg_cls_acc: 0.9929\tavg_cls_loss: 0.0257\tavg_loss: 0.0257\n",
      "batch 799\tavg_bag_size: 49673.8250\tavg_cls_acc: 0.9925\tavg_cls_loss: 0.0250\tavg_loss: 0.0250\n",
      "batch 899\tavg_bag_size: 49238.6600\tavg_cls_acc: 0.9911\tavg_cls_loss: 0.0259\tavg_loss: 0.0259\n",
      "batch 999\tavg_bag_size: 49474.3340\tavg_cls_acc: 0.9920\tavg_cls_loss: 0.0246\tavg_loss: 0.0246\n",
      "batch 1099\tavg_bag_size: 49481.8173\tavg_cls_acc: 0.9918\tavg_cls_loss: 0.0246\tavg_loss: 0.0246\n",
      "batch 1199\tavg_bag_size: 49874.5567\tavg_cls_acc: 0.9925\tavg_cls_loss: 0.0232\tavg_loss: 0.0232\n",
      "batch 1299\tavg_bag_size: 50225.3523\tavg_cls_acc: 0.9923\tavg_cls_loss: 0.0269\tavg_loss: 0.0269\n",
      "batch 1399\tavg_bag_size: 50055.2457\tavg_cls_acc: 0.9907\tavg_cls_loss: 0.0292\tavg_loss: 0.0292\n",
      "batch 1499\tavg_bag_size: 49974.1293\tavg_cls_acc: 0.9893\tavg_cls_loss: 0.0364\tavg_loss: 0.0364\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9887\tavg_cls_loss: 0.0375\tavg_loss: 0.0375\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 14 ##########\n",
      "batch 99\tavg_bag_size: 48724.1500\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0200\tavg_loss: 0.0200\n",
      "batch 199\tavg_bag_size: 49557.0750\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0152\tavg_loss: 0.0152\n",
      "batch 299\tavg_bag_size: 49580.7700\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0240\tavg_loss: 0.0240\n",
      "batch 399\tavg_bag_size: 50580.7425\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0408\tavg_loss: 0.0408\n",
      "batch 499\tavg_bag_size: 50945.2220\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0363\tavg_loss: 0.0363\n",
      "batch 599\tavg_bag_size: 50733.3717\tavg_cls_acc: 0.9883\tavg_cls_loss: 0.0371\tavg_loss: 0.0371\n",
      "batch 699\tavg_bag_size: 51579.6400\tavg_cls_acc: 0.9871\tavg_cls_loss: 0.0356\tavg_loss: 0.0356\n",
      "batch 799\tavg_bag_size: 50954.3512\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0358\tavg_loss: 0.0358\n",
      "batch 899\tavg_bag_size: 50710.4900\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0445\tavg_loss: 0.0445\n",
      "batch 999\tavg_bag_size: 50371.4060\tavg_cls_acc: 0.9840\tavg_cls_loss: 0.0518\tavg_loss: 0.0518\n",
      "batch 1099\tavg_bag_size: 50093.2718\tavg_cls_acc: 0.9827\tavg_cls_loss: 0.0520\tavg_loss: 0.0520\n",
      "batch 1199\tavg_bag_size: 49997.3133\tavg_cls_acc: 0.9833\tavg_cls_loss: 0.0498\tavg_loss: 0.0498\n",
      "batch 1299\tavg_bag_size: 50156.7792\tavg_cls_acc: 0.9846\tavg_cls_loss: 0.0473\tavg_loss: 0.0473\n",
      "batch 1399\tavg_bag_size: 50203.2350\tavg_cls_acc: 0.9857\tavg_cls_loss: 0.0451\tavg_loss: 0.0451\n",
      "batch 1499\tavg_bag_size: 50180.3160\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0427\tavg_loss: 0.0427\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0407\tavg_loss: 0.0407\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 15 ##########\n",
      "batch 99\tavg_bag_size: 46904.7300\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0066\tavg_loss: 0.0066\n",
      "batch 199\tavg_bag_size: 47069.4650\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0208\tavg_loss: 0.0208\n",
      "batch 299\tavg_bag_size: 46820.1133\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0332\tavg_loss: 0.0332\n",
      "batch 399\tavg_bag_size: 47982.9875\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0393\tavg_loss: 0.0393\n",
      "batch 499\tavg_bag_size: 47529.4380\tavg_cls_acc: 0.9880\tavg_cls_loss: 0.0473\tavg_loss: 0.0473\n",
      "batch 599\tavg_bag_size: 48169.4467\tavg_cls_acc: 0.9867\tavg_cls_loss: 0.0525\tavg_loss: 0.0525\n",
      "batch 699\tavg_bag_size: 48292.5329\tavg_cls_acc: 0.9871\tavg_cls_loss: 0.0527\tavg_loss: 0.0527\n",
      "batch 799\tavg_bag_size: 47971.1675\tavg_cls_acc: 0.9888\tavg_cls_loss: 0.0478\tavg_loss: 0.0478\n",
      "batch 899\tavg_bag_size: 48661.0456\tavg_cls_acc: 0.9889\tavg_cls_loss: 0.0457\tavg_loss: 0.0457\n",
      "batch 999\tavg_bag_size: 48297.2980\tavg_cls_acc: 0.9890\tavg_cls_loss: 0.0430\tavg_loss: 0.0430\n",
      "batch 1099\tavg_bag_size: 48830.4873\tavg_cls_acc: 0.9891\tavg_cls_loss: 0.0409\tavg_loss: 0.0409\n",
      "batch 1199\tavg_bag_size: 48946.3808\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0388\tavg_loss: 0.0388\n",
      "batch 1299\tavg_bag_size: 49292.3515\tavg_cls_acc: 0.9908\tavg_cls_loss: 0.0378\tavg_loss: 0.0378\n",
      "batch 1399\tavg_bag_size: 49403.5471\tavg_cls_acc: 0.9914\tavg_cls_loss: 0.0355\tavg_loss: 0.0355\n",
      "batch 1499\tavg_bag_size: 49538.6920\tavg_cls_acc: 0.9920\tavg_cls_loss: 0.0337\tavg_loss: 0.0337\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9918\tavg_cls_loss: 0.0328\tavg_loss: 0.0328\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 16 ##########\n",
      "batch 99\tavg_bag_size: 47656.4400\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0034\tavg_loss: 0.0034\n",
      "batch 199\tavg_bag_size: 50043.2550\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0023\tavg_loss: 0.0023\n",
      "batch 299\tavg_bag_size: 50581.4000\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0025\tavg_loss: 0.0025\n",
      "batch 399\tavg_bag_size: 48399.3700\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0024\tavg_loss: 0.0024\n",
      "batch 499\tavg_bag_size: 48497.1220\tavg_cls_acc: 0.9940\tavg_cls_loss: 0.0101\tavg_loss: 0.0101\n",
      "batch 599\tavg_bag_size: 49134.9200\tavg_cls_acc: 0.9950\tavg_cls_loss: 0.0102\tavg_loss: 0.0102\n",
      "batch 699\tavg_bag_size: 49987.6214\tavg_cls_acc: 0.9914\tavg_cls_loss: 0.0177\tavg_loss: 0.0177\n",
      "batch 799\tavg_bag_size: 50111.1062\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0197\tavg_loss: 0.0197\n",
      "batch 899\tavg_bag_size: 49566.2967\tavg_cls_acc: 0.9856\tavg_cls_loss: 0.0289\tavg_loss: 0.0289\n",
      "batch 999\tavg_bag_size: 49247.7160\tavg_cls_acc: 0.9870\tavg_cls_loss: 0.0279\tavg_loss: 0.0279\n",
      "batch 1099\tavg_bag_size: 49014.6791\tavg_cls_acc: 0.9864\tavg_cls_loss: 0.0302\tavg_loss: 0.0302\n",
      "batch 1199\tavg_bag_size: 49732.6392\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0291\tavg_loss: 0.0291\n",
      "batch 1299\tavg_bag_size: 49510.4354\tavg_cls_acc: 0.9885\tavg_cls_loss: 0.0274\tavg_loss: 0.0274\n",
      "batch 1399\tavg_bag_size: 49803.0850\tavg_cls_acc: 0.9886\tavg_cls_loss: 0.0280\tavg_loss: 0.0280\n",
      "batch 1499\tavg_bag_size: 49740.5293\tavg_cls_acc: 0.9887\tavg_cls_loss: 0.0326\tavg_loss: 0.0326\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9868\tavg_cls_loss: 0.0346\tavg_loss: 0.0346\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 17 ##########\n",
      "batch 99\tavg_bag_size: 52598.8900\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0074\tavg_loss: 0.0074\n",
      "batch 199\tavg_bag_size: 48425.8450\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0075\tavg_loss: 0.0075\n",
      "batch 299\tavg_bag_size: 48469.1233\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0253\tavg_loss: 0.0253\n",
      "batch 399\tavg_bag_size: 48724.0725\tavg_cls_acc: 0.9875\tavg_cls_loss: 0.0381\tavg_loss: 0.0381\n",
      "batch 499\tavg_bag_size: 48099.9320\tavg_cls_acc: 0.9840\tavg_cls_loss: 0.0692\tavg_loss: 0.0692\n",
      "batch 599\tavg_bag_size: 47628.8000\tavg_cls_acc: 0.9817\tavg_cls_loss: 0.0650\tavg_loss: 0.0650\n",
      "batch 699\tavg_bag_size: 47762.5914\tavg_cls_acc: 0.9829\tavg_cls_loss: 0.0598\tavg_loss: 0.0598\n",
      "batch 799\tavg_bag_size: 48232.5312\tavg_cls_acc: 0.9838\tavg_cls_loss: 0.0551\tavg_loss: 0.0551\n",
      "batch 899\tavg_bag_size: 48580.2856\tavg_cls_acc: 0.9856\tavg_cls_loss: 0.0503\tavg_loss: 0.0503\n",
      "batch 999\tavg_bag_size: 49095.5520\tavg_cls_acc: 0.9860\tavg_cls_loss: 0.0527\tavg_loss: 0.0527\n",
      "batch 1099\tavg_bag_size: 49327.2155\tavg_cls_acc: 0.9873\tavg_cls_loss: 0.0495\tavg_loss: 0.0495\n",
      "batch 1199\tavg_bag_size: 49424.1150\tavg_cls_acc: 0.9883\tavg_cls_loss: 0.0467\tavg_loss: 0.0467\n",
      "batch 1299\tavg_bag_size: 49504.7285\tavg_cls_acc: 0.9885\tavg_cls_loss: 0.0467\tavg_loss: 0.0467\n",
      "batch 1399\tavg_bag_size: 49303.8814\tavg_cls_acc: 0.9886\tavg_cls_loss: 0.0455\tavg_loss: 0.0455\n",
      "batch 1499\tavg_bag_size: 49342.6893\tavg_cls_acc: 0.9893\tavg_cls_loss: 0.0437\tavg_loss: 0.0437\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9900\tavg_cls_loss: 0.0416\tavg_loss: 0.0416\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 18 ##########\n",
      "batch 99\tavg_bag_size: 51231.0000\tavg_cls_acc: 1.0000\tavg_cls_loss: 0.0021\tavg_loss: 0.0021\n",
      "batch 199\tavg_bag_size: 51549.1950\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0430\tavg_loss: 0.0430\n",
      "batch 299\tavg_bag_size: 51842.6067\tavg_cls_acc: 0.9833\tavg_cls_loss: 0.0347\tavg_loss: 0.0347\n",
      "batch 399\tavg_bag_size: 50203.5675\tavg_cls_acc: 0.9750\tavg_cls_loss: 0.0615\tavg_loss: 0.0615\n",
      "batch 499\tavg_bag_size: 50023.4440\tavg_cls_acc: 0.9740\tavg_cls_loss: 0.0672\tavg_loss: 0.0672\n",
      "batch 599\tavg_bag_size: 49789.9450\tavg_cls_acc: 0.9750\tavg_cls_loss: 0.0648\tavg_loss: 0.0648\n",
      "batch 699\tavg_bag_size: 49702.3414\tavg_cls_acc: 0.9771\tavg_cls_loss: 0.0608\tavg_loss: 0.0608\n",
      "batch 799\tavg_bag_size: 49856.2012\tavg_cls_acc: 0.9788\tavg_cls_loss: 0.0557\tavg_loss: 0.0557\n",
      "batch 899\tavg_bag_size: 49679.8844\tavg_cls_acc: 0.9789\tavg_cls_loss: 0.0558\tavg_loss: 0.0558\n",
      "batch 999\tavg_bag_size: 49477.3440\tavg_cls_acc: 0.9800\tavg_cls_loss: 0.0546\tavg_loss: 0.0546\n",
      "batch 1099\tavg_bag_size: 49545.3464\tavg_cls_acc: 0.9809\tavg_cls_loss: 0.0520\tavg_loss: 0.0520\n",
      "batch 1199\tavg_bag_size: 49065.8300\tavg_cls_acc: 0.9825\tavg_cls_loss: 0.0485\tavg_loss: 0.0485\n",
      "batch 1299\tavg_bag_size: 49179.5015\tavg_cls_acc: 0.9838\tavg_cls_loss: 0.0454\tavg_loss: 0.0454\n",
      "batch 1399\tavg_bag_size: 49440.0957\tavg_cls_acc: 0.9850\tavg_cls_loss: 0.0427\tavg_loss: 0.0427\n",
      "batch 1499\tavg_bag_size: 49527.8540\tavg_cls_acc: 0.9860\tavg_cls_loss: 0.0402\tavg_loss: 0.0402\n",
      "batch 1593\tavg_bag_size: 49570.5954\tavg_cls_acc: 0.9868\tavg_cls_loss: 0.0384\tavg_loss: 0.0384\n",
      "##################################### \n",
      "\n",
      "########## TRAIN Epoch: 19 ##########\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the static variables\n",
    "data_source = r\"/data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\"\n",
    "task = \"mutation\"\n",
    "batch_size = 4\n",
    "results_dir = \"/data/temporary/nadieh/mutation_prediction/results\"\n",
    "split_names = \"train,tune,test\"\n",
    "in_dim = 1024\n",
    "epoch = 20\n",
    "model_configs = [ \"ABMIL_default\"]\n",
    "label = 'TP53'\n",
    "# Loop over k from 1 to 5\n",
    "for model_config in model_configs:\n",
    "    model_type = \"ABMIL\"\n",
    "    for k in range(0, 2):\n",
    "        split_dir = f\"/data/temporary/nadieh/mutation_prediction/3-folds-correct/fold-{k}\"\n",
    "        # Build the command\n",
    "        cmd = [\n",
    "            'python3', 'training/main_classification.py',\n",
    "            '--data_source', data_source,\n",
    "            '--split_dir', split_dir,\n",
    "            '--task', task,\n",
    "            '--batch_size', str(batch_size),\n",
    "            '--results_dir', results_dir,\n",
    "            '--split_names', split_names,\n",
    "            '--in_dim', str(in_dim),\n",
    "            '--model_config', model_config,\n",
    "            '--target_col', label,\n",
    "            '--model_type', model_type,\n",
    "            '--max_epochs', str(epoch),\n",
    "        ]\n",
    "        # Execute the command\n",
    "        subprocess.run(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec4346-a472-4065-a27c-92e8c0a180a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "csv_root_folder = r\"/data/temporary/nadieh/mutation_prediction/3-folds-prostate-only\"\n",
    "corrected_folder = r\"/data/temporary/nadieh/mutation_prediction/3-folds-correct-only-prostate\"\n",
    "target_folder = r\"/data/temporary/nadieh/feat_uni/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\"\n",
    "\n",
    "# Ensure the corrected directory exists\n",
    "os.makedirs(corrected_folder, exist_ok=True)\n",
    "\n",
    "# Specify fold names and dataset types\n",
    "folds = [\"fold-0\", \"fold-1\", \"fold-2\"]\n",
    "data_types = [\"train\", \"tune\", \"test\"]  # Process train, tune, and test separately\n",
    "\n",
    "# Iterate through each fold\n",
    "for fold in folds:\n",
    "    fold_path = os.path.join(csv_root_folder, fold)\n",
    "    corrected_fold_path = os.path.join(corrected_folder, fold)\n",
    "    os.makedirs(corrected_fold_path, exist_ok=True)  # Ensure fold structure in corrected folder\n",
    "\n",
    "    if not os.path.exists(fold_path):\n",
    "        print(f\"Skipping {fold} (folder not found).\")\n",
    "        continue\n",
    "\n",
    "    for data_type in data_types:\n",
    "        csv_file_path = os.path.join(fold_path, f\"{data_type}.csv\")\n",
    "        print(f\"Processing: {csv_file_path}\")\n",
    "\n",
    "        # Skip if the train/test/tune CSV file does not exist\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            print(f\"Skipping {data_type}.csv in {fold} (file not found).\")\n",
    "            continue\n",
    "\n",
    "        # Read CSV and check for 'case_id' column\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        if \"case_id\" not in df.columns:\n",
    "            print(f\"Skipping {data_type}.csv in {fold} (no 'case_id' column).\")\n",
    "            continue\n",
    "\n",
    "        # Ensure case_id is a string\n",
    "        df[\"case_id\"] = df[\"case_id\"].astype(str)\n",
    "\n",
    "        # Get the three relevant columns (assuming they exist in CSV)\n",
    "        relevant_columns = [\"case_id\"]  # Always include case_id\n",
    "        for col in df.columns:\n",
    "            if col not in relevant_columns and len(relevant_columns) < 4:  # Limit to 3 extra columns\n",
    "                relevant_columns.append(col)\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        df = df[relevant_columns]\n",
    "\n",
    "        # Get list of all filenames in the target folder\n",
    "        if not os.path.exists(target_folder):\n",
    "            print(f\"Target folder '{target_folder}' does not exist.\")\n",
    "            continue\n",
    "        target_files = os.listdir(target_folder)\n",
    "\n",
    "        # Create a new DataFrame to store updated rows\n",
    "        new_rows = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            case_id = row[\"case_id\"]\n",
    "\n",
    "            # Find all matching files\n",
    "            matched_files = [file for file in target_files if file.startswith(case_id)]\n",
    "\n",
    "            if matched_files:\n",
    "                # Create a new row for each slide_id match while keeping original column values\n",
    "                for matched_file in matched_files:\n",
    "                    new_row = row.copy()\n",
    "                    new_row[\"slide_id\"] = matched_file[:-3]\n",
    "                    new_row[\"TP53\"] = row[\"TP53\"]\n",
    "                    new_rows.append(new_row)\n",
    "            \n",
    "\n",
    "\n",
    "        # Convert updated rows into DataFrame\n",
    "        updated_df = pd.DataFrame(new_rows)\n",
    "\n",
    "        # Ensure all original columns plus slide_id exist in the final output\n",
    "        original_columns = relevant_columns + [\"slide_id\"]\n",
    "        updated_df = updated_df[original_columns]  # Maintain column order\n",
    "\n",
    "        # Save updated data with all original columns\n",
    "        output_file = os.path.join(corrected_fold_path, f\"{data_type}.csv\")\n",
    "        updated_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"✅ {fold} - {data_type}: Processed {len(updated_df)} rows. Results saved in {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d174f53-18ed-435d-8e72-2e4a9e111756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 103, in <module>\n",
      "    results = main(args)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 52, in main\n",
      "    loader_train = dataset_splits['train']\n",
      "KeyError: 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_dir:  /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_1\n",
      "Using the following split names: ['train', 'test']\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_1/train.csv\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_1/test.csv\n",
      "\n",
      "successfully read splits for:  []\n",
      "\n",
      "Init Datasets... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 103, in <module>\n",
      "    results = main(args)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 52, in main\n",
      "    loader_train = dataset_splits['train']\n",
      "KeyError: 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_dir:  /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_2\n",
      "Using the following split names: ['train', 'test']\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_2/train.csv\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_2/test.csv\n",
      "\n",
      "successfully read splits for:  []\n",
      "\n",
      "Init Datasets... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 103, in <module>\n",
      "    results = main(args)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 52, in main\n",
      "    loader_train = dataset_splits['train']\n",
      "KeyError: 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_dir:  /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_3\n",
      "Using the following split names: ['train', 'test']\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_3/train.csv\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_3/test.csv\n",
      "\n",
      "successfully read splits for:  []\n",
      "\n",
      "Init Datasets... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 103, in <module>\n",
      "    results = main(args)\n",
      "  File \"/data/temporary/nadieh/PANTHER/src/training/main_prototype.py\", line 52, in main\n",
      "    loader_train = dataset_splits['train']\n",
      "KeyError: 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_dir:  /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_4\n",
      "Using the following split names: ['train', 'test']\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_4/train.csv\n",
      "hhh /data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_4/test.csv\n",
      "\n",
      "successfully read splits for:  []\n",
      "\n",
      "Init Datasets... "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "# Set the environment variable for the feature directories\n",
    "target_col = 'TP53'\n",
    "\n",
    "all_feat_dirs = \"/data/temporary/nadieh/patches_uni/features/tcga_brca/extracted_mag20x_patch256_fp/extracted-vit_large_patch16_224.dinov2.uni_mass100k/feats_pt\"\n",
    "for i in range(1,5):\n",
    "\n",
    "    split_dir = f'/data/temporary/nadieh/OmicsGen/src/splits/mutation/TCGA_PRAD_{i}'\n",
    "    # Define other variables\n",
    "    in_dim = 1024\n",
    "    n_proto_patches = 1000000\n",
    "    n_proto = 16\n",
    "    n_init = 5\n",
    "    seed = 1\n",
    "    num_workers = 5\n",
    "\n",
    "    # Build the command\n",
    "    cmd = [\n",
    "        'python3', '-m', 'training.main_prototype',\n",
    "        '--mode', 'faiss',\n",
    "        '--data_source', all_feat_dirs,\n",
    "        '--split_dir', split_dir,\n",
    "        '--split_names', 'train,test',\n",
    "        '--in_dim', str(in_dim),\n",
    "        '--n_proto_patches', str(n_proto_patches),\n",
    "        '--n_proto', str(n_proto),\n",
    "        '--n_init', str(n_init),\n",
    "        '--seed', str(seed),\n",
    "        '--num_workers', str(num_workers)\n",
    "    ]\n",
    "\n",
    "    # Execute the command\n",
    "    subprocess.run(cmd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
